@InProceedings{Lowe2017MADDPG,
  author           = {Ryan Lowe and Yi Wu and Aviv Tamar and Jean Harb and Pieter Abbeel and Igor Mordatch},
  booktitle        = {NIPS},
  title            = {Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
  year             = {2017},
  pages            = {6382--6393},
  abstract         = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case:
Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows.
We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multiagent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies.
We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
  cdate            = {1483228800000},
  file             = {:Lowe_2017_Multi Agent Actor Critic for Mixed Cooperative Competitive Environments (MADDPG).pdf:PDF},
  modificationdate = {2022-06-15T15:55:59},
  url              = {http://papers.nips.cc/paper/7217-multi-agent-actor-critic-for-mixed-cooperative-competitive-environments},
}

@Misc{Duan2016RL2,
  author           = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
  month            = nov,
  title            = {{RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning}},
  year             = {2016},
  abstract         = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL$^2$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL$^2$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL$^2$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL$^2$ on a vision-based navigation task and show that it scales up to high-dimensional problems.},
  archiveprefix    = {arXiv},
  eprint           = {1611.02779},
  file             = {:Meta_Learning/Duan_2016_RL^2_ Fast Reinforcement Learning Via Slow Reinforcement Learning.pdf:PDF},
  groups           = {Meta Learning},
  keywords         = {cs.AI, cs.LG, cs.NE, stat.ML},
  language         = {en},
  modificationdate = {2022-03-02T11:35:54},
  primaryclass     = {cs.AI},
  url              = {https://openreview.net/forum?id=HkLXCE9lx},
  urldate          = {2021-11-17},
}

@MastersThesis{Kwiatkowski2020Improving,
  author           = {Kwiatkowski, Ariel},
  school           = {Aalto University. School of Science},
  title            = {{Improving Ad-Hoc Cooperation in Multiagent Reinforcement Learning via Skill Modeling}},
  year             = {2020},
  type             = {Master's thesis},
  abstract         = {Machine learning is a versatile tool allowing for, among other things, training intelligent agents capable of autonomously acting in their environments. In particular, Multiagent Reinforcement Learning has made tremendous progress enabling such agents to interact with one another in an effective manner. One of the challenges that this field is still facing, however, is the problem of ad-hoc cooperation, or cooperation with agents that have not been previously encountered. 

This thesis explores one possible approach to tackle this issue, using the psychology-inspired idea of Theory of Mind. Specifically, a component designed to explicitly model the skill level of the other agent is included, to allow the primary agent to better choose its actions. 

The results show that this approach does in fact facilitate better coordination in an environment designed to test this skill and is a promising method for more complicated scenarios. 

The potential applications can be found in any situation that requires coordination between multiple intelligent agents (which may also include humans), such as traffic coordination between autonomous vehicles, or rescue operations where autonomous agents and humans have to work together to efficiently search an area.},
  file             = {:Ad_Hoc_Teamwork/Kwiatkowski_2020_Improving Ad Hoc Cooperation in Multiagent Reinforcement Learning Via Skill Modeling.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  keywords         = {machine learning, reinforcement learning, artificial intelligence, multiagent systems, theory of mind},
  language         = {English},
  modificationdate = {2022-10-18T22:50:54},
  pages            = {1--64},
  url              = {https://aaltodoc.aalto.fi/handle/123456789/46046},
}

@Misc{Canaan2020Generating,
  author        = {Rodrigo Canaan and Xianbo Gao and Julian Togelius and Andy Nealen and Stefan Menzel},
  title         = {Generating and Adapting to Diverse Ad-Hoc Cooperation Agents in Hanabi},
  year          = {2020},
  month         = apr,
  abstract      = {Hanabi is a cooperative game that brings the problem of modeling other players to the forefront. In this game, coordinated groups of players can leverage pre-established conventions to great effect, but playing in an ad-hoc setting requires agents to adapt to its partner's strategies with no previous coordination. Evaluating an agent in this setting requires a diverse population of potential partners, but so far, the behavioral diversity of agents has not been considered in a systematic way. This paper proposes Quality Diversity algorithms as a promising class of algorithms to generate diverse populations for this purpose, and generates a population of diverse Hanabi agents using MAP-Elites. We also postulate that agents can benefit from a diverse population during training and implement a simple "meta-strategy" for adapting to an agent's perceived behavioral niche. We show this meta-strategy can work better than generalist strategies even outside the population it was trained with if its partner's behavioral niche can be correctly inferred, but in practice a partner's behavior depends and interferes with the meta-agent's own behavior, suggesting an avenue for future research in characterizing another agent's behavior during gameplay.},
  archiveprefix = {arXiv},
  eprint        = {2004.13710},
  file          = {:Ad_Hoc_Teamwork/Canaan_2020_Generating and Adapting to Diverse Ad Hoc Cooperation Agents in Hanabi.pdf:PDF},
  groups        = {Ad Hoc Teamwork},
  keywords      = {cs.AI, cs.NE},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2004.13710},
}

@PhdThesis{Chen2020Thesis,
  author    = {Shuo Chen},
  school    = {Nanyang Technological University},
  title     = {Coping with uncertainties in agent cooperation},
  year      = {2020},
  abstract  = {In multi-agent systems, intelligent agents interact with one another to achieve either individual or shared goals. This thesis focuses on scenarios where agents cooperate to achieve their goals. In particular, agents can help one another by information sharing or collaboration. Thus, agent cooperation is usually beneficial for the agent performance. However, there exist uncertainties that can jeopardise the successful achievement of goals. This thesis tackles the problems that arise from those uncertainties. The first problem emerges when agents pursue individual goals. Agents can share the information of system states with one another to make more informed decisions while pursuing their goals. However, agents may send false information to mislead others and thus, increase their own benefits. Therefore, there is uncertainty about whether the information shared by other agents is trustworthy or not. The existing approaches use trust management schemes to compute the trustworthiness of shared information. However, they only focus on the accuracies of trustworthiness without considering the cost/delay incurred during the trust computation. This thesis proposes a partially observable Markov decision process (POMDP) model. The model queries the information about uncertain states from neighbouring agents while taking into account their potential malicious behaviours. We also propose an algorithm to learn model parameters in the dynamic scenario where malicious agents change their behaviour from time to time. Experimental results demonstrate that our model can effectively balance the decision quality and response time while still being robust to sophisticated malicious attacks. When goals are too complex for a single agent, agents can compose teams and coordinate their actions to achieve them. There, the uncertainty arises when agents cannot communicate or share team strategies with their teammates. Specifically, an agent has to understand the behaviour of its teammates and plan its actions accordingly. Here, the behaviour amounts to a function that takes a state as input and outputs an action that can further change the state. Note that the agent can only observe its teammates’ actions or state changes instead of their underlying behaviours. Thus, there is uncertainty about teammates’ behaviours. Agents performing the teamwork without relying on communication or team strategies is called the ad hoc teamwork. We refer to the domains where the agent can fully observe its teammates’ actions as simple domains. We refer to the domains where teammates’ actions are partially observable as complex domains. There, the agent can only rely on its teammates’ state changes. For ad hoc teamwork in simple domains, the existing approaches use teammates’ behaviour models to predict their actions and choose the ad hoc agent’s action accordingly. However, the behaviour models may not be accurate, which can compromise teamwork. In this thesis, we propose Ad Hoc Teamwork by Sub-task Inference and Selection (ATSIS) algorithm that uses a sub-task inference without relying on teammates’ models. First, the ad hoc agent observes its teammates to infer which sub-tasks they are handling. Based on that, it selects its own sub-task using a POMDP model that handles the uncertainty of the sub-task inference. Last, the ad hoc agent uses the Monte Carlo tree search (MCTS) to find the set of actions to perform the chosen sub-task. Experiments demonstrate that ATSIS achieves the teamwork robustly. Also, ATSIS makes much faster decisions than state-of-the-art schemes, which is significant for time-sensitive tasks. Moreover, ATSIS can further improve its performance by integrating the learned model. For ad hoc teamwork in complex domains, the most advanced approach learns policies based on previous experiences and reuses one of the policies to interact with new teammates. However, the selected policy in many cases is sub-optimal. Switching between policies to adapt to new teammates’ behaviour takes time, which threatens the successful performance of a task. In this thesis, we propose Achieving the Ad Hoc Teamwork by Employing the Attention Mechanism (AATEAM) algorithm that uses the attention-based neural networks to cope with new teammates’ behaviour in real-time. We train one attention network per teammate type. The attention networks learn both to extract the temporal correlations from the sequence of states (i.e. contexts) and the mapping from contexts to actions. Each attention network also learns to predict a future state given the current context and its output action. The prediction accuracies help to determine which actions the ad hoc agent should take. Experimental results indicate that when working with both known and unknown teammates, in most cases our algorithm outperforms the most advanced approach. This demonstrates that AATEAM can adapt to new teammates’ changing behaviour faster than the state-of-the-art.},
  doi       = {10.32657/10356/137074},
  file      = {:Ad_Hoc_Teamwork/Chen_2020_Coping with Uncertainties in Agent Cooperation.pdf:PDF},
  groups    = {Ad Hoc Teamwork},
  keywords  = {Engineering::Computer science and engineering::Computing methodologies::Artificial intelligence},
  publisher = {Nanyang Technological University},
  url       = {https://hdl.handle.net/10356/137074},
}

@Misc{Ajaykumar2021FACT,
  author        = {Gopika Ajaykumar and Annie Mao and Jeremy Brown and Chien-Ming Huang},
  title         = {FACT: A Full-body Ad-hoc Collaboration Testbed for Modeling Complex Teamwork},
  year          = {2021},
  month         = jun,
  abstract      = {Robots are envisioned to work alongside humans in applications ranging from in-home assistance to collaborative manufacturing. Research on human-robot collaboration (HRC) has helped develop various aspects of social intelligence necessary for robots to participate in effective, fluid collaborations with humans. However, HRC research has focused on dyadic, structured, and minimal collaborations between humans and robots that may not fully represent the large scale and emergent nature of more complex, unstructured collaborative activities. Thus, there remains a need for shared testbeds, datasets, and evaluation metrics that researchers can use to better model natural, ad-hoc human collaborative behaviors and develop robot capabilities intended for large scale emergent collaborations. We present one such shared resource - FACT (Full-body Ad-hoc Collaboration Testbed), an openly accessible testbed for researchers to obtain an expansive view of the individual and team-based behaviors involved in complex, co-located teamwork. We detail observations from a preliminary exploration with teams of various sizes and discuss potential research questions that may be investigated using the testbed. Our goal is for FACT to be an initial resource that supports a more holistic investigation of human-robot collaboration.},
  archiveprefix = {arXiv},
  eprint        = {2106.03290},
  file          = {:Ajaykumar_2021_FACT_ a Full Body Ad Hoc Collaboration Testbed for Modeling Complex Teamwork.pdf:PDF},
  groups        = {Ad Hoc Teamwork},
  keywords      = {cs.RO},
  primaryclass  = {cs.RO},
}

@Misc{Tu2021Adversarial,
  author        = {James Tu and Tsunhsuan Wang and Jingkang Wang and Sivabalan Manivasagam and Mengye Ren and Raquel Urtasun},
  title         = {Adversarial Attacks On Multi-Agent Communication},
  year          = {2021},
  month         = jan,
  abstract      = {Growing at a very fast pace, modern autonomous systems will soon be deployed at scale, opening up the possibility for cooperative multi-agent systems. By sharing information and distributing workloads, autonomous agents can better perform their tasks and enjoy improved computation efficiency. However, such advantages rely heavily on communication channels which have been shown to be vulnerable to security breaches. Thus, communication can be compromised to execute adversarial attacks on deep learning models which are widely employed in modern systems. In this paper, we explore such adversarial attacks in a novel multi-agent setting where agents communicate by sharing learned intermediate representations. We observe that an indistinguishable adversarial message can severely degrade performance, but becomes weaker as the number of benign agents increase. Furthermore, we show that transfer attacks are more difficult in this setting when compared to directly perturbing the inputs, as it is necessary to align the distribution of communication messages with domain adaptation. Finally, we show that low-budget online attacks can be achieved by exploiting the temporal consistency of streaming sensory inputs.},
  archiveprefix = {arXiv},
  eprint        = {2101.06560},
  file          = {:Tu_2021_Adversarial Attacks on Multi Agent Communication.pdf:PDF},
  keywords      = {cs.LG, cs.CR, cs.CV},
  primaryclass  = {cs.LG},
}

@Misc{Nagabandi2018Learning,
  author        = {Anusha Nagabandi and Ignasi Clavera and Simin Liu and Ronald S. Fearing and Pieter Abbeel and Sergey Levine and Chelsea Finn},
  title         = {Learning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning},
  year          = {2018},
  month         = mar,
  abstract      = {Although reinforcement learning methods can achieve impressive results in simulation, the real world presents two major challenges: generating samples is exceedingly expensive, and unexpected perturbations or unseen situations cause proficient but specialized policies to fail at test time. Given that it is impractical to train separate policies to accommodate all situations the agent may see in the real world, this work proposes to learn how to quickly and effectively adapt online to new tasks. To enable sample-efficient learning, we consider learning online adaptation in the context of model-based reinforcement learning. Our approach uses meta-learning to train a dynamics model prior such that, when combined with recent data, this prior can be rapidly adapted to the local context. Our experiments demonstrate online adaptation for continuous control tasks on both simulated and real-world agents. We first show simulated agents adapting their behavior online to novel terrains, crippled body parts, and highly-dynamic environments. We also illustrate the importance of incorporating online adaptation into autonomous agents that operate in the real world by applying our method to a real dynamic legged millirobot. We demonstrate the agent's learned ability to quickly adapt online to a missing leg, adjust to novel terrains and slopes, account for miscalibration or errors in pose estimation, and compensate for pulling payloads.},
  archiveprefix = {arXiv},
  code          = {https://github.com/iclavera/learning_to_adapt},
  eprint        = {1803.11347},
  file          = {:Nagabandi_2018_Learning to Adapt in Dynamic, Real World Environments through Meta Reinforcement Learning.pdf:PDF},
  groups        = {Meta Learning},
  keywords      = {cs.LG, cs.RO, stat.ML},
  primaryclass  = {cs.LG},
  ranking       = {rank4},
}

@InCollection{Ribeiro2021Helping,
  author           = {Jo{\~{a}}o G. Ribeiro and Miguel Faria and Alberto Sardinha and Francisco S. Melo},
  publisher        = {Springer International Publishing},
  title            = {Helping People on the Fly: Ad Hoc Teamwork for Human-Robot Teams},
  year             = {2021},
  pages            = {635--647},
  abstract         = {We present the Bayesian Online Prediction for Ad hoc teamwork (BOPA), a novel algorithm for ad hoc teamwork which enables a robot to collaborate, on the fly, with human teammates without any pre-coordination protocol. Unlike previous works, BOPA relies only on state observations/transitions of the environment in order to identify the task being performed by a given teammate (without observing the teammate’s actions and environment’s reward signals). We evaluate BOPA in two distinct settings, namely (i) an empirical evaluation in a simulated environment with three different types of teammates, and (ii) an experimental evaluation in a real-world environment, deploying BOPA into an ad hoc robot with the goal of assisting a human teammate in completing a given task. Our results show that BOPA is effective at correctly identifying the target task, efficient at solving the correct task in optimal and near-optimal times, scalable by adapting to different problem sizes, and robust to non-optimal teammates, such as humans.},
  doi              = {10.1007/978-3-030-86230-5_50},
  file             = {:Ad_Hoc_Teamwork/Ribeiro_2021_Helping People on the Fly_ Ad Hoc Teamwork for Human Robot Teams.pdf:PDF},
  groups           = {Human-Agent Ad Hoc},
  modificationdate = {2022-09-03T10:14:27},
}

@InProceedings{Yourdshahi2020Online,
  author           = {Shafipour Yourdshahi, Elnaz and Aparecido do Carmo Alves, Matheus and Soriano Marcolino, Leandro and Angelov, Plamen},
  booktitle        = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  title            = {On-Line Estimators for Ad-Hoc Task Allocation},
  year             = {2020},
  address          = {Richland, SC},
  pages            = {1999--2001},
  publisher        = {International Foundation for Autonomous Agents and Multiagent Systems},
  series           = {AAMAS '20},
  abstract         = {It is essential for agents to work together with others to accomplish common missions without previous knowledge of the team-mates, a challenge known as ad-hoc teamwork. In these systems, an agent estimates the algorithm and parameters of others in an on-line manner, in order to decide its own actions for effective teamwork. Meanwhile, agents often must coordinate in a decentralised fashion to complete tasks that are displaced in an environment (e.g., in foraging, demining, rescue or fire control), where each member autonomously chooses which task to perform. By harnessing this knowledge, better estimation techniques would lead to better performance. Hence, we present On-line Estimators for Ad-hoc Task Allocation, a novel algorithm for team-mates' type and parameter estimation in decentralised task allocation. We ran experiments in the level-based foraging domain, where we obtain lower error in parameter and type estimation than previous approaches, and a significantly better performance in finishing all tasks.},
  doi              = {10.5555/3398761.3399054},
  file             = {:Ad_Hoc_Teamwork/AdHocTaskAllocation/Shafipour Yourdshahi_2020_On Line Estimators for Ad Hoc Task Allocation.pdf:PDF},
  groups           = {Ad Hoc Task Allocation},
  isbn             = {9781450375184},
  keywords         = {coordination and collaboration, on-line learning, ad-hoc teamwork},
  location         = {Auckland, New Zealand},
  modificationdate = {2022-06-15T15:58:49},
  numpages         = {3},
}

@InProceedings{Giampapa2009Toward,
  author           = {Joseph A. Giampapa and Katia P. Sycara and Gita Sukthankar},
  title            = {Toward identifying process models in ad hoc and distributed teams},
  year             = {2009},
  publisher        = {{ACM} Press},
  abstract         = {This article reports work on first steps toward characterizing a negotiation process model for ad hoc and distributed groups or teams, so that automation can more accurately track the states of a negotiation from human discourse. We devised three experimental scenarios and ran human subject experiments that involved group decision-making and consensus building. Our experiments showed that the communication patterns of successful distributed ad hoc teams differed in two significantly different conditions.
We describe our motivations, experimental design and results.},
  doi              = {10.1145/1609170.1609177},
  file             = {:Giampapa_2009_Toward Identifying Process Models in Ad Hoc and Distributed Teams.pdf:PDF},
  modificationdate = {2022-03-29T22:02:40},
}

@InProceedings{Yourdshahi2020Estimators,
  author    = {Shafipour Yourdshahi, Elnaz and Do Carmo Alves, Matheus and Soriano Marcolino, Leandro and Angelov, Plamen},
  booktitle = {11th International Workshop on Optimization and Learning in Multiagent Systems},
  title     = {Decentralised Task Allocation in the Fog:Estimators for Effective Ad-hoc Teamwork},
  year      = {2020},
  month     = may,
  file      = {:Ad_Hoc_Teamwork/AdHocTaskAllocation/Shafipour Yourdshahi_2020_Decentralised Task Allocation in the Fog_Estimators for Effective Ad Hoc Teamwork.pdf:PDF},
  groups    = {Ad Hoc Task Allocation},
  url       = {https://eprints.lancs.ac.uk/id/eprint/144074},
}

@InProceedings{Hong2018ADeep,
  author           = {Hong, Zhang-Wei and Su, Shih-Yang and Shann, Tzu-Yun and Chang, Yi-Hsiang and Lee, Chun-Yi},
  booktitle        = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
  title            = {A Deep Policy Inference Q-Network for Multi-Agent Systems},
  year             = {2018},
  address          = {Richland, SC},
  pages            = {1388--1396},
  publisher        = {International Foundation for Autonomous Agents and Multiagent Systems},
  series           = {AAMAS '18},
  abstract         = {We present DPIQN, a deep policy inference Q-network that targets  multi-agent systems composed of controllable agents, collaborators, and opponents that interact with each other. We focus on one  challenging issue in such systems—modeling agents with varying  strategies—and propose to employ “policy features” learned from  raw observations (e.g., raw images) of collaborators and opponents  by inferring their policies. DPIQN incorporates the learned policy  features as a hidden vector into its own deep Q-network (DQN),  such that it is able to predict better Q values for the controllable  agents than the state-of-the-art deep reinforcement learning models.
We further propose an enhanced version of DPIQN, called deep recurrent policy inference Q-network (DRPIQN), for handling partial  observability. Both DPIQN and DRPIQN are trained by an adaptive  training procedure, which adjusts the network’s attention to learn  the policy features and its own Q-values at different phases of the  training process. We present a comprehensive analysis of DPIQN  and DRPIQN, and highlight their effectiveness and generalizability in various multi-agent settings. Our models are evaluated in a  classic soccer game involving both competitive and collaborative  scenarios. Experimental results performed on 1 vs. 1 and 2 vs. 2  games show that DPIQN and DRPIQN demonstrate superior performance to the baseline DQN and deep recurrent Q-network (DRQN)  models. We also explore scenarios in which collaborators or opponents dynamically change their policies, and show that DPIQN and
DRPIQN do lead to better overall performance in terms of stability  and mean scores.},
  doi              = {10.5555/3237383.3237907},
  file             = {:Hong_2018_A Deep Policy Inference Q Network for Multi Agent Systems.pdf:PDF},
  keywords         = {opponent modeling, multi-agent learning, deep reinforcement learning},
  location         = {Stockholm, Sweden},
  modificationdate = {2022-06-15T15:58:55},
  numpages         = {9},
  url              = {https://dl.acm.org/doi/10.5555/3237383.3237907},
}

@InProceedings{Chakraborty2013Cooperating,
  author           = {Chakraborty, Doran and Stone, Peter},
  booktitle        = {Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems},
  title            = {Cooperating with a Markovian Ad Hoc Teammate},
  year             = {2013},
  address          = {Richland, SC},
  pages            = {1085--1092},
  publisher        = {International Foundation for Autonomous Agents and Multiagent Systems},
  series           = {AAMAS '13},
  abstract         = {This paper focuses on learning in the presence of a Markovian teammate in Ad hoc teams. A Markovian teammate's policy is a function of a set of discrete feature values derived from the joint history of interaction, where the feature values transition in a Markovian fashion on each time step. We introduce a novel algorithm "Learning to Cooperate with a Markovian teammate", or LCM, that converges to optimal cooperation with any Markovian teammate, and achieves safety with any arbitrary teammate. The novel aspect of LCM is the manner in which it satisfies the above two goals via efficient exploration and exploitation. The main contribution of this paper is a full specification and a detailed analysis of LCM's theoretical properties.},
  doi              = {10.5555/2484920.2485091},
  file             = {:Ad_Hoc_Teamwork/Chakraborty_2013_Cooperating with a Markovian Ad Hoc Teammate.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  isbn             = {9781450319935},
  keywords         = {learning, ad hoc teamwork, sample complexity analysist},
  location         = {St. Paul, MN, USA},
  modificationdate = {2022-06-15T16:06:13},
  numpages         = {8},
  url              = {https://dl.acm.org/doi/10.5555/2484920.2485091},
}

@Article{Lv2021Recruitment,
  author           = {Shuai Lü and Shuai Han and Wenbo Zhou and Junwei Zhang},
  journal          = {Information Sciences},
  title            = {Recruitment-imitation mechanism for evolutionary reinforcement learning},
  year             = {2021},
  month            = apr,
  pages            = {172--188},
  volume           = {553},
  abstract         = {Reinforcement learning, evolutionary algorithms and imitation learning are three principal methods to deal with continuous control tasks. Reinforcement learning is sample efficient, yet sensitive to hyperparameters settings and needs efficient exploration; Evolutionary algorithms are stable, but with low sample efficiency; Imitation learning is both sample efficient and stable, however it requires the guidance of expert data. In this paper, we propose Recruitment-imitation Mechanism (RIM) for evolutionary reinforcement learning, a scalable framework that combines advantages of the three methods mentioned above. The core of this framework is a dual-actors and single critic reinforcement learning agent. This agent can recruit high-fitness actors from the population performing evolutionary algorithms, which instructs itself to learn from experience replay buffer. At the same time, low-fitness actors in the evolutionary population can imitate behavior patterns of the reinforcement learning agent and promote their fitness level. Reinforcement and imitation learners in this framework can be replaced with any off-policy actor-critic reinforcement learner and data-driven imitation learner. We evaluate RIM on a series of benchmarks for continuous control tasks in Mujoco. The experimental results show that RIM outperforms prior evolutionary or reinforcement learning methods. The performance of RIM’s components is significantly better than components of previous evolutionary reinforcement learning algorithm, and the recruitment using soft update enables reinforcement learning agent to learn faster than that using hard update.},
  doi              = {10.1016/j.ins.2020.12.017},
  file             = {:Lü_2021_Recruitment Imitation Mechanism for Evolutionary Reinforcement Learning.pdf:PDF},
  modificationdate = {2022-03-30T12:46:04},
  publisher        = {Elsevier {BV}},
}

@Article{Wang2021Multiagent,
  author           = {Huimu Wang and Zhen Liu and Jianqiang Yi and Zhiqiang Pu},
  journal          = {Algorithms},
  title            = {Multiagent Hierarchical Cognition Difference Policy for Multiagent Cooperation},
  year             = {2021},
  month            = mar,
  number           = {3},
  pages            = {98},
  volume           = {14},
  abstract         = {Multiagent cooperation is one of the most attractive research fields in multiagent systems. There are many attempts made by researchers in this field to promote cooperation behavior.
However, several issues still exist, such as complex interactions among different groups of agents, redundant communication contents of irrelevant agents, which prevents the learning and convergence of agent cooperation behaviors. To address the limitations above, a novel method called multiagent hierarchical cognition difference policy (MA-HCDP) is proposed in this paper. It includes a hierarchical group network (HGN), a cognition difference network (CDN), and a soft communication network (SCN). HGN is designed to distinguish different underlying information of diverse groups’ observations (including friendly group, enemy group, and object group) and extract different high-dimensional state representations of different groups. CDN is designed based on a variational auto-encoder to allow each agent to choose its neighbors (communication targets) adaptively with its environment cognition difference. SCN is designed to handle the complex interactions among the agents with a soft attention mechanism. The results of simulations demonstrate the superior effectiveness of our method compared with existing methods.},
  doi              = {10.3390/a14030098},
  file             = {:Wang_2021_Multiagent Hierarchical Cognition Difference Policy for Multiagent Cooperation.pdf:PDF},
  modificationdate = {2022-06-15T16:02:57},
  publisher        = {{MDPI} {AG}},
}

@Article{Bolsover2018Chinese,
  author           = {Gillian Bolsover and Philip Howard},
  journal          = {INFORMATION, COMMUNICATION \& SOCIETY},
  title            = {Chinese computational propaganda: automation, algorithms and the manipulation of information about Chinese politics on Twitter and Weibo},
  year             = {2018},
  month            = may,
  number           = {14},
  pages            = {2063--2080},
  volume           = {22},
  doi              = {10.1080/1369118x.2018.1476576},
  file             = {:Bolsover_2018_Chinese Computational Propaganda_ Automation, Algorithms and the Manipulation of Information about Chinese Politics on Twitter and Weibo.pdf:PDF},
  modificationdate = {2022-06-15T16:03:19},
  publisher        = {Informa {UK} Limited},
}

@InProceedings{Xing2021Learning,
  author           = {Dong Xing and Qianhui Liu and Qian Zheng and Gang Pan},
  booktitle        = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence},
  title            = {Learning with Generated Teammates to Achieve Type-Free Ad-Hoc Teamwork},
  year             = {2021},
  month            = aug,
  publisher        = {International Joint Conferences on Artificial Intelligence Organization},
  abstract         = {In ad-hoc teamwork, an agent is required to cooperate with unknown teammates without prior coordination. To swiftly adapt to an unknown teammate, most works adopt a type-based approach, which pre-trains the agent with a set of pre-prepared teammate types, then associates the unknown teammate with a particular type. Typically, these types are collected manually. This hampers previous works by both the availability and diversity of types they manage to obtain. To eliminate these limitations, this work addresses to achieve ad-hoc teamwork in a type-free approach. Specifically, we propose the model of Entropy-regularized Deep Recurrent Q-Network (EDRQN) to generate teammates automatically, meanwhile utilize them to pre-train our agent. These teammates are obtained from scratch and are designed to perform the task with various behaviors, therefore their availability and diversity are both ensured. We evaluate our model on several benchmark domains of ad-hoc teamwork. The result shows that even if our model has no access to any pre-prepared teammate types, it still achieves significant performance.},
  doi              = {10.24963/ijcai.2021/66},
  file             = {:./Ad_Hoc_Teamwork/Xing_2021_Learning with Generated Teammates to Achieve Type Free Ad Hoc Teamwork.pdf:PDF;:Ad_Hoc_Teamwork/(poster) Xing_2021_Learning with Generated Teammates to Achieve Type Free Ad Hoc Teamwork.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  modificationdate = {2022-06-15T16:03:23},
}

@Misc{Ni2021Adaptive,
  author           = {Tianwei Ni and Huao Li and Siddharth Agrawal and Suhas Raja and Fan Jia and Yikang Gui and Dana Hughes and Michael Lewis and Katia Sycara},
  month            = mar,
  title            = {Adaptive Agent Architecture for Real-time Human-Agent Teaming},
  year             = {2021},
  abstract         = {Teamwork is a set of interrelated reasoning, actions and behaviors of team members that facilitate common objectives. Teamwork theory and experiments have resulted in a set of states and processes for team effectiveness in both human-human and agent-agent teams. However, human-agent teaming is less well studied because it is so new and involves asymmetry in policy and intent not present in human teams. To optimize team performance in human-agent teaming, it is critical that agents infer human intent and adapt their polices for smooth coordination. Most literature in human-agent teaming builds agents referencing a learned human model. Though these agents are guaranteed to perform well with the learned model, they lay heavy assumptions on human policy such as optimality and consistency, which is unlikely in many real-world scenarios. In this paper, we propose a novel adaptive agent architecture in human-model-free setting on a two-player cooperative game, namely Team Space Fortress (TSF). Previous human-human team research have shown complementary policies in TSF game and diversity in human players' skill, which encourages us to relax the assumptions on human policy. Therefore, we discard learning human models from human data, and instead use an adaptation strategy on a pre-trained library of exemplar policies composed of RL algorithms or rule-based methods with minimal assumptions of human behavior. The adaptation strategy relies on a novel similarity metric to infer human policy and then selects the most complementary policy in our library to maximize the team performance. The adaptive agent architecture can be deployed in real-time and generalize to any off-the-shelf static agents. We conducted human-agent experiments to evaluate the proposed adaptive agent framework, and demonstrated the suboptimality, diversity, and adaptability of human policies in human-agent teams.},
  archiveprefix    = {arXiv},
  eprint           = {2103.04439},
  file             = {:Ni_2021_Adaptive Agent Architecture for Real Time Human Agent Teaming.pdf:PDF},
  keywords         = {cs.RO, cs.AI, cs.HC},
  modificationdate = {2022-02-24T21:39:40},
  primaryclass     = {cs.RO},
}

@InProceedings{Kheirkhahan2013Effect,
  author           = {Matin Kheirkhahan and Mohammadreza Kangavari and Farimah Farahmandi},
  booktitle        = {2013 21st Iranian Conference on Electrical Engineering (ICEE)},
  title            = {Effect of observer agent on ad hoc teamwork in the pursuit domain},
  year             = {2013},
  month            = may,
  publisher        = {{IEEE}},
  abstract         = {As autonomous agents proliferate in the real world, both in software and robotic settings, they will increasingly need to band together for cooperative activities with previously unfamiliar teammates. Having autonomous agents capable of learning its previously unknown teammates’ preferences and acting in harmony with them to achieve the system’s end has been declared as a challenge to the AI. An agent capable of ad hoc teamwork is one that can effectively cooperate with multiple potential teammates on a set of collaborative tasks. This agent has to learn other agents’ behaviours and preferences in order to be able to predict their actions on some level. This paper presents an empirical study of ad hoc teamwork, taking advantage of Observer System’s merits. Specifically, we evaluate a learning mechanism for on-line behaviour generation on the part of a single ad hoc team agent that must collaborate with other unknown teammates in the pursuit domain.},
  doi              = {10.1109/iraniancee.2013.6599845},
  file             = {:Ad_Hoc_Teamwork/Kheirkhahan_2013_Effect of Observer Agent on Ad Hoc Teamwork in the Pursuit Domain.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  modificationdate = {2022-06-15T16:06:51},
}

@MastersThesis{Verhoeven2020Team,
  author = {Yannick Verhoeven},
  school = {University of Münster},
  title  = {Team Behavior of Artificial Intelligence Bots in Games},
  year   = {2020},
  month  = feb,
  file   = {:Verhoeven_2020_Team Behavior of Artificial Intelligence Bots in Games.pdf:PDF},
}

@InProceedings{Agmon2012Leading,
  author           = {Noa Agmon and Peter Stone},
  booktitle        = {Proc. of 12th Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS 2012)},
  title            = {Leading Ad Hoc Agents in Joint Action Settings with Multiple Teammates},
  year             = {2012},
  month            = jun,
  abstract         = {The growing use of autonomous agents in practice may require agents to cooperate as a team in situations where they have limited prior knowledge about one another, cannot communicate directly, or do not share the same world models. These situations raise the need to design ad hoc team members, i.e., agents that will be able to cooperate without coordination in order to reach an optimal team behavior.
This paper considers the problem of leading N-agent teams by an agent toward their optimal joint utility, where the agents compute their next actions based only on their most recent observations of their teammates’ actions. We show that compared to previous results in two-agent teams, in larger teams the agent might not be able to lead the team to the action with maximal joint utility, thus its optimal strategy is to lead the team to the best possible reachable cycle of joint actions. We describe a graphical model of the problem and a polynomial time algorithm for solving it.
We then consider other variations of the problem, including leading teams of agents where they base their actions on longer history of past observations, leading a team by more than one ad hoc agent, and leading a teammate while the ad hoc agent is uncertain of its behavior.},
  doi              = {10.5555/2343576.2343625},
  file             = {:Agmon_2012_Leading Ad Hoc Agents in Joint Action Settings with Multiple Teammates.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  modificationdate = {2022-06-15T16:03:51},
  url              = {http://www.cs.utexas.edu/users/ai-lab?AAMAS12-agmon},
}

@InCollection{Stone2010Leading,
  author           = {Peter Stone and Gal A. Kaminka and Jeffrey S. Rosenschein},
  publisher        = {Springer Berlin Heidelberg},
  title            = {Leading a Best-Response Teammate in an Ad Hoc Team},
  year             = {2010},
  pages            = {132--146},
  abstract         = {Teams of agents may not always be developed in a planned, coordinated fashion. Rather, as deployed agents become more common in e-commerce and other settings, there are increasing opportunities for previously unacquainted agents to cooperate in ad hoc team settings. In such scenarios, it is useful for individual agents to be able to collaborate with a wide variety of possible teammates under the philosophy that not all agents are fully rational. This paper considers an agent that is to interact repeatedly with a teammate that will adapt to this interaction in a particular suboptimal, but natural way. We formalize this setting in game-theoretic terms, provide and analyze a fully-implemented algorithm for finding optimal action sequences, prove some theoretical results pertaining to the lengths of these action sequences, and provide empirical results pertaining to the prevalence of our problem of interest in random interaction settings.},
  doi              = {10.1007/978-3-642-15117-0_10},
  file             = {:Stone_2010_Leading a Best Response Teammate in an Ad Hoc Team.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  modificationdate = {2022-03-29T22:03:05},
}

@InProceedings{Genter2013Ad,
  author           = {Katie {Genter} and Noa {Agmon} and Peter {Stone}},
  booktitle        = {Proceedings of the 2013 international conference on Autonomous agents and multi-agent systems},
  title            = {Ad hoc teamwork for leading a flock},
  year             = {2013},
  pages            = {531--538},
  abstract         = {Designing agents that can cooperate with other agents as a team, without prior coordination or explicit communication, is becoming more desirable as autonomous agents become more prevalent. In this paper we examine an aspect of the problem of leading teammates in an ad hoc teamwork setting, where the designed ad hoc agents lead the other teammates to a desired behavior that maximizes team utility. Specifically, we consider the problem of leading a flock of agents to a desired orientation using a subset of ad hoc agents. We examine the problem theoretically, and set bounds on the extent of influence the ad hoc agents can have on the team when the agents are stationary. We use these results to examine the complicated problem of orienting a stationary team to a desired orientation using a set of nonstationary ad hoc agents. We then provide an empirical evaluation of the suggested solution using our custom-designed simulator FlockSim.},
  comment          = {随着自主代理越来越普遍，设计可以与其他代理作为一个团队进行合作，而无需事先协调或明确沟通的代理变得越来越可取。在这篇论文中，我们研究了在临时团队环境中领导队友的问题的一个方面，其中设计的临时代理引导其他队友达到最大化团队效用的期望行为。具体地说，我们考虑的问题是使用一个特设代理的子集将一群代理引导到一个期望的方向。我们从理论上研究了这个问题，并设定了当代理静止时，临时代理对团队的影响程度的界限。我们使用这些结果来检查一个复杂的问题，即使用一组非平稳的临时代理将一个平稳的团队导向一个期望的方向。然后，我们使用我们的定制设计提供建议的解决方案的实证评估羊群模拟模拟器。},
  file             = {:Ad_Hoc_Teamwork/Genter_2013_Ad Hoc Teamwork for Leading a Flock.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  modificationdate = {2022-12-07T11:18:57},
  notes            = {Sourced from Microsoft Academic - https://academic.microsoft.com/paper/1832000881},
}

@Article{Stone2013Improving,
  author           = {Katie Genter and Noa Agmon and Peter {Stone}},
  journal          = {AAMAS Autonomous Robots and Multirobot Systems Workshop (ARMS13)},
  title            = {Improving Efficiency of Leading a Flock in Ad Hoc Teamwork Settings},
  year             = {2013},
  month            = may,
  abstract         = {Designing robots that can influence their teammates to behave in a particular manner is desirable -- especially if the robots are able to do this without prior coordination or explicit communication. In this paper, we examine an aspect of the problem of leading teammates in an ad hoc teamwork setting. Specifically, we consider how to best utilize ad hoc agents to lead a flock to a desired orientation. Specifically, we extend upon recent work by presenting a more efficient search methodology for determining how the ad hoc agents should orient themselves to optimally influence a flock.},
  file             = {:Genter_2013_Improving Efficiency of Leading a Flock in Ad Hoc Teamwork Settings.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  modificationdate = {2022-03-30T12:35:44},
  notes            = {Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2198563208},
}

@Misc{Sinha2015Learning,
  author           = {Arunesh Sinha and Debarun Kar and Milind Tambe},
  month            = oct,
  title            = {Learning Adversary Behavior in Security Games: A PAC Model Perspective},
  year             = {2015},
  abstract         = {Recent applications of Stackelberg Security Games (SSG), from wildlife crime to urban crime, have employed machine learning tools to learn and predict adversary behavior using available data about defender-adversary interactions. Given these recent developments, this paper commits to an approach of directly learning the response function of the adversary. Using the PAC model, this paper lays a firm theoretical foundation for learning in SSGs (e.g., theoretically answer questions about the numbers of samples required to learn adversary behavior) and provides utility guarantees when the learned adversary model is used to plan the defender's strategy. The paper also aims to answer practical questions such as how much more data is needed to improve an adversary model's accuracy. Additionally, we explain a recently observed phenomenon that prediction accuracy of learned adversary behavior is not enough to discover the utility maximizing defender strategy. We provide four main contributions: (1) a PAC model of learning adversary response functions in SSGs; (2) PAC-model analysis of the learning of key, existing bounded rationality models in SSGs; (3) an entirely new approach to adversary modeling based on a non-parametric class of response functions with PAC-model analysis and (4) identification of conditions under which computing the best defender strategy against the learned adversary behavior is indeed the optimal strategy. Finally, we conduct experiments with real-world data from a national park in Uganda, showing the benefit of our new adversary modeling approach and verification of our PAC model predictions.},
  archiveprefix    = {arXiv},
  eprint           = {1511.00043},
  file             = {:security_game/Sinha_2015_Learning Adversary Behavior in Security Games_ a PAC Model Perspective.pdf:PDF},
  groups           = {Security Game},
  keywords         = {cs.AI, cs.GT, cs.LG},
  modificationdate = {2022-03-20T10:47:45},
  primaryclass     = {cs.AI},
}

@Article{Askari2019Behavioral,
  author           = {Gholamreza Askari and Madjid Eshaghi Gordji and Choonkil Park},
  journal          = {Palgrave Communications},
  title            = {The behavioral model and game theory},
  year             = {2019},
  month            = may,
  number           = {1},
  volume           = {5},
  doi              = {10.1057/s41599-019-0265-2},
  file             = {:security_game/Askari_2019_The Behavioral Model and Game Theory.pdf:PDF},
  groups           = {Security Game},
  modificationdate = {2022-06-15T16:04:02},
  publisher        = {Springer Science and Business Media {LLC}},
  url              = {https://doi.org/10.1057/s41599-019-0265-2},
}

@InProceedings{Kar2015Thrones,
  author           = {Kar, Debarun and Fang, Fei and Delle Fave, Francesco and Sintov, Nicole and Tambe, Milind},
  booktitle        = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
  title            = {"A Game of Thrones": When Human Behavior Models Compete in Repeated Stackelberg Security Games},
  year             = {2015},
  address          = {Richland, SC},
  month            = may,
  pages            = {1381--1390},
  publisher        = {International Foundation for Autonomous Agents and Multiagent Systems},
  series           = {AAMAS '15},
  abstract         = {Several competing human behavior models have been proposed to model and protect against boundedly rational adversaries in repeated Stackelberg security games (SSGs). However, these existing models fail to address three main issues which are extremely detrimental to defender performance. First, while they attempt to learn adversary behavior models from adversaries' past actions ("attacks on targets"), they fail to take into account adversaries' future adaptation based on successes or failures of these past actions. Second, they assume that sufficient data in the initial rounds will lead to a reliable model of the adversary. However, our analysis reveals that the issue is not the amount of data, but that there just is not enough of the attack surface exposed to the adversary to learn a reliable model. Third, current leading approaches have failed to include probability weighting functions, even though it is well known that human beings' weighting of probability is typically nonlinear. The first contribution of this paper is a new human behavior model, SHARP, which mitigates these three limitations as follows: (i) SHARP reasons based on success or failure of the adversary's past actions on exposed portions of the attack surface to model adversary adaptiveness; (ii) SHARP reasons about similarity between exposed and unexposed areas of the attack surface, and also incorporates a discounting parameter to mitigate adversary's lack of exposure to enough of the attack surface; and (iii) SHARP integrates a non-linear probability weighting function to capture the adversary's true weighting of probability.
Our second contribution is a first "longitudinal study" -- at least in the context of SSGs -- of competing models in settings involving repeated interaction between the attacker and the defender. This study, where each experiment lasted a period of multiple weeks with individual sets of human subjects, illustrates the strengths and weaknesses of different models and shows the advantages of SHARP.},
  doi              = {10.5555/2772879.2773329},
  file             = {:security_game/Kar_2015__A Game of Thrones__ When Human Behavior Models Compete in Repeated Stackelberg Security Games.pdf:PDF},
  groups           = {Security Game},
  isbn             = {9781450334136},
  keywords         = {amazon mechanical turk, longitudinal experiments, human behavior modeling, game theory, repeated stackelberg games},
  location         = {Istanbul, Turkey},
  modificationdate = {2023-01-07T21:47:22},
  numpages         = {10},
}

@InProceedings{Kar2017Cloudy,
  author           = {Kar, Debarun and Ford, Benjamin and Gholami, Shahrzad and Fang, Fei and Plumptre, Andrew and Tambe, Milind and Driciru, Margaret and Wanyama, Fred and Rwetsiba, Aggrey and Nsubaga, Mustapha and Mabonga, Joshua},
  booktitle        = {Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
  title            = {Cloudy with a Chance of Poaching: Adversary Behavior Modeling and Forecasting with Real-World Poaching Data},
  year             = {2017},
  address          = {Richland, SC},
  month            = jul,
  pages            = {159--167},
  publisher        = {International Foundation for Autonomous Agents and Multiagent Systems},
  series           = {AAMAS '17},
  abstract         = {Wildlife conservation organizations task rangers to deter and capture wildlife poachers.
Since rangers are responsible for patrolling vast areas, adversary behavior modeling
can help more effectively direct future patrols. In this innovative application track
paper, we present an adversary behavior modeling system, INTERCEPT (INTERpretable
Classification Ensemble to Protect Threatened species), and provide the most extensive
evaluation in the AI literature of one of the largest poaching datasets from Queen
Elizabeth National Park (QENP) in Uganda, comparing INTERCEPT with its competitors;
we also present results from a month-long test of INTERCEPT in the field. We present
three major contributions. First, we present a paradigm shift in modeling and forecasting
wildlife poacher behavior. Some of the latest work in the AI literature (and in Conservation)
has relied on models similar to the Quantal Response model from Behavioral Game Theory
for poacher behavior prediction. In contrast, INTERCEPT presents a behavior model
based on an ensemble of decision trees (i) that more effectively predicts poacher
attacks and (ii) that is more effectively interpretable and verifiable. We augment
this model to account for spatial correlations and construct an ensemble of the best
models, significantly improving performance. Second, we conduct an extensive evaluation
on the QENP dataset, comparing 41 models in prediction performance over two years.
Third, we present the results of deploying INTERCEPT for a one-month field test in
QENP - a first for adversary behavior modeling applications in this domain. This field
test has led to finding a poached elephant and more than a dozen snares (including
a roll of elephant snares) before they were deployed, potentially saving the lives
of multiple animals - including elephants.},
  file             = {:security_game/Kar_2017_Cloudy with a Chance of Poaching_ Adversary Behavior Modeling and Forecasting with Real World Poaching Data.pdf:PDF},
  groups           = {Security Game},
  keywords         = {wildlife conservation, human behavior modeling, deployed applications, innovative applications},
  location         = {S\~{a}o Paulo, Brazil},
  modificationdate = {2023-01-07T21:59:37},
  numpages         = {9},
  url              = {http://dl.acm.org/citation.cfm?id=3091153},
}

@Misc{Bester2019MultiPassQ,
  author           = {Craig J. Bester and Steven D. James and George D. Konidaris},
  month            = may,
  title            = {Multi-Pass Q-Networks for Deep Reinforcement Learning with Parameterised Action Spaces},
  year             = {2019},
  abstract         = {Parameterised actions in reinforcement learning are composed of discrete actions with continuous action-parameters. This provides a framework for solving complex domains that require combining high-level actions with flexible control. The recent P-DQN algorithm extends deep Q-networks to learn over such action spaces. However, it treats all action-parameters as a single joint input to the Q-network, invalidating its theoretical foundations. We analyse the issues with this approach and propose a novel method, multi-pass deep Q-networks, or MP-DQN, to address them. We empirically demonstrate that MP-DQN significantly outperforms P-DQN and other previous algorithms in terms of data efficiency and converged policy performance on the Platform, Robot Soccer Goal, and Half Field Offense domains.},
  archiveprefix    = {arXiv},
  eprint           = {1905.04388},
  file             = {:Half_Field_Offense/Bester_2019_Multi Pass Q Networks for Deep Reinforcement Learning with Parameterised Action Spaces.pdf:PDF},
  keywords         = {cs.LG, stat.ML},
  modificationdate = {2022-03-20T10:32:31},
  primaryclass     = {cs.LG},
  url              = {https://github.com/cycraig/MP-DQN},
}

@Misc{Tafazzol2021Curious,
  author           = {Saeed Tafazzol and Erfan Fathi and Mahdi Rezaei and Ehsan Asali},
  month            = may,
  title            = {Curious Exploration and Return-based Memory Restoration for Deep Reinforcement Learning},
  year             = {2021},
  abstract         = {Reward engineering and designing an incentive reward function are non-trivial tasks to train agents in complex environments. Furthermore, an inaccurate reward function may lead to a biased behaviour which is far from an efficient and optimised behaviour. In this paper, we focus on training a single agent to score goals with binary success/failure reward function in Half Field Offense domain. As the major advantage of this research, the agent has no presumption about the environment which means it only follows the original formulation of reinforcement learning agents. The main challenge of using such a reward function is the high sparsity of positive reward signals. To address this problem, we use a simple prediction-based exploration strategy (called Curious Exploration) along with a Return-based Memory Restoration (RMR) technique which tends to remember more valuable memories. The proposed method can be utilized to train agents in environments with fairly complex state and action spaces. Our experimental results show that many recent solutions including our baseline method fail to learn and perform in complex soccer domain. However, the proposed method can converge easily to the nearly optimal behaviour. The video presenting the performance of our trained agent is available at http://bit.ly/HFO_Binary_Reward.},
  archiveprefix    = {arXiv},
  eprint           = {2105.00499},
  file             = {:Half_Field_Offense/Tafazzol_2021_Curious Exploration and Return Based Memory Restoration for Deep Reinforcement Learning.pdf:PDF},
  keywords         = {cs.RO, cs.AI, cs.LG, cs.MA},
  modificationdate = {2022-03-20T10:48:25},
  primaryclass     = {cs.RO},
  url              = {https://github.com/SaeedTafazzol/curious_explorer},
}

@InProceedings{Bowling2005Coordination,
  author           = {Michael H. Bowling and Peter McCracken},
  booktitle        = {Proceedings of the 20th national conference on Artificial intelligence},
  title            = {Coordination and Adaptation in Impromptu Teams},
  year             = {2005},
  editor           = {Manuela M. Veloso and Subbarao Kambhampati},
  pages            = {53--58},
  publisher        = {{AAAI} Press / The {MIT} Press},
  volume           = {1},
  abstract         = {Coordinating a team of autonomous agents is one of the major challenges in building effective multiagent systems. Many techniques have been devised for this problem, and coordinated teamwork has been demonstrated even in highly dynamic and adversarial environments. A key assumption of these techniques, though, is that the team members are developed together as a whole. In many multiagent scenarios, this assumption is violated. We study the problem of coordination in impromptu teams, where a team is composed of independent agents each unknown to the others. The team members have their own skills, models, strategies, and coordination mechanisms, and no external organization is imposed upon them. In particular, we propose two techniques, one adaptive and one predictive, for coordinating a single agent that joins an unknown team of existing agents. We experimentally evaluate these mechanisms in the robot soccer domain, while introducing useful baselines for evaluating the performance of impromptu teams. We show some encouraging success while demonstrating this is a very fertile area of research.},
  bibsource        = {dblp computer science bibliography, https://dblp.org},
  biburl           = {https://dblp.org/rec/conf/aaai/BowlingM05.bib},
  file             = {:Ad_Hoc_Teamwork/Bowling_2005_Coordination and Adaptation in Impromptu Teams.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  modificationdate = {2023-01-07T22:03:56},
  timestamp        = {Wed, 10 Feb 2021 08:43:01 +0100},
  url              = {http://www.aaai.org/Library/AAAI/2005/aaai05-009.php},
}

@InProceedings{Chen2020AATEAM,
  author           = {Shuo Chen and Ewa Andrejczuk and Zhiguang Cao and Jie Zhang},
  booktitle        = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title            = {{AATEAM}: Achieving the Ad Hoc Teamwork by Employing the Attention Mechanism},
  year             = {2020},
  month            = apr,
  number           = {05},
  pages            = {7095--7102},
  publisher        = {Association for the Advancement of Artificial Intelligence ({AAAI})},
  volume           = {34},
  abstract         = {In the ad hoc teamwork setting, a team of agents needs to perform a task without prior coordination. The most advanced approach learns policies based on previous experiences and reuses one of the policies to interact with new teammates. However, the selected policy in many cases is sub-optimal. Switching between policies to adapt to new teammates' behaviour takes time, which threatens the successful performance of a task. In this paper, we propose AATEAM – a method that uses the attention-based neural networks to cope with new teammates' behaviour in real-time. We train one attention network per teammate type. The attention networks learn both to extract the temporal correlations from the sequence of states (i.e. contexts) and the mapping from contexts to actions. Each attention network also learns to predict a future state given the current context and its output action. The prediction accuracies help to determine which actions the ad hoc agent should take. We perform extensive experiments to show the effectiveness of our method.},
  comment          = {对于每个队友类型，训练一个attention网络，输入状态动作序列，给出每个动作的概率，相当于训练了一个针对该类型的策略。
然后用每个attention网络输出的h和t之间的距离，作为相似度权重，},
  doi              = {10.1609/aaai.v34i05.6196},
  file             = {:Ad_Hoc_Teamwork/Chen_2020_AATEAM_ Achieving the Ad Hoc Teamwork by Employing the Attention Mechanism.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  modificationdate = {2022-12-07T22:54:17},
  url              = {https://ojs.aaai.org//index.php/AAAI/article/view/6196},
}

@InProceedings{Sarratt2015Role,
  author           = {Sarratt, Trevor and Jhala, Arnav},
  booktitle        = {Proceedings of the Third Annual Conference on Advances in Cognitive Systems Poster Collection},
  title            = {The role of models and communication in the ad hoc multiagent team decision problem},
  year             = {2015},
  number           = {27},
  abstract         = {Ad hoc teams are formed of members who have little or no information regarding one another. In order to achieve a shared goal, agents are tasked with learning the capabilities of their teammates such that they can coordinate effectively. Typically, the capabilities of the agent teammates encountered are constrained by the particular domain specifications. However, for wide application, it is desirable to develop systems that are able to coordinate with general ad hoc agents independent of the choice of domain. We propose examining ad hoc multiagent teamwork from a generalized perspective and discuss existing domains within the context of our framework. Furthermore, we consider how communication of agent intentions can provide a means of reducing teammate model uncertainty at key junctures, requiring an agent to consider its own information deficiencies in order to form communicative acts improving team coordination.},
  file             = {:Ad_Hoc_Teamwork/AdHocCommunication/Sarratt_2015_The Role of Models and Communication in the Ad Hoc Multiagent Team Decision Problem.pdf:PDF},
  groups           = {Ad Hoc Communication},
  modificationdate = {2022-06-15T16:07:41},
}

@MastersThesis{Trivedi2016Inverse,
  author           = {Trivedi, Maulesh},
  school           = {University of Georgia},
  title            = {Inverse learning of robot behavior for ad-hoc teamwork},
  year             = {2016},
  abstract         = {Machine Learning and Robotics present a very intriguing combination of research in Artificial Intelligence. Inverse Reinforcement Learning (IRL) algorithms have generated a great deal of interest in the AI community in recent years. However, very little research has been done on modelling agent interactions in multi-robot ad-hoc settings after learning is complete. Moreover, incorporating IRL for practical robot environments that deal with online learning and high levels of uncertainty is a challenge. While decision theoretic frameworks used for planning in these environments provide good approximations for computing an optimal policy for an agent, these model parameters are usually specified by a human designer. We describe a unique Bayesian approach to approximate unknown state transition functions. We then propose a novel multi-agent Best Response Model that plugs in the expert’s reward structure learnt through Maximum Entropy Inverse Reinforcement Learning, and use the learnt transition functions from our Bayes Adaptive approach to compute an optimal best response policy for our multi-robot ad-hoc setting. We test our algorithms on a robot debris-sorting task.},
  file             = {:Ad_Hoc_Teamwork/Trivedi_2016_Inverse Learning of Robot Behavior for Ad Hoc Teamwork.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  keywords         = {Inverse Reinforcement Learning;Markov Decision Process;Bayes Adaptive Markov Decision Process;Best Response Model;Dec MDP;Optimal Policy;Reward Function},
  modificationdate = {2022-03-20T10:48:37},
}

@InProceedings{Sarratt2016Policy,
  author           = {Sarratt, Trevor and Jhala, Arnav},
  booktitle        = {Workshops at the Thirtieth AAAI Conference on Artificial Intelligence},
  title            = {Policy Communication for Coordination with Unknown Teammates},
  year             = {2016},
  publisher        = {{AAAI} Press},
  volume           = {WS-16-11},
  comment          = {提到了STEAM框架},
  file             = {:Ad_Hoc_Teamwork/AdHocCommunication/Sarratt_2016_Policy Communication for Coordination with Unknown Teammates.pdf:PDF},
  groups           = {Ad Hoc Communication},
  modificationdate = {2023-03-09T12:03:12},
  url              = {http://www.aaai.org/ocs/index.php/WS/AAAIW16/paper/view/12646},
}

@InProceedings{Song2018Multi,
  author    = {Song, Jiaming and Ren, Hongyu and Sadigh, Dorsa and Ermon, Stefano},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Multi-Agent Generative Adversarial Imitation Learning},
  year      = {2018},
  editor    = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {31},
  file      = {:Song_2018_Multi Agent Generative Adversarial Imitation Learning.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper/2018/file/240c945bb72980130446fc2b40fbb8e0-Paper.pdf},
}

@Misc{Rabinowitz2018Machine,
  author           = {Neil C. Rabinowitz and Frank Perbet and H. Francis Song and Chiyuan Zhang and S. M. Ali Eslami and Matthew Botvinick},
  month            = feb,
  title            = {Machine Theory of Mind},
  year             = {2018},
  abstract         = {Theory of mind (ToM; Premack & Woodruff, 1978) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We propose to train a machine to build such models too. We design a Theory of Mind neural network -- a ToMnet -- which uses meta-learning to build models of the agents it encounters, from observations of their behaviour alone. Through this process, it acquires a strong prior model for agents' behaviour, as well as the ability to bootstrap to richer predictions about agents' characteristics and mental states using only a small number of behavioural observations. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep reinforcement learning agents from varied populations, and that it passes classic ToM tasks such as the "Sally-Anne" test (Wimmer & Perner, 1983; Baron-Cohen et al., 1985) of recognising that others can hold false beliefs about the world. We argue that this system -- which autonomously learns how to model other agents in its world -- is an important step forward for developing multi-agent AI systems, for building intermediating technology for machine-human interaction, and for advancing the progress on interpretable AI.},
  archiveprefix    = {arXiv},
  eprint           = {1802.07740},
  file             = {:Rabinowitz_2018_Machine Theory of Mind.pdf:PDF},
  keywords         = {cs.AI},
  modificationdate = {2022-02-25T13:25:19},
  primaryclass     = {cs.AI},
}

@Article{Rosenfeld2017Intelligent,
  author           = {Ariel Rosenfeld and Noa Agmon and Oleg Maksimov and Sarit Kraus},
  journal          = {Artificial Intelligence},
  title            = {Intelligent agent supporting human{\textendash}multi-robot team collaboration},
  year             = {2017},
  month            = nov,
  pages            = {211--231},
  volume           = {252},
  abstract         = {The number of multi-robot systems deployed in field applications has risen dramatically over the years. Nevertheless, supervising and operating multiple robots simultaneously is a difficult task for a single operator to execute. In this article we propose a novel approach for utilizing automated advising agents in assisting an operator to better manage a team of multiple robots in complex environments. We introduce an advice provision methodology and exemplify its implementation using automated advising agents in two real-world human–multi-robot team collaboration tasks: the Search And Rescue (SAR) and the warehouse operation tasks. Our intelligent advising agents were evaluated through extensive field trials, with over 150 human operators using both simulated and physical mobile robots, and showed a significant improvement in the team's performance.},
  doi              = {10.1016/j.artint.2017.08.005},
  file             = {:Rosenfeld_2017_Intelligent Agent Supporting Human_multi Robot Team Collaboration.pdf:PDF},
  modificationdate = {2022-06-15T16:09:11},
  publisher        = {Elsevier {BV}},
}

@Article{Miyashita2020Analysis,
  author           = {Yuki Miyashita and Toshiharu Sugawara},
  journal          = {Applied Intelligence},
  title            = {Analysis of coordinated behavior structures with multi-agent deep reinforcement learning},
  year             = {2020},
  month            = sep,
  number           = {2},
  pages            = {1069--1085},
  volume           = {51},
  doi              = {10.1007/s10489-020-01832-y},
  file             = {:Miyashita_2020_Analysis of Coordinated Behavior Structures with Multi Agent Deep Reinforcement Learning.pdf:PDF},
  modificationdate = {2022-06-15T16:09:15},
  publisher        = {Springer Science and Business Media {LLC}},
}

@InProceedings{Suriadinata2021Reasoning,
  author           = {Jennifer Suriadinata and William Macke and Reuth Mirsky and Peter Stone},
  booktitle        = {Adaptive and learning Agents Workshop at AAMAS 2021},
  title            = {Reasoning about Human Behavior in Ad Hoc Teamwork},
  year             = {2021},
  address          = {London, UK},
  month            = may,
  abstract         = {Ad hoc teamwork is a decentralized multi-agent problem in which agents 
must collaborate online without pre-coordination.  An interesting challenge in 
ad hoc teammate design is working efficiently with human agents, which may 
require a model of how these agents behave in a team. In this paper, we 
investigate a scenario in which one of the teammates is a human, as part of a 
work in progress to construct an ad hoc teammate that can collaborate in mixed 
human-agent environments.  This paper presents an experiment that evaluates 
human behavior in ad hoc teamwork under three different conditions: A control 
group which is given a basic set of instructions and two treatment groups which
are given varying levels of additional information about the collaborative 
nature of the task. We measure the users' performance in terms of optimality 
and legibility. We show that these values are significantly different between 
the conditions, thus highlighting the importance of acquiring a model that 
encompasses different human behaviors.},
  file             = {:Suriadinata_2021_Reasoning about Human Behavior in Ad Hoc Teamwork.pdf:PDF},
  groups           = {Human-Agent Ad Hoc},
  modificationdate = {2022-09-03T10:15:06},
  ranking          = {rank5},
}

@Misc{Gupta2021HAMMER,
  author        = {Nikunj Gupta and G Srinivasaraghavan and Swarup Kumar Mohalik and Matthew E. Taylor},
  title         = {HAMMER: Multi-Level Coordination of Reinforcement Learning Agents via Learned Messaging},
  year          = {2021},
  month         = jan,
  abstract      = {Cooperative multi-agent reinforcement learning (MARL) has achieved significant results, most notably by leveraging the representation learning abilities of deep neural networks. However, large centralized approaches quickly become infeasible as the number of agents scale, and fully decentralized approaches can miss important opportunities for information sharing and coordination. Furthermore, not all agents are equal - in some cases, individual agents may not even have the ability to send communication to other agents or explicitly model other agents. This paper considers the case where there is a single, powerful, central agent that can observe the entire observation space, and there are multiple, low powered, local agents that can only receive local observations and cannot communicate with each other. The job of the central agent is to learn what message to send to different local agents, based on the global observations, not by centrally solving the entire problem and sending action commands, but by determining what additional information an individual agent should receive so that it can make a better decision. After explaining our MARL algorithm, hammer, and where it would be most applicable, we implement it in the cooperative navigation and multi-agent walker domains. Empirical results show that 1) learned communication does indeed improve system performance, 2) results generalize to multiple numbers of agents, and 3) results generalize to different reward structures.},
  archiveprefix = {arXiv},
  eprint        = {2102.00824},
  file          = {:Gupta_2021_HAMMER_ Multi Level Coordination of Reinforcement Learning Agents Via Learned Messaging.pdf:PDF},
  keywords      = {cs.MA, cs.AI, cs.LG},
  primaryclass  = {cs.MA},
}

@Article{Zeng2019Navigation,
  author           = {Junjie Zeng and Rusheng Ju and Long Qin and Yue Hu and Quanjun Yin and Cong Hu},
  journal          = {Sensors},
  title            = {Navigation in Unknown Dynamic Environments Based on Deep Reinforcement Learning},
  year             = {2019},
  month            = sep,
  number           = {18},
  volume           = {19},
  doi              = {10.3390/s19183837},
  file             = {:Zeng_2019_Navigation in Unknown Dynamic Environments Based on Deep Reinforcement Learning.pdf:PDF},
  modificationdate = {2022-06-15T16:04:45},
  publisher        = {{MDPI} {AG}},
  url              = {https://www.mdpi.com/1424-8220/19/18/3837},
}

@Misc{Tampuu2015IDQN,
  author           = {Ardi Tampuu and Tambet Matiisen and Dorian Kodelja and Ilya Kuzovkin and Kristjan Korjus and Juhan Aru and Jaan Aru and Raul Vicente},
  month            = nov,
  title            = {Multiagent Cooperation and Competition with Deep Reinforcement Learning},
  year             = {2015},
  abstract         = {Multiagent systems appear in most social, economical, and political situations. In the present work we extend the Deep Q-Learning Network architecture proposed by Google DeepMind to multiagent environments and investigate how two agents controlled by independent Deep Q-Networks interact in the classic videogame Pong. By manipulating the classical rewarding scheme of Pong we demonstrate how competitive and collaborative behaviors emerge. Competitive agents learn to play and score efficiently. Agents trained under collaborative rewarding schemes find an optimal strategy to keep the ball in the game as long as possible. We also describe the progression from competitive to collaborative behavior. The present work demonstrates that Deep Q-Networks can become a practical tool for studying the decentralized learning of multiagent systems living in highly complex environments.},
  archiveprefix    = {arXiv},
  comment          = {Indepent DQN},
  eprint           = {1511.08779},
  file             = {:Tampuu_2015_Multiagent Cooperation and Competition with Deep Reinforcement Learning.pdf:PDF},
  keywords         = {cs.AI, cs.LG, q-bio.NC},
  modificationdate = {2022-09-20T18:21:20},
  primaryclass     = {cs.AI},
}

@Misc{Lanctot2017Unified,
  author           = {Marc Lanctot and Vinicius Zambaldi and Audrunas Gruslys and Angeliki Lazaridou and Karl Tuyls and Julien Perolat and David Silver and Thore Graepel},
  month            = nov,
  title            = {A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning},
  year             = {2017},
  abstract         = {To achieve general intelligence, agents must learn how to interact with others in a shared environment: this is the challenge of multiagent reinforcement learning (MARL). The simplest form is independent reinforcement learning (InRL), where each agent treats its experience as part of its (non-stationary) environment. In this paper, we first observe that policies learned using InRL can overfit to the other agents' policies during training, failing to sufficiently generalize during execution. We introduce a new metric, joint-policy correlation, to quantify this effect. We describe an algorithm for general MARL, based on approximate best responses to mixtures of policies generated using deep reinforcement learning, and empirical game-theoretic analysis to compute meta-strategies for policy selection. The algorithm generalizes previous ones such as InRL, iterated best response, double oracle, and fictitious play. Then, we present a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally, we demonstrate the generality of the resulting policies in two partially observable settings: gridworld coordination games and poker.},
  archiveprefix    = {arXiv},
  eprint           = {1711.00832},
  file             = {:Lanctot_2017_A Unified Game Theoretic Approach to Multiagent Reinforcement Learning.pdf:PDF},
  keywords         = {cs.AI, cs.GT, cs.LG, cs.MA},
  modificationdate = {2022-03-20T10:44:13},
  primaryclass     = {cs.AI},
}

@Article{Barrett2017Making,
  author           = {Samuel Barrett and Avi Rosenfeld and Sarit Kraus and Peter Stone},
  journal          = {Artificial Intelligence},
  title            = {Making friends on the fly: Cooperating with new teammates},
  year             = {2017},
  month            = jan,
  pages            = {132--171},
  volume           = {242},
  abstract         = {Robots are being deployed in an increasing variety of environments for longer periods of time. As the number of robots grows, they will increasingly need to interact with other robots. Additionally, the number of companies and research laboratories producing these robots is increasing, leading to the situation where these robots may not share a common communication or coordination protocol. While standards for coordination and communication may be created, we expect that robots will need to additionally reason intelligently about their teammates with limited information. This problem motivates the area of ad hoc teamwork in which an agent may potentially cooperate with a variety of teammates in order to achieve a shared goal. This article focuses on a limited version of the ad hoc teamwork problem in which an agent knows the environmental dynamics and has had past experiences with other teammates, though these experiences may not be representative of the current teammates. To tackle this problem, this article introduces a new general-purpose algorithm, PLASTIC, that reuses knowledge learned from previous teammates or provided by experts to quickly adapt to new teammates. This algorithm is instantiated in two forms: 1) PLASTIC-Model – which builds models of previous teammates' behaviors and plans behaviors online using these models and 2) PLASTIC-Policy – which learns policies for cooperating with previous teammates and selects among these policies online. We evaluate PLASTIC on two benchmark tasks: the pursuit domain and robot soccer in the RoboCup 2D simulation domain. Recognizing that a key requirement of ad hoc teamwork is adaptability to previously unseen agents, the tests use more than 40 previously unknown teams on the first task and 7 previously unknown teams on the second. While PLASTIC assumes that there is some degree of similarity between the current and past teammates' behaviors, no steps are taken in the experimental setup to make sure this assumption holds. The teammates were created by a variety of independent developers and were not designed to share any similarities. Nonetheless, the results show that PLASTIC was able to identify and exploit similarities between its current and past teammates' behaviors, allowing it to quickly adapt to new teammates.},
  doi              = {10.1016/j.artint.2016.10.005},
  file             = {:Ad_Hoc_Teamwork/Barrett_2017_Making Friends on the Fly_ Cooperating with New Teammates.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  keywords         = {Ad hoc teamwork, Multiagent systems, Multiagent cooperation, Reinforcement learning, Pursuit domain, RoboCup soccer},
  modificationdate = {2022-06-15T16:04:07},
  publisher        = {Elsevier {BV}},
  url              = {https://www.sciencedirect.com/science/article/pii/S0004370216301266?via=ihub},
}

@Book{Barrett2015Making,
  author           = {Barrett, Samuel},
  publisher        = {Springer-Verlag GmbH},
  title            = {Making Friends on the Fly: Advances in Ad Hoc Teamwork},
  year             = {2015},
  isbn             = {9783319180694},
  month            = may,
  abstract         = {Given the continuing improvements in design and manufacturing processes in addition to improvements in artificial intelligence, robots are being deployed in an increasing variety of environments for longer periods of time. As the number of robots grows, it is expected that they will encounter and interact with other robots.
Additionally, the number of companies and research laboratories producing these robots is increasing, leading to the situation where these robots may not share a common communication or coordination protocol. While standards for coordination and communication may be created, we expect that any standards will lag behind the state-of-the-art protocols and robots will need to additionally reason intelligently about their teammates with limited information. This problem motivates the area of ad hoc teamwork in which an agent may potentially cooperate with a variety of teammates in order to achieve a shared goal. We argue that agents that effectively reason about ad hoc teamwork need to exhibit three capabilities: (1) robustness to teammate variety, (2) robustness to diverse tasks, and (3) fast adaptation. This book focuses on addressing all three of these challenges. In particular, this book introduces algorithms for quickly adapting to unknown teammates that enable agents to react to new teammates without extensive observations.
The majority of existing multiagent algorithms focus on scenarios where all agents share coordination and communication protocols. While previous research on ad hoc teamwork considers some of these three challenges, this book introduces a new algorithm, PLASTIC, that is the first to address all three challenges in a single algorithm. PLASTIC adapts quickly to unknown teammates by reusing knowledge it learns about previous teammates and exploiting any expert knowledge available. Given this knowledge, PLASTIC selects which previous teammates are most similar to the current ones online and uses this information to adapt to their behaviors. This book introduces two instantiations of PLASTIC. The first is a model-based approach, PLASTIC–Model, that builds models of previous teammates’ behaviors and plans online to determine the best course of action. The second uses a policy-based approach, PLASTIC–Policy, in which it learns policies for cooperating with past teammates and selects from among these policies online.
Furthermore, we introduce a new transfer learning algorithm, TwoStageTransfer, that allows transferring knowledge from many past teammates while considering how similar each teammate is to the current ones.
We theoretically analyze the computational tractability of PLASTIC–Model in a number of scenarios with unknown teammates. Additionally, we empirically evaluate PLASTIC in three domains that cover a spread of possible settings. Our evaluations show that PLASTIC can learn to communicate with unknown teammates using a limited set of messages, coordinate with externally-created teammates that do not reason about ad hoc teams, and act intelligently in domains with continuous states and actions. Furthermore, these evaluations show that
TwoStageTransfer outperforms existing transfer learning algorithms and enables
PLASTIC to adapt even better to new teammates. We also identify three dimensions that we argue best describe ad hoc teamwork scenarios. We hypothesize that these dimensions are useful for analyzing similarities among domains and determining which can be tackled by similar algorithms in addition to identifying avenues for future research. The work presented in this book represents an important step towards enabling agents to adapt to unknown teammates in the real world.
PLASTIC significantly broadens the robustness of robots to their teammates and allows them to quickly adapt to new teammates by reusing previously learned knowledge.},
  ean              = {9783319180694},
  file             = {:Ad_Hoc_Teamwork/Barrett_2015_Making Friends on the Fly_ Advances in Ad Hoc Teamwork.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  modificationdate = {2022-03-30T12:37:17},
  pagetotal        = {144},
  url              = {https://www.ebook.de/de/product/25041433/samuel_barrett_making_friends_on_the_fly_advances_in_ad_hoc_teamwork.html},
}

@PhdThesis{Santarra2019Communicating,
  author           = {Santarra, Trevor},
  school           = {UC Santa Cruz},
  title            = {Communicating Plans in Ad Hoc Multiagent Teams},
  year             = {2019},
  file             = {:Ad_Hoc_Teamwork/AdHocCommunication/Santarra_2019_Communicating Plans in Ad Hoc Multiagent Teams.pdf:PDF},
  groups           = {Ad Hoc Communication},
  modificationdate = {2022-03-20T10:47:16},
  url              = {https://escholarship.org/uc/item/9t52n2zt},
}

@InProceedings{Hausknecht2016Deep,
  author           = {Matthew Hausknecht and Peter Stone},
  booktitle        = {ICLR 2016 : International Conference on Learning Representations 2016},
  title            = {Deep Reinforcement Learning in Parameterized Action Space},
  year             = {2016},
  month            = may,
  file             = {:./Half_Field_Offense/Hausknecht_2016_Deep Reinforcement Learning in Parameterized Action Space.pdf:PDF},
  keywords         = {HFO},
  modificationdate = {2022-03-20T10:42:37},
  url              = {https://academic.microsoft.com/paper/2963616477},
}

@InProceedings{Haarnoja2018Soft,
  author           = {Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
  booktitle        = {Proceedings of the 35th International Conference on Machine Learning, {ICML} 2018},
  title            = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  year             = {2018},
  address          = {Stockholm, Sweden},
  editor           = {Jennifer G. Dy and Andreas Krause},
  month            = jul,
  pages            = {1856--1865},
  publisher        = {{PMLR}},
  series           = {Proceedings of Machine Learning Research},
  volume           = {80},
  biburl           = {https://dblp.org/rec/conf/icml/HaarnojaZAL18.bib},
  file             = {:Haarnoja_2018_Soft Actor Critic_ off Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.pdf:PDF},
  modificationdate = {2022-03-20T10:42:32},
  timestamp        = {Wed, 03 Apr 2019 18:17:30 +0200},
  url              = {http://proceedings.mlr.press/v80/haarnoja18b.html},
}

@Misc{Hernandez2020Selfplay,
  author    = {Hernandez, Daniel and Denamganaï, Kevin and Devlin, Sam and Samothrakis, Spyridon and Walker, James Alfred},
  title     = {A Comparison of Self-Play Algorithms Under a Generalized Framework},
  year      = {2020},
  doi       = {10.13140/RG.2.2.15492.76165},  
  archiveprefix = {arXiv},
  eprint    = {2006.04471},
  file      = {:Hernandez_2020_A Comparison of Self Play Algorithms under a Generalized Framework.pdf:PDF},
  keywords  = {self-play},
  language  = {en},
  publisher = {Unpublished},
}

@Misc{Shibata2021Deep,
  author           = {Kazuki Shibata and Tomohiko Jimbo and Takamitsu Matsubara},
  month            = mar,
  title            = {Deep reinforcement learning of event-triggered communication and control for multi-agent cooperative transport},
  year             = {2021},
  abstract         = {In this paper, we explore a multi-agent reinforcement learning approach to address the design problem of communication and control strategies for multi-agent cooperative transport. Typical end-to-end deep neural network policies may be insufficient for covering communication and control; these methods cannot decide the timing of communication and can only work with fixed-rate communications. Therefore, our framework exploits event-triggered architecture, namely, a feedback controller that computes the communication input and a triggering mechanism that determines when the input has to be updated again. Such event-triggered control policies are efficiently optimized using a multi-agent deep deterministic policy gradient. We confirmed that our approach could balance the transport performance and communication savings through numerical simulations.},
  archiveprefix    = {arXiv},
  eprint           = {2103.15260},
  file             = {:Shibata_2021_Deep Reinforcement Learning of Event Triggered Communication and Control for Multi Agent Cooperative Transport.pdf:PDF},
  keywords         = {cs.LG, cs.RO},
  modificationdate = {2022-03-20T10:47:40},
  primaryclass     = {cs.LG},
}

@Article{Souza2021Decentralized,
  author           = {Cristino de Souza and Rhys Newbury and Akansel Cosgun and Pedro Castillo and Boris Vidolov and Dana Kuli},
  journal          = {IEEE Robotics and Automation Letters},
  title            = {Decentralized Multi-Agent Pursuit Using Deep Reinforcement Learning},
  year             = {2021},
  month            = jul,
  number           = {3},
  pages            = {4552--4559},
  volume           = {6},
  abstract         = {Pursuit-evasion is the problem of capturing mobile targets with one or more pursuers. We use deep reinforcement learning for pursuing an omnidirectional target with multiple, homogeneous agents that are subject to unicycle kinematic constraints. We use shared experience to train a policy for a given number of pursuers, executed independently by each agent at run-time. The training uses curriculum learning, a sweeping-angle ordering to locally represent neighboring agents, and a reward structure that encourages a good formation and combines individual and group rewards. Simulated experiments with a reactive evader and up to eight pursuers show that our learning-based approach outperforms recent reinforcement learning techniques as well as nonholonomic adaptations of classical algorithms. The learned policy is successfully transferred to the real-world in a proof-of-concept demonstration with three motion-constrained pursuer drones.},
  doi              = {10.1109/lra.2021.3068952},
  file             = {:Souza_2021_Decentralized Multi Agent Pursuit Using Deep Reinforcement Learning.pdf:PDF},
  keywords         = {pursuit;},
  modificationdate = {2022-06-15T16:09:21},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
  url              = {https://ieeexplore.ieee.org/document/9387125},
}

@InProceedings{Hu2020OtherPlay,
  author           = {Hu, Hengyuan and Lerer, Adam and Peysakhovich, Alex and Foerster, Jakob},
  booktitle        = {Proceedings of the 37th International Conference on Machine Learning},
  title            = {“{O}ther-Play” for Zero-Shot Coordination},
  year             = {2020},
  editor           = {III, Hal Daumé and Singh, Aarti},
  month            = jul,
  pages            = {4399--4410},
  publisher        = {PMLR},
  series           = {Proceedings of Machine Learning Research},
  volume           = {119},
  abstract         = {We consider the problem of zero-shot coordination - constructing AI agents that can coordinate with novel partners they have not seen before (e.g.humans). Standard Multi-Agent Reinforcement Learning (MARL) methods typically focus on the self-play (SP) setting where agents construct strategies by playing the game with themselves repeatedly. Unfortunately, applying SP naively to the zero-shot coordination problem can produce agents that establish highly specialized conventions that do not carry over to novel partners they have not been trained with. We introduce a novel learning algorithm called other-play (OP), that enhances self-play by looking for more robust strategies. We characterize OP theoretically as well as experimentally. We study the cooperative card game Hanabi and show that OP agents achieve higher scores when paired with independently trained agents as well as with human players than SP agents.},
  file             = {:Hu_2020_“Other Play” for Zero Shot Coordination.pdf:PDF},
  modificationdate = {2022-03-28T14:56:14},
  url              = {https://proceedings.mlr.press/v119/hu20a.html},
}

@Misc{Alkoby2018AdhocMoral,
  author           = {Shani Alkoby and Avilash Rath and Peter Stone},
  month            = sep,
  title            = {Ad hoc Teamwork and Moral Feedback as a Framework for Safe Agent Behavior},
  year             = {2018},
  abstract         = {As technology develops, it is only a matter of time before agents will be capable of long term autonomy, i.e., will need to choose their actions by themselves for a long period of time. Thus, in many cases agents will not be able to be coordinated in advance with all other agents with which they may interact. Instead, agents will need to cooperate in order to accomplish unanticipated joint goals without pre-coordination. As a result, the "ad hoc teamwork" problem, in which teammates must work together to obtain a common goal without any prior agreement regarding how to do so, has emerged as a recent area of study in the AI literature. However, to date, no attention has been dedicated to the moral aspect of the agents' behavior, which is required to ensure that their actions' influences on other agents conform with social norms. In this research, we introduce the M-TAMER framework (a novel variant of TAMER) used to teach agents to act in accordance with human morality with respect to their teammates. Using a hybrid team (agents and people), if taking an action considered to be morally unacceptable, the agents will receive negative feedback from the human teammate(s). Using M-TAMER, agents will learn to act more consistently with respect to human morality.},
  archiveprefix    = {arXiv},
  eprint           = {1809.07880},
  file             = {:Ad_Hoc_Teamwork/Alkoby_2018_Ad Hoc Teamwork and Moral Feedback As a Framework for Safe Agent Behavior.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  keywords         = {cs.CY, 68T},
  modificationdate = {2022-06-15T16:06:01},
  primaryclass     = {cs.CY},
  ranking          = {rank5},
}

@Article{Kelly2018Discovering,
  author           = {Kelly, Stephen and Heywood, Malcolm I.},
  journal          = {IEEE Transactions on Games},
  title            = {Discovering {Agent} {Behaviors} {Through} {Code} {Reuse}: {Examples} {From} {Half}-{Field} {Offense} and {Ms}. {Pac}-{Man}},
  year             = {2018},
  month            = jun,
  number           = {2},
  pages            = {195--208},
  volume           = {10},
  abstract         = {This paper demonstrates how code reuse allows genetic programming (GP) to discover strategies for difficult gaming scenarios while maintaining relatively low model complexity. Critical factors in the proposed approach are illustrated through an in-depth study in two challenging task domains: RoboCup soccer and Ms. Pac-Man. In RoboCup, we demonstrate how policies initially evolved for simple subtasks can be reused, with no additional training or transfer function, in order to improve learning in the complex half-field offense (HFO) task. We then show how the same approach to code reuse can be applied directly in Ms. Pac-Man. In the latter case, the use of task-agnostic diversity maintenance removes the need to explicitly identify suitable subtasks a priori . The resulting GP policies achieve state-of-the-art levels of play in HFO and surpass scores previously reported in the Ms. Pac-Man literature, while employing less domain knowledge during training. Moreover, the highly modular policies discovered by GP are shown to be significantly less complex than state-of-the-art solutions in both domains. Throughout this paper, we pay special attention to a pair of task-agnostic diversity maintenance techniques, and empirically demonstrate their importance to the development of strong policies.},
  doi              = {10.1109/TCIAIG.2017.2766980},
  file             = {:Half_Field_Offense/Kelly_2018_Discovering Agent Behaviors through Code Reuse_ Examples from Half Field Offense and Ms. Pac Man.pdf:PDF},
  modificationdate = {2022-06-15T16:08:12},
  shorttitle       = {Discovering {Agent} {Behaviors} {Through} {Code} {Reuse}},
  url              = {https://ieeexplore.ieee.org/document/8085186/},
  urldate          = {2021-11-04},
}

@InProceedings{Kelly2015Knowledge,
  author           = {Kelly, Stephen and Heywood, Malcolm I.},
  booktitle        = {Proceedings of the 2015 {Annual} {Conference} on {Genetic} and {Evolutionary} {Computation}},
  title            = {Knowledge {Transfer} from {Keepaway} {Soccer} to {Half}-field {Offense} through {Program} {Symbiosis}: {Building} {Simple} {Programs} for a {Complex} {Task}},
  year             = {2015},
  address          = {Madrid Spain},
  month            = jul,
  pages            = {1143--1150},
  publisher        = {ACM},
  abstract         = {Half-field Offense (HFO) is a sub-task of Robocup 2D Simulated Soccer. HFO is a challenging, multi-agent machine learning problem in which a team of offense players attempt to manoeuvre the ball past a defending team and around the goalie in order to score. The agent's sensors and actuators are noisy, making the problem highly stochastic and partially observable. These same real-world characteristics have made Keepaway soccer, which represents one sub-task of HFO, a popular testbed in the reinforcement learning and task-transfer literature in particular. We demonstrate how policies initially evolved for Keepaway can be reused within a symbiotic framework for coevolving policies in genetic programming (GP), with no additional training or transfer function, in order to improve learning in the HFO task. Moreover, the highly modular policies discovered by GP are shown to be significantly less complex than solutions based on traditional value-function optimization while achieving the same level of play in HFO.},
  doi              = {10.1145/2739480.2754798},
  file             = {:Half_Field_Offense/Kelly_2015_Knowledge Transfer from Keepaway Soccer to Half Field Offense through Program Symbiosis_ Building Simple Programs for a Complex Task.pdf:PDF},
  isbn             = {9781450334723},
  language         = {en},
  modificationdate = {2022-03-20T10:43:08},
  shorttitle       = {Knowledge {Transfer} from {Keepaway} {Soccer} to {Half}-field {Offense} through {Program} {Symbiosis}},
  url              = {https://dl.acm.org/doi/10.1145/2739480.2754798},
  urldate          = {2021-11-04},
}

@InCollection{Kalyanakrishnan2007Half,
  author           = {Shivaram Kalyanakrishnan and Yaxin Liu and Peter Stone},
  publisher        = {Springer Berlin Heidelberg},
  title            = {Half Field Offense in {RoboCup} Soccer: A Multiagent Reinforcement Learning Case Study},
  year             = {2007},
  pages            = {72--85},
  abstract         = {We present half field offense, a novel subtask of RoboCup simulated soccer, and pose it as a problem for reinforcement learning. In this task, an offense team attempts to outplay a defense team in order to shoot goals. Half field offense extends keepaway [11], a simpler subtask of RoboCup soccer in which one team must try to keep possession of the ball within a small rectangular region, and away from the opposing team. Both keepaway and half field offense have to cope with the usual problems of RoboCup soccer, such as a continuous state space, noisy actions, and multiple agents, but the latter is a significantly harder multiagent reinforcement learning problem because of sparse rewards, a larger state space, a richer action set, and the sheer complexity of the policy to be learned. We demonstrate that the algorithm that has been successful for keepaway is inadequate to scale to the more complex half field offense task, and present a new algorithm to address the aforementioned problems in multiagent reinforcement learning. The main feature of our algorithm is the use of inter-agent communication, which allows for more frequent and reliable learning updates. We show empirical results verifying that our algorithm registers significantly higher performance and faster learning than the earlier approach. We also assess the contribution of inter-agent communication by considering several variations of the basic learning method. This work is a step further in the ongoing challenge to learn complete team behavior for the RoboCup simulated soccer task.},
  doi              = {10.1007/978-3-540-74024-7_7},
  file             = {:Half_Field_Offense/Kalyanakrishnan_2007_Half Field Offense in RoboCup Soccer_ a Multiagent Reinforcement Learning Case Study.pdf:PDF},
  modificationdate = {2022-03-20T10:42:56},
}

@Misc{Xiong2018Parametrized,
  author           = {Xiong, Jiechao and Wang, Qing and Yang, Zhuoran and Sun, Peng and Han, Lei and Zheng, Yang and Fu, Haobo and Zhang, Tong and Liu, Ji and Liu, Han},
  month            = oct,
  title            = {Parametrized {Deep} {Q}-{Networks} {Learning}: {Reinforcement} {Learning} with {Discrete}-{Continuous} {Hybrid} {Action} {Space}},
  year             = {2018},
  abstract         = {Most existing deep reinforcement learning (DRL) frameworks consider either discrete action space or continuous action space solely. Motivated by applications in computer games, we consider the scenario with discrete-continuous hybrid action space. To handle hybrid action space, previous works either approximate the hybrid space by discretization, or relax it into a continuous set. In this paper, we propose a parametrized deep Q-network (P- DQN) framework for the hybrid action space without approximation or relaxation. Our algorithm combines the spirits of both DQN (dealing with discrete action space) and DDPG (dealing with continuous action space) by seamlessly integrating them. Empirical results on a simulation example, scoring a goal in simulated RoboCup soccer and the solo mode in game King of Glory (KOG) validate the efficiency and effectiveness of our method.},
  eprint           = {1810.06394},
  file             = {:Xiong_2018_Parametrized Deep Q Networks Learning_ Reinforcement Learning with Discrete Continuous Hybrid Action Space.pdf:PDF},
  keywords         = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
  modificationdate = {2022-06-15T16:08:35},
  shorttitle       = {Parametrized {Deep} {Q}-{Networks} {Learning}},
  url              = {http://arxiv.org/abs/1810.06394},
  urldate          = {2021-11-09},
}

@InProceedings{Chen2019ATSIS,
  author           = {Shuo Chen and Ewa Andrejczuk and Athirai A. Irissappane and Jie Zhang},
  booktitle        = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19)},
  title            = {{ATSIS}: Achieving the Ad hoc Teamwork by Sub-task Inference and Selection},
  year             = {2019},
  month            = aug,
  pages            = {172--179},
  publisher        = {International Joint Conferences on Artificial Intelligence Organization},
  abstract         = {In an ad hoc teamwork setting, the team needs to coordinate their activities to perform a task without prior agreement on how to achieve it. The ad hoc agent cannot communicate with its teammates but it can observe their behaviour and plan accordingly. To do so, the existing approaches rely on the teammates' behaviour models. However, the models may not be accurate, which can compromise teamwork. For this reason, we present Ad Hoc Teamwork by Sub-task Inference and Selection (ATSIS) algorithm that uses a sub-task inference without relying on teammates' models. First, the ad hoc agent observes its teammates to infer which sub-tasks they are handling. Based on that, it selects its own sub-task using a partially observable Markov decision process that handles the uncertainty of the sub-task inference. Last, the ad hoc agent uses the Monte Carlo tree search to find the set of actions to perform the sub-task. Our experiments show the benefits of ATSIS for robust teamwork.},
  doi              = {10.24963/ijcai.2019/25},
  file             = {:Ad_Hoc_Teamwork/Chen_2019_ATSIS_ Achieving the Ad Hoc Teamwork by Sub Task Inference and Selection.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  keywords         = {Agent-based and Multi-agent Systems; Coordination and CooperationUncertainty in AI; Sequential Decision Making},
  modificationdate = {2023-01-07T21:58:54},
}

@PhdThesis{Wei2015Cognitive,
  author    = {Wei, C.},
  school    = {Delft University of Technology},
  title     = {Cognitive Coordination for Cooperative Multi-Robot Teamwork},
  year      = {2015},
  abstract  = {Multi-robot teams have potential advantages over a single robot. Robots in a team can serve different functionalities, so a team of robots can be more efficient, robust and reliable than a single robot. In this dissertation, we are in particular interested in human level intelligent multi-robot teams. Social deliberation should be taken into consideration in such a multi-robot system, which requires that the robots are capable of generating long term plans to achieve a global or team goal, rather than just dealing with the problems at hand. Robots in a team have to cope with dynamic environments due to the presence of the others. Thus, a robot cannot foresee what its environment will be because other robots may change the environment. Moreover, multiple robots may interfere with each other. We can say that the need for coordination in a robot team stems from interdependence relationships between the robots. More specifically, one robot performing an activity may influence another robot's activity. In order to achieve good team performance, the robots in a team all need to well coordinate their activities. This dissertation studies the multi-robot teamwork in the context of search and retrieval, which is known as foraging in robotics. In a foraging task, a team of robots is required to search targets of interest in the environment and also deliver them back to a home base. Many practical applications require search and retrieval such as urban search and rescue robots, deep-sea mining robots, and autonomous warehouse robots. Requiring both searching and delivering makes a foraging task more complicated than a pure searching, exploration or coverage task. Foraging robots have to consider not only where to explore but also when to explore. Coordination for a foraging task concerns how to direct the movements of the robots and how to distribute the workload more evenly in a team. In this dissertation, we first proposed an agent-based cognitive robot architecture that is used to bridge the gap between low-level robotic control with high-level cognitive reasoning. Cognitive agents realized by means of the agent programming language GOAL are used to control both real and simulated robots. We carried out an empirical study to investigate the role of communication and its impact on team performance. The results and findings were used to study the multi-robot pathfinding and multi-robot task allocation problems. A novel fully decentralized approach was proposed to deal with the multi-robot pathfinding problem, which also reduces the communication overhead, compared to usual decentralized approaches. An auction-based approach and a prediction approach were proposed to deal with the dynamic foraging task allocation problem. The difference is that the prediction approach performs better with respect to completion time, while the auction-based approach performs better with respect to travel costs. In order to facilitate the identification of interdependence relationships between the agents in the early design phase of a multi-agent system, we developed a formal domain-independent graphical language that reflects the need for coordination in multi-agent teamwork.},
  doi       = {10.4233/UUID:24FF01B3-CB96-4A30-8AB0-1B88F65CC9C7},
  file      = {:Wei_2015_Cognitive Coordination for Cooperative Multi Robot Teamwork.pdf:PDF},
  publisher = {Delft University of Technology},
}

@InProceedings{Li2019Integrating,
  author    = {Minglong Li and Wenjing Yang and Zhongxuan Cai and Shaowu Yang and Ji Wang},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, {IJCAI-19}},
  title     = {Integrating Decision Sharing with Prediction in Decentralized Planning for Multi-Agent Coordination under Uncertainty},
  year      = {2019},
  month     = aug,
  pages     = {450--456},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  abstract  = {The performance of decentralized multi-agent systems tends to benefit from information sharing and its effective utilization. However, too much or unnecessary sharing may hinder the performance due to the delay, instability and additional overhead of communications. Aiming to a satisfiable coordination performance, one would prefer the cost of communications as less as possible. In this paper, we propose an approach for improving the sharing utilization by integrating information sharing with prediction in decentralized planning. We present a novel planning algorithm by combining decision sharing and prediction based on decentralized Monte Carlo Tree Search called Dec-MCTS-SP. Each agent grows a search tree guided by the rewards calculated by the joint actions, which can not only be sampled from the shared probability distributions over action sequences, but also be predicted by a sufficiently-accurate and computationally-cheap heuristics-based method. Besides, several policies including sparse and discounted UCT and DIY-bonus are leveraged for performance improvement. We have implemented Dec-MCTS-SP in the case study on multi-agent information gathering under threat and uncertainty, which is formulated as Decentralized Partially Observable Markov Decision Process (Dec-POMDP). The factored belief vectors are integrated into Dec-MCTS-SP to handle the uncertainty. Comparing with the random, auction-based algorithm and Dec-MCTS, the evaluation shows that Dec-MCTS-SP can reduce communication cost significantly while still achieving a surprisingly higher coordination performance.},
  doi       = {10.24963/ijcai.2019/64},
  file      = {:Li_2019_Integrating Decision Sharing with Prediction in Decentralized Planning for Multi Agent Coordination under Uncertainty.pdf:PDF},
  url       = {https://doi.org/10.24963/ijcai.2019/64},
}

@Article{Li2019Swarm,
  author    = {Minglong Li and Wenjing Yang and Xiaodong Yi and Yanzhen Wang and Ji Wang},
  journal   = {Journal of Mechanical Engineering},
  title     = {Swarm Robot Task Planning Based on Air and Ground Coordination for Disaster Search and Rescue},
  year      = {2019},
  number    = {11},
  pages     = {1--9},
  volume    = {55},
  doi       = {10.3901/jme.2019.11.001},
  file      = {:Li_2019_Swarm Robot Task Planning Based on Air and Ground Coordination for Disaster Search and Rescue.pdf:PDF},
  publisher = {Chinese Journal of Mechanical Engineering},
}

@InProceedings{Li2021DecSGTS,
  author           = {Minglong Li and Zhongxuan Cai and Wenjing Yang and Lixia Wu and Yinghui Xu and Ji Wang},
  booktitle        = {Thirty-Fifth {AAAI} Conference on Artificial Intelligence, {AAAI} 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, {IAAI} 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2021, Virtual Event, February 2-9, 2021},
  title            = {Dec-SGTS: Decentralized Sub-Goal Tree Search for Multi-Agent Coordination},
  year             = {2021},
  month            = may,
  number           = {13},
  pages            = {11282--11289},
  publisher        = {{AAAI} Press},
  volume           = {35},
  abstract         = {Multi-agent coordination tends to benefit from efficient communication, where cooperation often happens based on exchanging information about what the agents intend to do, i.e. intention sharing. It becomes a key problem to model the intention by some proper abstraction. Currently, it is either too coarse such as final goals or too fined as primitive steps, which is inefficient due to the lack of modularity and semantics. In this paper, we design a novel multi-agent coordination protocol based on subgoal intentions, defined as the probability distribution over feasible subgoal sequences. The subgoal intentions encode macro-action behaviors with modularity so as to facilitate joint decision making at higher abstraction. Built over the proposed protocol, we present Dec-SGTS (Decentralized Sub-Goal Tree Search) to solve decentralized online multi-agent planning hierarchically and efficiently. Each agent runs Dec-SGTS asynchronously by iteratively performing three phases including local sub-goal tree search, local subgoal intention update and global subgoal intention sharing. We conduct the experiments on courier dispatching problem, and the results show that Dec-SGTS achieves much better reward while enjoying a significant reduction of planning time and communication cost compared with Dec-MCTS (Decentralized Monte Carlo Tree Search).},
  bibsource        = {dblp computer science bibliography, https://dblp.org},
  biburl           = {https://dblp.org/rec/conf/aaai/LiCYWXW21.bib},
  file             = {:Li_2021_Dec SGTS_ Decentralized Sub Goal Tree Search for Multi Agent Coordination.pdf:PDF},
  modificationdate = {2022-03-30T13:09:08},
  timestamp        = {Sat, 05 Jun 2021 18:11:55 +0200},
  url              = {https://ojs.aaai.org/index.php/AAAI/article/view/17345},
}

@InProceedings{Albrecht2017Reasoning,
  author           = {Albrecht, Stefano V. and Stone, Peter},
  booktitle        = {Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
  title            = {Reasoning about Hypothetical Agent Behaviours and Their Parameters},
  year             = {2017},
  address          = {Richland, SC},
  pages            = {547--555},
  publisher        = {International Foundation for Autonomous Agents and Multiagent Systems},
  series           = {AAMAS '17},
  abstract         = {Agents can achieve effective interaction with previously unknown other agents by maintaining beliefs over a set of hypothetical behaviours, or types, that these agents may have. A current limitation in this method is that it does not recognise parameters within type specifications, because types are viewed as blackbox mappings from interaction histories to probability distributions over actions. In this work, we propose a general method which allows an agent to reason about both the relative likelihood of types and the values of any bounded continuous parameters within types. The method maintains individual parameter estimates for each type and selectively updates the estimates for some types after each observation. We propose different methods for the selection of types and the estimation of parameter values. The proposed methods are evaluated in detailed experiments, showing that updating the parameter estimates of a single type after each observation can be sufficient to achieve good performance.},
  doi              = {10.5555/3091125.3091206},
  file             = {:Albrecht_2017_Reasoning about Hypothetical Agent Behaviours and Their Parameters.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  keywords         = {agent types, parameter learning, ad hoc teamwork},
  location         = {S\~{a}o Paulo, Brazil},
  modificationdate = {2022-06-15T15:59:06},
  numpages         = {9},
}

@Misc{Zhang2020CollaQ,
  author           = {Tianjun Zhang and Huazhe Xu and Xiaolong Wang and Yi Wu and Kurt Keutzer and Joseph E. Gonzalez and Yuandong Tian},
  month            = oct,
  title            = {Multi-Agent Collaboration via Reward Attribution Decomposition},
  year             = {2020},
  abstract         = {Recent advances in multi-agent reinforcement learning (MARL) have achieved super-human performance in games like Quake 3 and Dota 2. Unfortunately, these techniques require orders-of-magnitude more training rounds than humans and don't generalize to new agent configurations even on the same game. In this work, we propose Collaborative Q-learning (CollaQ) that achieves state-of-the-art performance in the StarCraft multi-agent challenge and supports ad hoc team play. We first formulate multi-agent collaboration as a joint optimization on reward assignment and show that each agent has an approximately optimal policy that decomposes into two parts: one part that only relies on the agent's own state, and the other part that is related to states of nearby agents. Following this novel finding, CollaQ decomposes the Q-function of each agent into a self term and an interactive term, with a Multi-Agent Reward Attribution (MARA) loss that regularizes the training. CollaQ is evaluated on various StarCraft maps and shows that it outperforms existing state-of-the-art techniques (i.e., QMIX, QTRAN, and VDN) by improving the win rate by 40% with the same number of samples. In the more challenging ad hoc team play setting (i.e., reweight/add/remove units without re-training or finetuning), CollaQ outperforms previous SoTA by over 30%.},
  archiveprefix    = {arXiv},
  comment          = {StarCraft},
  eprint           = {2010.08531},
  file             = {:Zhang_2020_Multi Agent Collaboration Via Reward Attribution Decomposition.pdf:PDF;:CollaQ_Multi_agent_Adhoc_sup.pdf:PDF},
  groups           = {RTS-MARL},
  keywords         = {cs.LG, cs.AI, cs.MA, stat.ML},
  modificationdate = {2022-06-06T16:58:20},
  primaryclass     = {cs.LG},
  url              = {https://openreview.net/forum?id=GVNGAaY2Dr1},
}

@Misc{Wang2016L2RL,
  author        = {Jane X Wang and Zeb Kurth-Nelson and Dhruva Tirumala and Hubert Soyer and Joel Z Leibo and Remi Munos and Charles Blundell and Dharshan Kumaran and Matt Botvinick},
  title         = {Learning to reinforcement learn},
  year          = {2016},
  month         = nov,
  abstract      = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
  archiveprefix = {arXiv},
  eprint        = {1611.05763},
  file          = {:Meta_Learning/Wang_2016_Learning to Reinforcement Learn.pdf:PDF},
  groups        = {Meta Learning},
  keywords      = {cs.LG, cs.AI, stat.ML},
  primaryclass  = {cs.LG},
}

@Misc{Kim2020Policy,
  author           = {Dong-Ki Kim and Miao Liu and Matthew Riemer and Chuangchuang Sun and Marwa Abdulhai and Golnaz Habibi and Sebastian Lopez-Cot and Gerald Tesauro and Jonathan P. How},
  month            = oct,
  title            = {A Policy Gradient Algorithm for Learning to Learn in Multiagent Reinforcement Learning},
  year             = {2020},
  abstract         = {A fundamental challenge in multiagent reinforcement learning is to learn beneficial behaviors in a shared environment with other simultaneously learning agents. In particular, each agent perceives the environment as effectively non-stationary due to the changing policies of other agents. Moreover, each agent is itself constantly learning, leading to natural non-stationarity in the distribution of experiences encountered. In this paper, we propose a novel meta-multiagent policy gradient theorem that directly accounts for the non-stationary policy dynamics inherent to multiagent learning settings. This is achieved by modeling our gradient updates to consider both an agent's own non-stationary policy dynamics and the non-stationary policy dynamics of other agents in the environment. We show that our theoretically grounded approach provides a general solution to the multiagent learning problem, which inherently comprises all key aspects of previous state of the art approaches on this topic. We test our method on a diverse suite of multiagent benchmarks and demonstrate a more efficient ability to adapt to new agents as they learn than baseline methods across the full spectrum of mixed incentive, competitive, and cooperative domains.},
  archiveprefix    = {arXiv},
  comment          = {non-stationary},
  eprint           = {2011.00382},
  file             = {:Meta_Learning/Kim_2020_A Policy Gradient Algorithm for Learning to Learn in Multiagent Reinforcement Learning.pdf:PDF},
  groups           = {Multi_Agent_Meta_Learning},
  keywords         = {cs.LG, cs.AI, cs.MA},
  modificationdate = {2022-02-28T10:03:31},
  primaryclass     = {cs.LG},
  ranking          = {rank5},
  url              = {https://openreview.net/forum?id=zdrls6LIX4W},
}

@Article{Luo2021Opponent,
  author           = {罗俊仁 and 张万鹏 and 袁唯淋 and 胡振震 and 陈少飞 and 陈璟},
  journal          = {系统仿真学报},
  title            = {面向多智能体博弈对抗的对手建模框架},
  year             = {2021},
  issn             = {1004-731X},
  pages            = {1--13},
  abstract         = {对手建模作为多智能体博弈对抗的关键技术，是一种典型的智能体认知行为建模方法。本文首先介绍了多智能体博弈对抗几类典型模型、非平稳问题和元博弈相关理论；随后梳理总结对手建模方法，归纳了对手建模前沿理论，并对其应用前景及面对的挑战进行分析；最后基于元博弈理论，构建了一个包括对手策略识别与生成、对手策略空间重构和对手利用共三个模块的通用对手建模框架，期望为多智能体博弈对抗对手建模方面的理论与方法的研究提供有价值的参考。},
  file             = {:罗俊仁_2021_面向多智能体博弈对抗的对手建模框架.pdf:PDF},
  keywords         = {多智能体;对手建模;认知行为建模;元博弈},
  modificationdate = {2022-03-20T10:46:20},
  ranking          = {rank5},
}

@Misc{Kayaalp2020DifMAML,
  author           = {Mert Kayaalp and Stefan Vlaski and Ali H. Sayed},
  title            = {Dif-MAML: Decentralized Multi-Agent Meta-Learning},
  year             = {2020},
  month            = oct,
  abstract         = {The objective of meta-learning is to exploit the knowledge obtained from observed tasks to improve adaptation to unseen tasks. As such, meta-learners are able to generalize better when they are trained with a larger number of observed tasks and with a larger amount of data per task. Given the amount of resources that are needed, it is generally difficult to expect the tasks, their respective data, and the necessary computational capacity to be available at a single central location. It is more natural to encounter situations where these resources are spread across several agents connected by some graph topology. The formalism of meta-learning is actually well-suited to this decentralized setting, where the learner would be able to benefit from information and computational power spread across the agents. Motivated by this observation, in this work, we propose a cooperative fully-decentralized multi-agent meta-learning algorithm, referred to as Diffusion-based MAML or Dif-MAML. Decentralized optimization algorithms are superior to centralized implementations in terms of scalability, avoidance of communication bottlenecks, and privacy guarantees. The work provides a detailed theoretical analysis to show that the proposed strategy allows a collection of agents to attain agreement at a linear rate and to converge to a stationary point of the aggregate MAML objective even in non-convex environments. Simulation results illustrate the theoretical findings and the superior performance relative to the traditional non-cooperative setting.},
  archiveprefix    = {arXiv},
  eprint           = {2010.02870},
  file             = {:Meta_Learning/Kayaalp_2020_Dif MAML_ Decentralized Multi Agent Meta Learning.pdf:PDF},
  groups           = {Multi_Agent_Meta_Learning},
  keywords         = {cs.LG, cs.MA},
  modificationdate = {2022-02-22T10:41:16},
  primaryclass     = {cs.LG},
}

@InProceedings{Zintgraf19FastContext,
  author           = {Zintgraf, Luisa and Shiarli, Kyriacos and Kurin, Vitaly and Hofmann, Katja and Whiteson, Shimon},
  booktitle        = {Proceedings of the 36th International Conference on Machine Learning},
  title            = {Fast Context Adaptation via Meta-Learning},
  year             = {2019},
  editor           = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  month            = jun,
  pages            = {7693--7702},
  publisher        = {PMLR},
  series           = {Proceedings of Machine Learning Research},
  volume           = {97},
  abstract         = {We propose CAVIA for meta-learning, a simple extension to MAML that is less prone to meta-overfitting, easier to parallelise, and more interpretable. CAVIA partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, only the context parameters are updated, leading to a low-dimensional task representation. We show empirically that CAVIA outperforms MAML for regression, classification, and reinforcement learning. Our experiments also highlight weaknesses in current benchmarks, in that the amount of adaptation needed in some cases is small.},
  code             = {https://github.com/lmzintgraf/cavia},
  file             = {:Meta_Learning/Zintgraf_2019_Fast Context Adaptation Via Meta Learning.pdf:PDF},
  groups           = {Meta Learning},
  modificationdate = {2022-06-15T16:04:38},
  url              = {http://proceedings.mlr.press/v97/zintgraf19a.html},
}

@InProceedings{Lin2021Accelerating,
  author           = {Sen Lin and Mehmet Dedeoglu and Junshan Zhang},
  booktitle        = {Proceedings of the Twenty-second International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
  title            = {Accelerating Distributed Online Meta-Learning via Multi-Agent Collaboration under Limited Communication},
  year             = {2021},
  month            = jul,
  publisher        = {{ACM}},
  comment          = {元学习加速，通过多个代理在学习时通信提高学习效率。实验场景就是几个图像数据集one-shot分类},
  doi              = {10.1145/3466772.3467055},
  file             = {:Meta_Learning/Lin_2021_Accelerating Distributed Online Meta Learning Via Multi Agent Collaboration under Limited Communication.pdf:PDF},
  groups           = {Meta Learning},
  modificationdate = {2022-06-15T16:09:26},
}

@Article{Munir2021MultiAgentML,
  author           = {Md. Shirajum Munir and Nguyen H. Tran and Walid Saad and Choong Seon Hong},
  journal          = {IEEE Transactions on Network and Service Management},
  title            = {Multi-Agent Meta-Reinforcement Learning for Self-Powered and Sustainable Edge Computing Systems},
  year             = {2021},
  pages            = {3353--3374},
  volume           = {18},
  file             = {:Meta_Learning/Munir_2021_Multi Agent Meta Reinforcement Learning for Self Powered and Sustainable Edge Computing Systems.pdf:PDF},
  groups           = {Multi_Agent_Meta_Learning},
  modificationdate = {2022-02-25T22:31:45},
}

@InProceedings{Jia2019Fast,
  author           = {Hongda Jia and Bo Ding and Huaimin Wang and Xudong Gong and Xing Zhou},
  booktitle        = {2019 IEEE SmartWorld, Ubiquitous Intelligence \& Computing, Advanced \& Trusted Computing, Scalable Computing \& Communications, Cloud \& Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)},
  title            = {Fast Adaptation via Meta Learning in Multi-agent Cooperative Tasks},
  year             = {2019},
  month            = aug,
  pages            = {707--714},
  publisher        = {{IEEE}},
  abstract         = {Multi-agent deep reinforcement learning (MADRL) has been used for disaster rescue and other robotic applications. In most MADRL cases, agents are trained only to deal with specific tasks, so once there are unpredictable changes in target tasks, the earlier trained-models might no longer work, and agents have to get trained again from scratch in limited time, which means traditional MADRL is unsuited to unpredictable post-disaster environments. In order to promote the scalability and flexibility of the multi-agent system, meta-learning, which has achieved few shot learning successfully in supervised deep learning field, could reuse the earlier models as the prior knowledge to guide and speed up the new task learning process. In this work, we propose a framework to apply meta learning methods to multi-agent deep deterministic policy gradient (MADDPG), and achieve efficient adaption to new tasks with less time and fewer samples. Some common experiences in the previous tasks learning will be refined as the meta knowledge. Once the tasks get changed with new scenarios, agents will get retrained with good initial network parameters based on this meta knowledge. Although the new scenarios might be different from the previous scenarios, agents could quickly adjust their policy and get better performance within only few episodes. In two experiments, our method performs better learning efficiency than others, getting higher rewards in the first several episodes of new tasks learning.},
  doi              = {10.1109/smartworld-uic-atc-scalcom-iop-sci.2019.00156},
  file             = {:Meta_Learning/Jia_2019_Fast Adaptation Via Meta Learning in Multi Agent Cooperative Tasks.pdf:PDF},
  groups           = {Multi_Agent_Meta_Learning},
  modificationdate = {2023-01-07T22:04:30},
  ranking          = {rank5},
}

@Misc{Gupta2021Dynamic,
  author           = {Abhinav Gupta and Marc Lanctot and Angeliki Lazaridou},
  month            = oct,
  title            = {Dynamic population-based meta-learning for multi-agent communication with natural language},
  year             = {2021},
  abstract         = {In this work, our goal is to train agents that can coordinate with seen, unseen as well as human partners in a multi-agent communication environment involving natural language. Previous work using a single set of agents has shown great progress in generalizing to known partners, however it struggles when coordinating with unfamiliar agents. To mitigate that, recent work explored the use of population-based approaches, where multiple agents interact with each other with the goal of learning more generic protocols. These methods, while able to result in good coordination between unseen partners, still only achieve so in cases of simple languages, thus failing to adapt to human partners using natural language. We attribute this to the use of static populations and instead propose a dynamic population-based meta-learning approach that builds such a population in an iterative manner. We perform a holistic evaluation of our method on two different referential games, and show that our agents outperform all prior work when communicating with seen partners and humans. Furthermore, we analyze the natural language generation skills of our agents, where we find that our agents also outperform strong baselines. Finally, we test the robustness of our agents when communicating with out-of-population agents and carefully test the importance of each component of our method through ablation studies.},
  archiveprefix    = {arXiv},
  eprint           = {2110.14241},
  file             = {:Meta_Learning/Gupta_2021_Dynamic Population Based Meta Learning for Multi Agent Communication with Natural Language.pdf:PDF},
  groups           = {Multi_Agent_Meta_Learning},
  keywords         = {cs.LG, cs.AI, cs.CL, cs.MA},
  modificationdate = {2022-02-28T10:46:30},
  primaryclass     = {cs.LG},
  ranking          = {rank1},
}

@Misc{Kirk2021Survey,
  author        = {Robert Kirk and Amy Zhang and Edward Grefenstette and Tim Rocktäschel},
  title         = {A Survey of Generalisation in Deep Reinforcement Learning},
  year          = {2021},
  month         = nov,
  abstract      = {The study of generalisation in deep Reinforcement Learning (RL) aims to produce RL algorithms whose policies generalise well to novel unseen situations at deployment time, avoiding overfitting to their training environments. Tackling this is vital if we are to deploy reinforcement learning algorithms in real world scenarios, where the environment will be diverse, dynamic and unpredictable. This survey is an overview of this nascent field. We provide a unifying formalism and terminology for discussing different generalisation problems, building upon previous works. We go on to categorise existing benchmarks for generalisation, as well as current methods for tackling the generalisation problem. Finally, we provide a critical discussion of the current state of the field, including recommendations for future work. Among other conclusions, we argue that taking a purely procedural content generation approach to benchmark design is not conducive to progress in generalisation, we suggest fast online adaptation and tackling RL-specific problems as some areas for future work on methods for generalisation, and we recommend building benchmarks in underexplored problem settings such as offline RL generalisation and reward-function variation.},
  archiveprefix = {arXiv},
  eprint        = {2111.09794},
  file          = {:Kirk_2021_A Survey of Generalisation in Deep Reinforcement Learning.pdf:PDF},
  groups        = {Review},
  keywords      = {cs.LG, cs.AI},
  primaryclass  = {cs.LG},
}

@InProceedings{Al-Shedivat18RoboSumo,
  author           = {Maruan Al{-}Shedivat and Trapit Bansal and Yura Burda and Ilya Sutskever and Igor Mordatch and Pieter Abbeel},
  booktitle        = {6th International Conference on Learning Representations, ICLR 2018},
  title            = {Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments},
  year             = {2018},
  address          = {Vancouver, BC, Canada},
  month            = apr,
  pages            = {1--21},
  publisher        = {OpenReview.net},
  bibsource        = {dblp computer science bibliography, https://dblp.org},
  biburl           = {https://dblp.org/rec/conf/iclr/Al-ShedivatBBSM18.bib},
  code             = {https://github.com/openai/robosumo},
  comment          = {RoboSumo},
  file             = {:Meta_Learning/Al-Shedivat_2018_Continuous Adaptation Via Meta Learning in Nonstationary and Competitive Environments (RoboSumo).pdf:PDF},
  groups           = {Multi_Agent_Meta_Learning},
  modificationdate = {2023-01-07T22:00:53},
  timestamp        = {Thu, 04 Apr 2019 13:20:09 +0200},
  url              = {https://openreview.net/forum?id=Sk2u1g-0-},
}

@InProceedings{kim2021Policy,
  author           = {Kim, Dong Ki and Liu, Miao and Riemer, Matthew D and Sun, Chuangchuang and Abdulhai, Marwa and Habibi, Golnaz and Lopez-Cot, Sebastian and Tesauro, Gerald and How, Jonathan},
  booktitle        = {International Conference on Machine Learning},
  title            = {A policy gradient algorithm for learning to learn in multiagent reinforcement learning},
  year             = {2021},
  organization     = {PMLR},
  pages            = {5541--5550},
  file             = {:Meta_Learning/Kim_2021_A Policy Gradient Algorithm for Learning to Learn in Multiagent Reinforcement Learning.pdf:PDF},
  groups           = {Multi_Agent_Meta_Learning},
  modificationdate = {2022-03-20T10:50:08},
  ranking          = {rank5},
}

@Misc{Terry2020PettingZoo,
  author           = {J. K. Terry and Benjamin Black and Nathaniel Grammel and Mario Jayakumar and Ananth Hari and Ryan Sullivan and Luis Santos and Rodrigo Perez and Caroline Horsch and Clemens Dieffendahl and Niall L. Williams and Yashas Lokesh and Praveen Ravi},
  month            = sep,
  title            = {PettingZoo: Gym for Multi-Agent Reinforcement Learning},
  year             = {2020},
  abstract         = {This paper introduces the PettingZoo library and the accompanying Agent Environment Cycle ("AEC") games model. PettingZoo is a library of diverse sets of multi-agent environments with a universal, elegant Python API. PettingZoo was developed with the goal of accelerating research in Multi-Agent Reinforcement Learning ("MARL"), by making work more interchangeable, accessible and reproducible akin to what OpenAI's Gym library did for single-agent reinforcement learning. PettingZoo's API, while inheriting many features of Gym, is unique amongst MARL APIs in that it's based around the novel AEC games model. We argue, in part through case studies on major problems in popular MARL environments, that the popular game models are poor conceptual models of games commonly used in MARL and accordingly can promote confusing bugs that are hard to detect, and that the AEC games model addresses these problems.},
  archiveprefix    = {arXiv},
  eprint           = {2009.14471},
  file             = {:Terry_2020_PettingZoo_ Gym for Multi Agent Reinforcement Learning.pdf:PDF},
  keywords         = {cs.LG, cs.MA, stat.ML},
  modificationdate = {2022-02-25T13:10:40},
  primaryclass     = {cs.LG},
}

@Article{Gronauer2021Survey,
  author           = {Sven Gronauer and Klaus Diepold},
  journal          = {Artificial Intelligence Review},
  title            = {Multi-agent deep reinforcement learning: a survey},
  year             = {2021},
  month            = apr,
  doi              = {10.1007/s10462-021-09996-w},
  file             = {:Gronauer_2021_Multi Agent Deep Reinforcement Learning_ a Survey.pdf:PDF},
  groups           = {Review},
  modificationdate = {2022-06-15T16:09:51},
  publisher        = {Springer Science and Business Media {LLC}},
}

@Misc{Charakorn2021Metaadhoc,
  author           = {Rujikorn Charakorn and Poramate Manoonpong and Nat Dilokthanakul},
  month            = nov,
  title            = {Learning to Cooperate with Unseen Agent via Meta-Reinforcement Learning},
  year             = {2021},
  abstract         = {Ad hoc teamwork problem describes situations where an agent has to cooperate with previously unseen agents to achieve a common goal. For an agent to be successful in these scenarios, it has to have a suitable cooperative skill. One could implement cooperative skills into an agent by using domain knowledge to design the agent's behavior. However, in complex domains, domain knowledge might not be available. Therefore, it is worthwhile to explore how to directly learn cooperative skills from data. In this work, we apply meta-reinforcement learning (meta-RL) formulation in the context of the ad hoc teamwork problem. Our empirical results show that such a method could produce robust cooperative agents in two cooperative environments with different cooperative circumstances: social compliance and language interpretation. (This is a full paper of the extended abstract version.)},
  archiveprefix    = {arXiv},
  eprint           = {2111.03431},
  file             = {:Ad_Hoc_Teamwork/Charakorn_2021_Learning to Cooperate with Unseen Agent Via Meta Reinforcement Learning.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  keywords         = {cs.AI, cs.LG, cs.MA},
  modificationdate = {2022-06-15T16:06:24},
  primaryclass     = {cs.AI},
}

@Misc{Yang2020Overview,
  author           = {Yaodong Yang and Jun Wang},
  month            = nov,
  title            = {An Overview of Multi-Agent Reinforcement Learning from Game Theoretical Perspective},
  year             = {2020},
  abstract         = {Following the remarkable success of the AlphaGO series, 2019 was a booming year that witnessed significant advances in multi-agent reinforcement learning (MARL) techniques. MARL corresponds to the learning problem in a multi-agent system in which multiple agents learn simultaneously. It is an interdisciplinary domain with a long history that includes game theory, machine learning, stochastic control, psychology, and optimisation. Although MARL has achieved considerable empirical success in solving real-world games, there is a lack of a self-contained overview in the literature that elaborates the game theoretical foundations of modern MARL methods and summarises the recent advances. In fact, the majority of existing surveys are outdated and do not fully cover the recent developments since 2010. In this work, we provide a monograph on MARL that covers both the fundamentals and the latest developments in the research frontier. The goal of our monograph is to provide a self-contained assessment of the current state-of-the-art MARL techniques from a game theoretical perspective. We expect this work to serve as a stepping stone for both new researchers who are about to enter this fast-growing domain and existing domain experts who want to obtain a panoramic view and identify new directions based on recent advances.},
  archiveprefix    = {arXiv},
  eprint           = {2011.00583},
  file             = {:Yang_2020_An Overview of Multi Agent Reinforcement Learning from Game Theoretical Perspective.pdf:PDF},
  groups           = {Review, MARL},
  keywords         = {cs.MA, cs.AI},
  modificationdate = {2022-02-25T00:04:36},
  primaryclass     = {cs.MA},
  ranking          = {rank4},
}

@Book{Sutton2018Reinforcement,
  author           = {Sutton, Richard S and Barto, Andrew G},
  publisher        = {MIT press},
  title            = {Reinforcement learning: An introduction},
  year             = {2018},
  file             = {:Sutton_2018_Reinforcement Learning_ an Introduction_v2.pdf:PDF},
  modificationdate = {2022-03-20T10:48:21},
}

@Misc{Grimbly2021Review,
  author        = {St John Grimbly and Jonathan Shock and Arnu Pretorius},
  title         = {Causal Multi-Agent Reinforcement Learning: Review and Open Problems},
  year          = {2021},
  month         = nov,
  abstract      = {This paper serves to introduce the reader to the field of multi-agent reinforcement learning (MARL) and its intersection with methods from the study of causality. We highlight key challenges in MARL and discuss these in the context of how causal methods may assist in tackling them. We promote moving toward a 'causality first' perspective on MARL. Specifically, we argue that causality can offer improved safety, interpretability, and robustness, while also providing strong theoretical guarantees for emergent behaviour. We discuss potential solutions for common challenges, and use this context to motivate future research directions.},
  archiveprefix = {arXiv},
  eprint        = {2111.06721},
  file          = {:Grimbly_2021_Causal Multi Agent Reinforcement Learning_ Review and Open Problems.pdf:PDF},
  groups        = {Review},
  keywords      = {cs.LG, cs.AI, stat.ML},
  primaryclass  = {cs.LG},
}

@InProceedings{Gupta2021Meta,
  author           = {Abhinav Gupta and Angeliki Lazaridou and Marc Lanctot},
  booktitle        = {Learning to Learn - Workshop at ICLR 2021},
  title            = {Meta Learning for Multi-agent Communication},
  year             = {2021},
  abstract         = {Recent works have shown remarkable progress in training artificial agents to understand natural language but are focused on using large amounts of raw data involving huge compute requirements. An interesting hypothesis follows the idea of training artificial agents via multi-agent communication while using small amounts of task-specific human data to ground the emergent language into natural language. This allows agents to communicate with humans without needing enormous expensive human demonstrations. Evolutionary studies have showed that simpler and easily adaptable languages arise as a result of communicating with a diverse group of large population. We propose to model this supposition with artificial agents and propose an adaptive population-based meta-reinforcement learning approach that builds such a population in an iterative manner. We show empirical results on referential games involving natural language where our agents outperform all baselines on both the task performance and language score including human evaluation. We demonstrate that our method induces constructive diversity into a growing population of agents that is beneficial in training the meta-agent.},
  file             = {:Meta_Learning/Gupta_2021_Meta Learning for Multi Agent Communication.pdf:PDF},
  groups           = {Multi_Agent_Meta_Learning},
  modificationdate = {2022-03-20T10:42:28},
  url              = {https://openreview.net/forum?id=mo_rSxN38VO},
}

@Misc{Wang2021Alchemy,
  author        = {Jane X. Wang and Michael King and Nicolas Porcel and Zeb Kurth-Nelson and Tina Zhu and Charlie Deck and Peter Choy and Mary Cassin and Malcolm Reynolds and Francis Song and Gavin Buttimore and David P. Reichert and Neil Rabinowitz and Loic Matthey and Demis Hassabis and Alexander Lerchner and Matthew Botvinick},
  title         = {Alchemy: A benchmark and analysis toolkit for meta-reinforcement learning agents},
  year          = {2021},
  month         = feb,
  abstract      = {There has been rapidly growing interest in meta-learning as a method for increasing the flexibility and sample efficiency of reinforcement learning. One problem in this area of research, however, has been a scarcity of adequate benchmark tasks. In general, the structure underlying past benchmarks has either been too simple to be inherently interesting, or too ill-defined to support principled analysis. In the present work, we introduce a new benchmark for meta-RL research, emphasizing transparency and potential for in-depth analysis as well as structural richness. Alchemy is a 3D video game, implemented in Unity, which involves a latent causal structure that is resampled procedurally from episode to episode, affording structure learning, online inference, hypothesis testing and action sequencing based on abstract domain knowledge. We evaluate a pair of powerful RL agents on Alchemy and present an in-depth analysis of one of these agents. Results clearly indicate a frank and specific failure of meta-learning, providing validation for Alchemy as a challenging benchmark for meta-RL. Concurrent with this report, we are releasing Alchemy as public resource, together with a suite of analysis tools and sample agent trajectories.},
  archiveprefix = {arXiv},
  comment       = {Alchemy是在Unity中实现的单人视频游戏。玩家可以看到桌子上的第一人称视角，桌子上有许多物体，包括一组彩色的石头，一组装有彩色药水的盘子和一个中央大锅。石头具有不同的点值，并且在将石头添加到大锅中时会收集点。通过将石头浸入魔药中，玩家可以改变石头的外观，从而改变它们的价值，从而增加可赢得的分数。

但是，Alchemy也涉及到一个至关重要的陷阱：每次玩游戏时，控制药水如何影响石头的“化学作用”都会改变。熟练的演奏者必须执行一组有针对性的实验，以发现当前化学反应的原理，并使用这些实验的结果来指导战略行动序列。在多轮Alchemy中学习如何做到这一点，正是meta-RL的挑战。},
  eprint        = {2102.02926},
  file          = {:Meta_Learning/Wang_2021_Alchemy_ a Benchmark and Analysis Toolkit for Meta Reinforcement Learning Agents.pdf:PDF},
  groups        = {Meta Learning},
  keywords      = {cs.LG, cs.AI},
  primaryclass  = {cs.LG},
}

@Misc{Li2021MetaDrive,
  author           = {Quanyi Li and Zhenghao Peng and Zhenghai Xue and Qihang Zhang and Bolei Zhou},
  month            = sep,
  title            = {MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning},
  year             = {2021},
  abstract         = {Driving safely requires multiple capabilities from human and intelligent agents, such as the generalizability to unseen environments, the decision making in complex multi-agent settings, and the safety awareness of the surrounding traffic. Despite the great success of reinforcement learning, most of the RL research studies each capability separately due to the lack of the integrated interactive environments. In this work, we develop a new driving simulation platform called MetaDrive for the study of generalizable reinforcement learning algorithms. MetaDrive is highly compositional, which can generate an infinite number of diverse driving scenarios from both the procedural generation and the real traffic data replay. Based on MetaDrive, we construct a variety of RL tasks and baselines in both single-agent and multi-agent settings, including benchmarking generalizability across unseen scenes, safe exploration, and learning multi-agent traffic. We open-source this simulator and maintain its development at: https://github.com/decisionforce/metadrive},
  archiveprefix    = {arXiv},
  eprint           = {2109.12674},
  file             = {:Meta_Learning/Li_2021_MetaDrive_ Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning.pdf:PDF},
  groups           = {Meta Learning},
  keywords         = {cs.LG, cs.RO},
  modificationdate = {2022-02-25T13:24:52},
  primaryclass     = {cs.LG},
}

@Misc{Parisotto2019Concurrent,
  author        = {Emilio Parisotto and Soham Ghosh and Sai Bhargav Yalamanchi and Varsha Chinnaobireddy and Yuhuai Wu and Ruslan Salakhutdinov},
  title         = {Concurrent Meta Reinforcement Learning},
  year          = {2019},
  month         = mar,
  abstract      = {State-of-the-art meta reinforcement learning algorithms typically assume the setting of a single agent interacting with its environment in a sequential manner. A negative side-effect of this sequential execution paradigm is that, as the environment becomes more and more challenging, and thus requiring more interaction episodes for the meta-learner, it needs the agent to reason over longer and longer time-scales. To combat the difficulty of long time-scale credit assignment, we propose an alternative parallel framework, which we name "Concurrent Meta-Reinforcement Learning" (CMRL), that transforms the temporal credit assignment problem into a multi-agent reinforcement learning one. In this multi-agent setting, a set of parallel agents are executed in the same environment and each of these "rollout" agents are given the means to communicate with each other. The goal of the communication is to coordinate, in a collaborative manner, the most efficient exploration of the shared task the agents are currently assigned. This coordination therefore represents the meta-learning aspect of the framework, as each agent can be assigned or assign itself a particular section of the current task's state space. This framework is in contrast to standard RL methods that assume that each parallel rollout occurs independently, which can potentially waste computation if many of the rollouts end up sampling the same part of the state space. Furthermore, the parallel setting enables us to define several reward sharing functions and auxiliary losses that are non-trivial to apply in the sequential setting. We demonstrate the effectiveness of our proposed CMRL at improving over sequential methods in a variety of challenging tasks.},
  archiveprefix = {arXiv},
  eprint        = {1903.02710},
  file          = {:Meta_Learning/Parisotto_2019_Concurrent Meta Reinforcement Learning.pdf:PDF},
  groups        = {Meta Learning},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
}

@InProceedings{Liu2021Cooperative,
  author           = {Liu, Iou-Jen and Jain, Unnat and Yeh, Raymond A and Schwing, Alexander},
  booktitle        = {Proceedings of the 38th International Conference on Machine Learning},
  title            = {Cooperative Exploration for Multi-Agent Deep Reinforcement Learning},
  year             = {2021},
  editor           = {Meila, Marina and Zhang, Tong},
  month            = jul,
  pages            = {6826--6836},
  publisher        = {PMLR},
  series           = {Proceedings of Machine Learning Research},
  volume           = {139},
  abstract         = {Exploration is critical for good results in deep reinforcement learning and has attracted much attention. However, existing multi-agent deep reinforcement learning algorithms still use mostly noise-based techniques. Very recently, exploration methods that consider cooperation among multiple agents have been developed. However, existing methods suffer from a common challenge: agents struggle to identify states that are worth exploring, and hardly coordinate exploration efforts toward those states. To address this shortcoming, in this paper, we propose cooperative multi-agent exploration (CMAE): agents share a common goal while exploring. The goal is selected from multiple projected state spaces by a normalized entropy-based technique. Then, agents are trained to reach the goal in a coordinated manner. We demonstrate that CMAE consistently outperforms baselines on various tasks, including a sparse-reward version of multiple-particle environment (MPE) and the Starcraft multi-agent challenge (SMAC).},
  comment          = {讨论MARL中的探索问题，属于理论研究？},
  creationdate     = {2021-11-26T11:03:39},
  file             = {:Liu_2021_Cooperative Exploration for Multi Agent Deep Reinforcement Learning.pdf:PDF},
  modificationdate = {2022-06-15T16:10:30},
  pdf              = {http://proceedings.mlr.press/v139/liu21j/liu21j.pdf},
  url              = {https://proceedings.mlr.press/v139/liu21j.html},
}

@Misc{Zhang2021Meta,
  author           = {Shenao Zhang and Li Shen and Lei Han and Li Shen},
  title            = {Learning Meta Representations for Agents in Multi-Agent Reinforcement Learning},
  year             = {2021},
  month            = aug,
  abstract         = {In multi-agent reinforcement learning, the behaviors that agents learn in a single Markov Game (MG) are typically confined to the given agent number (i.e., population size). Every single MG induced by varying population sizes may possess distinct optimal joint strategies and game-specific knowledge, which are modeled independently in modern multi-agent algorithms. In this work, we focus on creating agents that generalize across population-varying MGs. Instead of learning a unimodal policy, each agent learns a policy set that is formed by effective strategies across a variety of games. We propose Meta Representations for Agents (MRA) that explicitly models the game-common and game-specific strategic knowledge. By representing the policy sets with multi-modal latent policies, the common strategic knowledge and diverse strategic modes are discovered with an iterative optimization procedure. We prove that as an approximation to a constrained mutual information maximization objective, the learned policies can reach Nash Equilibrium in every evaluation MG under the assumption of Lipschitz game on a sufficiently large latent space. When deploying it at practical latent models with limited size, fast adaptation can be achieved by leveraging the first-order gradient information. Extensive experiments show the effectiveness of MRA on both training performance and generalization ability in hard and unseen games.},
  archiveprefix    = {arXiv},
  comment          = {如何泛化到agent数量不同的任务？},
  creationdate     = {2021-11-26T11:16:41},
  eprint           = {2108.12988},
  file             = {:Meta_Learning/Zhang_2021_Learning Meta Representations for Agents in Multi Agent Reinforcement Learning.pdf:PDF},
  groups           = {Multi_Agent_Meta_Learning},
  keywords         = {cs.LG, cs.MA},
  modificationdate = {2022-02-22T11:42:50},
  primaryclass     = {cs.LG},
}

@Misc{Asayesh2021Least,
  author           = {Salar Asayesh and Mo Chen and Mehran Mehrandezh and Kamal Gupta},
  month            = jun,
  title            = {Least-Restrictive Multi-Agent Collision Avoidance via Deep Meta Reinforcement Learning and Optimal Control},
  year             = {2021},
  abstract         = {Multi-agent collision-free trajectory planning and control subject to different goal requirements and system dynamics has been extensively studied, and is gaining recent attention in the realm of machine and reinforcement learning. However, in particular when using a large number of agents, constructing a least-restrictive collision avoidance policy is of utmost importance for both classical and learning-based methods. In this paper, we propose a Least-Restrictive Collision Avoidance Module (LR-CAM) that evaluates the safety of multi-agent systems and takes over control only when needed to prevent collisions. The LR-CAM is a single policy that can be wrapped around policies of all agents in a multi-agent system. It allows each agent to pursue any objective as long as it is safe to do so. The benefit of the proposed least-restrictive policy is to only interrupt and overrule the default controller in case of an upcoming inevitable danger. We use a Long Short-Term Memory (LSTM) based Variational Auto-Encoder (VAE) to enable the LR-CAM to account for a varying number of agents in the environment. Moreover, we propose an off-policy meta-reinforcement learning framework with a novel reward function based on a Hamilton-Jacobi value function to train the LR-CAM. The proposed method is fully meta-trained through a ROS based simulation and tested on real multi-agent system. Our results show that LR-CAM outperforms the classical least-restrictive baseline by 30 percent. In addition, we show that even if a subset of agents in a multi-agent system use LR-CAM, the success rate of all agents will increase significantly.},
  archiveprefix    = {arXiv},
  creationdate     = {2021-11-26T11:19:11},
  eprint           = {2106.00936},
  file             = {:Meta_Learning/Asayesh_2021_Least Restrictive Multi Agent Collision Avoidance Via Deep Meta Reinforcement Learning and Optimal Control.pdf:PDF},
  groups           = {Multi_Agent_Meta_Learning},
  keywords         = {cs.RO},
  modificationdate = {2022-02-28T11:53:04},
  primaryclass     = {cs.RO},
  ranking          = {rank5},
}

@Article{Sun2020Multi,
  author           = {Shan Sun and Mariam Kiran},
  journal          = {Unpublished},
  title            = {Multi-agent meta reinforcement learning for packet routing in dynamic network environments},
  year             = {2020},
  abstract         = {Traffic optimization challenges, such as flow scheduling and completion time reducing, are difficult online decision-making problems in wide area networks. Previous works apply heuristics that rely on full knowledge of the system to design optimization algorithms. In this work, we explore building a model-free approach, applying multi-agent meta reinforcement learning to solve complex online control problem that generates optimal paths to reroute traffic. Focusing on decentralized solutions, our experiment aims to efficiently minimize the average packet completion time while reducing packet loss across complex network topologies. To evaluate, we test with a static topology and dynamically changing network topologies and compare results to the classical shorted path algorithm.},
  creationdate     = {2021-11-26T11:30:53},
  file             = {:Meta_Learning/(poster) Sun_2020_Multi-agent meta reinforcement learning for packet routing in dynamic network environments.pdf:PDF;:Meta_Learning/Sun_2020_Multi-agent meta reinforcement learning for packet routing in dynamic network environments.pdf:PDF},
  groups           = {Multi_Agent_Meta_Learning},
  modificationdate = {2022-06-30T09:18:47},
}

@InProceedings{Xu2018MetaPolicy,
  author           = {Xu, Tianbing and Liu, Qiang and Zhao, Liang and Peng, Jian},
  booktitle        = {Proceedings of the 35th International Conference on Machine Learning},
  title            = {Learning to Explore via Meta-Policy Gradient},
  year             = {2018},
  editor           = {Dy, Jennifer and Krause, Andreas},
  month            = jul,
  pages            = {5463--5472},
  publisher        = {PMLR},
  series           = {Proceedings of Machine Learning Research},
  volume           = {80},
  abstract         = {The performance of off-policy learning, including deep Q-learning and deep deterministic policy gradient (DDPG), critically depends on the choice of the exploration policy. Existing exploration methods are mostly based on adding noise to the on-going actor policy and can only explore <em>local</em> regions close to what the actor policy dictates. In this work, we develop a simple meta-policy gradient algorithm that allows us to adaptively learn the exploration policy in DDPG. Our algorithm allows us to train flexible exploration behaviors that are independent of the actor policy, yielding a <em>global exploration</em> that significantly speeds up the learning process. With an extensive study, we show that our method significantly improves the sample-efficiency of DDPG on a variety of reinforcement learning continuous control tasks.},
  creationdate     = {2021-12-01T11:34:59},
  file             = {:Meta_Learning/Xu_2018_Learning to Explore Via Meta-Policy Gradient.pdf:PDF},
  groups           = {M},
  modificationdate = {2022-06-15T16:10:33},
  url              = {https://proceedings.mlr.press/v80/xu18d.html},
}

@Misc{Nichol2018Reptile,
  author           = {Alex Nichol and Joshua Achiam and John Schulman},
  month            = mar,
  title            = {On First-Order Meta-Learning Algorithms},
  year             = {2018},
  abstract         = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.},
  archiveprefix    = {arXiv},
  creationdate     = {2021-12-01T16:32:12},
  eprint           = {1803.02999},
  file             = {:Meta_Learning/Nichol_2018_On First Order Meta Learning Algorithms (Reptile).pdf:PDF;:Nichol_2018_On First Order Meta Learning Algorithms (Reptile v1).pdf:PDF},
  groups           = {Meta Learning},
  keywords         = {cs.LG},
  modificationdate = {2023-02-07T16:59:33},
  primaryclass     = {cs.LG},
}

@Misc{Sun2021MAMRL,
  author           = {Shan Sun and Mariam Kiran and Wei Ren},
  month            = nov,
  title            = {MAMRL: Exploiting Multi-agent Meta Reinforcement Learning in WAN Traffic Engineering},
  year             = {2021},
  abstract         = {Traffic optimization challenges, such as load balancing, flow scheduling, and improving packet delivery time, are difficult online decision-making problems in wide area networks (WAN). Complex heuristics are needed for instance to find optimal paths that improve packet delivery time and minimize interruptions which may be caused by link failures or congestion. The recent success of reinforcement learning (RL) algorithms can provide useful solutions to build better robust systems that learn from experience in model-free settings. In this work, we consider a path optimization problem, specifically for packet routing, in large complex networks. We develop and evaluate a model-free approach, applying multi-agent meta reinforcement learning (MAMRL) that can determine the next-hop of each packet to get it delivered to its destination with minimum time overall. Specifically, we propose to leverage and compare deep policy optimization RL algorithms for enabling distributed model-free control in communication networks and present a novel meta-learning-based framework, MAMRL, for enabling quick adaptation to topology changes. To evaluate the proposed framework, we simulate with various WAN topologies. Our extensive packet-level simulation results show that compared to classical shortest path and traditional reinforcement learning approaches, MAMRL significantly reduces the average packet delivery time even when network demand increases; and compared to a non-meta deep policy optimization algorithm, our results show the reduction of packet loss in much fewer episodes when link failures occur while offering comparable average packet delivery time.},
  archiveprefix    = {arXiv},
  comment          = {用元强化学习解决路由的下一跳问题，在遇到节点故障时，相比之前的方法可以有效缩短跳数},
  creationdate     = {2021-12-04T15:47:02},
  eprint           = {2111.15087},
  file             = {:Meta_Learning/Sun_2021_MAMRL_ Exploiting Multi Agent Meta Reinforcement Learning in WAN Traffic Engineering.pdf:PDF},
  groups           = {Multi_Agent_Meta_Learning},
  keywords         = {cs.NI, cs.LG, cs.MA},
  modificationdate = {2022-02-25T22:31:49},
  primaryclass     = {cs.NI},
}

@InProceedings{Finn2017MAML,
  author           = {Chelsea Finn and Pieter Abbeel and Sergey Levine},
  booktitle        = {Proceedings of the 34th International Conference on Machine Learning},
  title            = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  year             = {2017},
  editor           = {Precup, Doina and Teh, Yee Whye},
  month            = aug,
  pages            = {1126--1135},
  publisher        = {PMLR},
  series           = {Proceedings of Machine Learning Research},
  volume           = {70},
  abstract         = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  creationdate     = {2021-12-05T11:38:38},
  file             = {:Meta_Learning/Finn_2017_Model Agnostic Meta Learning for Fast Adaptation of Deep Networks (MAML).pdf:PDF},
  groups           = {Meta Learning},
  modificationdate = {2022-06-15T16:09:41},
  pdf              = {http://proceedings.mlr.press/v70/finn17a/finn17a.pdf},
  url              = {https://proceedings.mlr.press/v70/finn17a.html},
}

@InProceedings{Li2019Feature,
  author           = {Li, Yiying and Yang, Yongxin and Zhou, Wei and Hospedales, Timothy},
  booktitle        = {Proceedings of the 36th International Conference on Machine Learning},
  title            = {Feature-Critic Networks for Heterogeneous Domain Generalization},
  year             = {2019},
  editor           = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  month            = jun,
  pages            = {3915--3924},
  publisher        = {PMLR},
  series           = {Proceedings of Machine Learning Research},
  volume           = {97},
  abstract         = {The well known domain shift issue causes model performance to degrade when deployed to a new target domain with different statistics to training. Domain adaptation techniques alleviate this, but need some instances from the target domain to drive adaptation. Domain generalisation is the recently topical problem of learning a model that generalises to unseen domains out of the box, and various approaches aim to train a domain-invariant feature extractor, typically by adding some manually designed losses. In this work, we propose a learning to learn approach, where the auxiliary loss that helps generalisation is itself learned. Beyond conventional domain generalisation, we consider a more challenging setting of heterogeneous domain generalisation, where the unseen domains do not share label space with the seen ones, and the goal is to train a feature representation that is useful off-the-shelf for novel data and novel categories. Experimental evaluation demonstrates that our method outperforms state-of-the-art solutions in both settings.},
  code             = {https://github.com/liyiying/Feature_Critic},
  creationdate     = {2021-12-06T10:58:38},
  file             = {:Meta_Learning/Li_2019_Feature Critic Networks for Heterogeneous Domain Generalization.pdf:PDF},
  groups           = {Meta Learning},
  modificationdate = {2022-06-15T16:10:35},
  pdf              = {http://proceedings.mlr.press/v97/li19l/li19l.pdf},
  url              = {https://proceedings.mlr.press/v97/li19l.html},
}

@InProceedings{Li2019MetaMADDPG,
  author           = {Li, Yiying and Zhou, Wei and Wang, Huaimin and Ding, Bo and Xu, Kele},
  booktitle        = {2019 IEEE SmartWorld, Ubiquitous Intelligence Computing, Advanced Trusted Computing, Scalable Computing Communications, Cloud Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)},
  title            = {Improving Fast Adaptation for Newcomers in Multi-Robot Reinforcement Learning System},
  year             = {2019},
  month            = aug,
  pages            = {753--760},
  abstract         = {Multi-robot system has been adopted as a kind of ubiquitous intelligent systems to perform critical tasks in various fields. In multi-robot systems, multi-agent reinforcement learning (MARL) is regarded as a promising technology to support decision-making. However, existing MARL approaches assume either a predefined system configuration or a unified model for agents with identical roles, and thus cannot effectively deal with the dynamic change in the number of robots, which is very common in the real world. This kind of "adaptation" problem seriously hinders the development of intelligence in multi-robot systems. In this paper, we propose a novel meta-MADDPG approach to enable new robots to integrate into an existing multi-robot system quickly. We build on the MADDPG (Multi-Agent Deep Deterministic Policy Gradient) algorithm and distill the meta-knowledge of a specific robot team by training a meta-actor and a meta-critic simultaneously. The meta-actor can learn an experienced policy net for new robots to perform reasonable actions directly if the situation is urgent, while the meta-critic trains a value net to criticize the current situation for better evolution of new robots. Our experiments on a typical application case (multi-robot collision avoidance) indicate that the meta-knowledge can significantly improve the fast adaptation for the newcomers. Our source code is available at https://github.com/liyiying/meta-MADDPG.},
  code             = {https://github.com/liyiying/meta-MADDPG},
  creationdate     = {2021-12-06T11:03:31},
  doi              = {10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00162},
  file             = {:Meta_Learning/Li_2019_Improving Fast Adaptation for Newcomers in Multi Robot Reinforcement Learning System.pdf:PDF},
  groups           = {Multi_Agent_Meta_Learning},
  modificationdate = {2022-06-15T16:10:38},
}

@Misc{Zhang2019MARLreview,
  author           = {Kaiqing Zhang and Zhuoran Yang and Tamer Başar},
  month            = nov,
  title            = {Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms},
  year             = {2019},
  abstract         = {Recent years have witnessed significant advances in reinforcement learning (RL), which has registered great success in solving various sequential decision-making problems in machine learning. Most of the successful RL applications, e.g., the games of Go and Poker, robotics, and autonomous driving, involve the participation of more than one single agent, which naturally fall into the realm of multi-agent RL (MARL), a domain with a relatively long history, and has recently re-emerged due to advances in single-agent RL techniques. Though empirically successful, theoretical foundations for MARL are relatively lacking in the literature. In this chapter, we provide a selective overview of MARL, with focus on algorithms backed by theoretical analysis. More specifically, we review the theoretical results of MARL algorithms mainly within two representative frameworks, Markov/stochastic games and extensive-form games, in accordance with the types of tasks they address, i.e., fully cooperative, fully competitive, and a mix of the two. We also introduce several significant but challenging applications of these algorithms. Orthogonal to the existing reviews on MARL, we highlight several new angles and taxonomies of MARL theory, including learning in extensive-form games, decentralized MARL with networked agents, MARL in the mean-field regime, (non-)convergence of policy-based methods for learning in games, etc. Some of the new angles extrapolate from our own research endeavors and interests. Our overall goal with this chapter is, beyond providing an assessment of the current state of the field on the mark, to identify fruitful future research directions on theoretical studies of MARL. We expect this chapter to serve as continuing stimulus for researchers interested in working on this exciting while challenging topic.},
  archiveprefix    = {arXiv},
  creationdate     = {2021-12-26T20:26:42},
  eprint           = {1911.10635},
  file             = {:Zhang_2019_Multi Agent Reinforcement Learning_ a Selective Overview of Theories and Algorithms.pdf:PDF},
  groups           = {Review},
  keywords         = {cs.LG, cs.AI, cs.MA, stat.ML},
  modificationdate = {2022-10-12T20:32:16},
  primaryclass     = {cs.LG},
  ranking          = {rank5},
}

@InProceedings{Gao2020Modeling,
  author           = {Gao, Katelyn and Sener, Ozan},
  booktitle        = {Advances in Neural Information Processing Systems},
  title            = {Modeling and Optimization Trade-off in Meta-learning},
  year             = {2020},
  editor           = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages            = {11154--11165},
  publisher        = {Curran Associates, Inc.},
  volume           = {33},
  abstract         = {By searching for shared inductive biases across tasks, meta-learning promises to accelerate learning on novel tasks, but with the cost of solving a complex bilevel optimization problem. We introduce and rigorously define the trade-off between accurate modeling and optimization ease in meta-learning. At one end, classic meta-learning algorithms account for the structure of meta-learning but solve a complex optimization problem, while at the other end domain randomized search
(otherwise known as joint training) ignores the structure of meta-learning and solves a single level optimization problem. Taking MAML as the representative meta-learning algorithm, we theoretically characterize the trade-off for general nonconvex risk functions as well as linear regression, for which we are able to provide explicit bounds on the errors associated with modeling and optimization. We also empirically study this trade-off for meta-reinforcement learning benchmarks.},
  comment          = {‎通过在任务之间搜索共享的归纳偏差，元学习有望加速对新任务的学习，但其成本是解决复杂的双水平优化问题。我们引入并严格定义了元学习中准确建模和优化易用性之间的权衡。一方面，经典的元学习算法考虑了元学习的结构，但解决了复杂的优化问题，而另一端则域随机搜索（也称为联合训练）忽略了元学习的结构，解决了单级优化问题。以MAML为代表性的元学习算法，理论上表征了一般非凸风险函数以及线性回归的权衡，为此我们能够为与建模和优化相关的误差提供明确的边界。我们还根据经验研究了元强化学习基准的这种权衡。‎},
  creationdate     = {2021-12-27T20:26:48},
  file             = {:Meta_Learning/Gao_2020_Modeling and Optimization Trade off in Meta Learning.pdf:PDF},
  groups           = {Meta Learning},
  modificationdate = {2022-03-29T22:01:58},
  url              = {https://proceedings.neurips.cc/paper/2020/hash/7fc63ff01769c4fa7d9279e97e307829-Abstract.html},
}

@InProceedings{Rothfuss2018Promp,
  author           = {Jonas Rothfuss and Dennis Lee and Ignasi Clavera and Tamim Asfour and Pieter Abbeel},
  booktitle        = {International Conference on Learning Representations},
  title            = {Pro{MP}: Proximal Meta-Policy Search},
  year             = {2019},
  abstract         = {Credit assignment in Meta-reinforcement learning (Meta-RL) is still poorly understood. Existing methods either neglect credit assignment to pre-adaptation behavior or implement it naively. This leads to poor sample-efficiency during metatraining as well as ineffective task identification strategies. This paper provides a theoretical analysis of credit assignment in gradient-based Meta-RL. Building on the gained insights we develop a novel meta-learning algorithm that overcomes both the issue of poor credit assignment and previous difficulties in estimating meta-policy gradients. By controlling the statistical distance of both pre-adaptation and adapted policies during meta-policy search, the proposed algorithm endows efficient and stable meta-learning. Our approach leads to superior pre-adaptation policy behavior and consistently outperforms previous Meta-RL algorithms in sample-efficiency, wall-clock time, and asymptotic performance.},
  comment          = {PPO MAML, meta-rl中的信用分配},
  creationdate     = {2021-12-28T22:39:27},
  file             = {:Meta_Learning/Rothfuss_2019_ProMP_ Proximal Meta Policy Search.pdf:PDF},
  groups           = {Meta Learning},
  modificationdate = {2022-03-29T22:01:34},
  url              = {https://openreview.net/forum?id=SkxXCi0qFX},
}

@Misc{Clune2019GeneralAI,
  author           = {Jeff Clune},
  month            = may,
  title            = {AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence},
  year             = {2019},
  abstract         = {Perhaps the most ambitious scientific quest in human history is the creation of general artificial intelligence, which roughly means AI that is as smart or smarter than humans. The dominant approach in the machine learning community is to attempt to discover each of the pieces required for intelligence, with the implicit assumption that some future group will complete the Herculean task of figuring out how to combine all of those pieces into a complex thinking machine. I call this the "manual AI approach". This paper describes another exciting path that ultimately may be more successful at producing general AI. It is based on the clear trend in machine learning that hand-designed solutions eventually are replaced by more effective, learned solutions. The idea is to create an AI-generating algorithm (AI-GA), which automatically learns how to produce general AI. Three Pillars are essential for the approach: (1) meta-learning architectures, (2) meta-learning the learning algorithms themselves, and (3) generating effective learning environments. I argue that either approach could produce general AI first, and both are scientifically worthwhile irrespective of which is the fastest path. Because both are promising, yet the ML community is currently committed to the manual approach, I argue that our community should increase its research investment in the AI-GA approach. To encourage such research, I describe promising work in each of the Three Pillars. I also discuss AI-GA-specific safety and ethical considerations. Because it it may be the fastest path to general AI and because it is inherently scientifically interesting to understand the conditions in which a simple algorithm can produce general AI (as happened on Earth where Darwinian evolution produced human intelligence), I argue that the pursuit of AI-GAs should be considered a new grand challenge of computer science research.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-01-03T16:29:23},
  eprint           = {1905.10985},
  file             = {:Clune_2019_AI GAs_ AI Generating Algorithms, an Alternate Paradigm for Producing General Artificial Intelligence.pdf:PDF},
  groups           = {Meta Learning},
  keywords         = {cs.AI},
  modificationdate = {2022-03-20T10:41:34},
  primaryclass     = {cs.AI},
}

@Misc{Ribeiro2022Assisting,
  author           = {João G. Ribeiro and Cassandro Martinho and Alberto Sardinha and Francisco S. Melo},
  month            = jan,
  title            = {Assisting Unknown Teammates in Unknown Tasks: Ad Hoc Teamwork under Partial Observability},
  year             = {2022},
  abstract         = {In this paper, we present a novel Bayesian online prediction algorithm for the problem setting of ad hoc teamwork under partial observability (ATPO), which enables on-the-fly collaboration with unknown teammates performing an unknown task without needing a pre-coordination protocol. Unlike previous works that assume a fully observable state of the environment, ATPO accommodates partial observability, using the agent's observations to identify which task is being performed by the teammates. Our approach assumes neither that the teammate's actions are visible nor an environment reward signal. We evaluate ATPO in three domains -- two modified versions of the Pursuit domain with partial observability and the overcooked domain. Our results show that ATPO is effective and robust in identifying the teammate's task from a large library of possible tasks, efficient at solving it in near-optimal time, and scalable in adapting to increasingly larger problem sizes.},
  archiveprefix    = {arXiv},
  comment          = {考虑部分观察下的AHT，提出一种新的贝叶斯在线预测算法，只是用观察O作为输入，无法使用队友的动作，也不假设环境提供奖励信号，从可能的任务库中推测队友正在执行的任务},
  creationdate     = {2022-02-20T15:53:35},
  eprint           = {2201.03538},
  file             = {:Ad_Hoc_Teamwork/Ribeiro_2022_Assisting Unknown Teammates in Unknown Tasks_ Ad Hoc Teamwork under Partial Observability.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  keywords         = {cs.AI, cs.LG, cs.MA},
  modificationdate = {2022-12-08T17:27:18},
  primaryclass     = {cs.AI},
}

@Article{Hu2021DistributedMM,
  author           = {Ye Hu and Mingzhe Chen and Walid Saad and H. Vincent Poor and Shuguang Cui},
  journal          = {IEEE Journal on Selected Areas in Communications},
  title            = {Distributed Multi-Agent Meta Learning for Trajectory Design in Wireless Drone Networks},
  year             = {2021},
  pages            = {3177--3192},
  volume           = {39},
  abstract         = {In this paper, the problem of the trajectory design for a group of energy-constrained drones operating in dynamic wireless network environments is studied. In the considered model, a team of drone base stations (DBSs) is dispatched to cooperatively serve clusters of ground users that have dynamic and unpredictable uplink access demands. In this scenario, the DBSs must cooperatively navigate in the considered area to maximize coverage of the dynamic requests of the ground users. This trajectory design problem is posed as an optimization framework whose goal is to find optimal trajectories that maximize the fraction of users served by all DBSs. To find an optimal solution for this non-convex optimization problem under unpredictable environments, a value decomposition based reinforcement learning (VD-RL) solution coupled with a meta-training mechanism is proposed. This algorithm allows the DBSs to dynamically learn their trajectories while generalizing their learning to unseen environments. Analytical results show that, the proposed VD-RL algorithm is guaranteed to converge to a locally optimal solution of the non-convex optimization problem. Simulation results show that, even without meta-training, the proposed VD-RL algorithm can achieve a 53.2% improvement of the service coverage and a 30.6% improvement in terms of the convergence speed, compared to baseline multi-agent algorithms. Meanwhile, the use of the meta-training mechanism improves the convergence speed of the VD-RL algorithm by up to 53.8% when the DBSs must deal with a previously unseen task.},
  creationdate     = {2022-02-22T10:23:13},
  doi              = {10.1109/jsac.2021.3088689},
  file             = {:Meta_Learning/Multi_Agent_Meta_Learning/Hu_2021_Distributed Multi Agent Meta Learning for Trajectory Design in Wireless Drone Networks.pdf:PDF;:Meta_Learning/Multi_Agent_Meta_Learning/Hu_2021_Distributed Multi Agent Meta Learning for Trajectory Design in Wireless Drone Networks (long pages).pdf:PDF},
  groups           = {Multi_Agent_Meta_Learning},
  modificationdate = {2022-06-15T15:59:11},
}

@MastersThesis{Kim2020Sample,
  author           = {Dong Ki Kim},
  school           = {Massachusetts Institute of Technology},
  title            = {Learning to teach and meta-learning for sample-efficient multiagent reinforcement learning},
  year             = {2020},
  abstract         = {Learning optimal policies in the presence of non-stationary policies of other simultaneously learning agents is a major challenge in multiagent reinforcement learning (MARL). The difficulty is further complicated by other challenges, including the multiagent credit assignment, the high dimensionality of the problems, and the lack of convergence guarantees. As a result, many experiences are often required to learn effective multiagent policies. This thesis introduces two frameworks to reduce the sample complexity in MARL. The first framework presented in this thesis provides a method to reduce the sample complexity by exchanging knowledge between agents. In particular, recent work on agents that learn to teach other teammates has demonstrated that action advising accelerates team-wide learning.
However, the prior work simplified the learning of advising policies by using simple function approximations and only considering advising with primitive (low-level) actions, both of which limit the scalability of learning and teaching to more complex domains. This thesis introduces a novel learning-to-teach framework, called hierarchical multiagent teaching (HMAT), that improves scalability to complex environments by using a deep representation for student policies and by advising with more expressive extended-action sequences over multiple levels of temporal abstraction. Our empirical evaluations demonstrate that HMAT improves team-wide learning progress in large, complex domains where previous approaches fail. HMAT also learns teaching policies that can effectively transfer knowledge to different teammates with knowledge of different tasks, even when the teammates have heterogeneous action spaces.
The second framework introduces the first policy gradient theorem based on meta-learning, which enables fast adaptation (i.e., need only a few iterations) with respect to the non-stationary fellow agents in MARL. The policy gradient theorem that we prove inherently includes both a self-shaping term that considers the impact of a meta-agent's initial policy on its adapted policy and an opponent-shaping term that exploits the learning dynamics of the other agents. We demonstrate that our meta-policy gradient provides agents to meta-learn about different sources of non-stationarity in the environment to improve their learning performances.},
  creationdate     = {2022-02-22T10:58:29},
  file             = {:Meta_Learning/Multi_Agent_Meta_Learning/Kim_2020_Learning to Teach and Meta Learning for Sample Efficient Multiagent Reinforcement Learning.pdf:PDF},
  groups           = {Multi_Agent_Meta_Learning},
  modificationdate = {2022-02-25T22:32:00},
  url              = {http://dspace.mit.edu/handle/1721.1/128312},
}

@InProceedings{Alshehri2020Evolving,
  author           = {Alshehri, Mona and Reyes, Napoleon and Barczak, Andre},
  booktitle        = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  title            = {Evolving Meta-Level Reasoning with Reinforcement Learning and A* for Coordinated Multi-Agent Path-Planning},
  year             = {2020},
  address          = {Richland, SC},
  pages            = {1744--1746},
  publisher        = {International Foundation for Autonomous Agents and Multiagent Systems},
  series           = {AAMAS '20},
  abstract         = {This work presents an extension to a graph-based evolutionary algorithm, called Genetic Network Programming with Reinforcement Learning (GNP-RL) to make it more amenable for solving coordinated multi-agent path-planning tasks in dynamic environments. We improve the algorithm's ability to evolve meta-level reasoning strategies in three aspects: genetic composition, search and learning strategies, using optimal search algorithm, constraint conformance and task prioritization techniques.},
  creationdate     = {2022-02-22T11:33:46},
  doi              = {10.5555/3398761.3398968},
  file             = {:Alshehri_2020_Evolving Meta Level Reasoning with Reinforcement Learning and A_ for Coordinated Multi Agent Path Planning.pdf:PDF},
  groups           = {Meta Learning},
  isbn             = {9781450375184},
  keywords         = {learning agent capabilities, optimal search, evolutionary algorithms, multi-agent learning, reinforcement learning},
  location         = {Auckland, New Zealand},
  modificationdate = {2022-06-15T15:58:02},
  numpages         = {3},
  ranking          = {rank4},
}

@Misc{Yang2021Adaptive,
  author           = {Jiachen Yang and Ethan Wang and Rakshit Trivedi and Tuo Zhao and Hongyuan Zha},
  month            = dec,
  title            = {Adaptive Incentive Design with Multi-Agent Meta-Gradient Reinforcement Learning},
  year             = {2021},
  abstract         = {Critical sectors of human society are progressing toward the adoption of powerful artificial intelligence (AI) agents, which are trained individually on behalf of self-interested principals but deployed in a shared environment. Short of direct centralized regulation of AI, which is as difficult an issue as regulation of human actions, one must design institutional mechanisms that indirectly guide agents' behaviors to safeguard and improve social welfare in the shared environment. Our paper focuses on one important class of such mechanisms: the problem of adaptive incentive design, whereby a central planner intervenes on the payoffs of an agent population via incentives in order to optimize a system objective. To tackle this problem in high-dimensional environments whose dynamics may be unknown or too complex to model, we propose a model-free meta-gradient method to learn an adaptive incentive function in the context of multi-agent reinforcement learning. Via the principle of online cross-validation, the incentive designer explicitly accounts for its impact on agents' learning and, through them, the impact on future social welfare. Experiments on didactic benchmark problems show that the proposed method can induce selfish agents to learn near-optimal cooperative behavior and significantly outperform learning-oblivious baselines. When applied to a complex simulated economy, the proposed method finds tax policies that achieve better trade-off between economic productivity and equality than baselines, a result that we interpret via a detailed behavioral analysis.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-02-22T12:56:54},
  eprint           = {2112.10859},
  file             = {:Meta_Learning/Multi_Agent_Meta_Learning/Yang_2021_Adaptive Incentive Design with Multi Agent Meta Gradient Reinforcement Learning.pdf:PDF},
  groups           = {Multi_Agent_Meta_Learning},
  keywords         = {cs.MA},
  modificationdate = {2022-03-01T22:19:48},
  primaryclass     = {cs.MA},
}

@InProceedings{Baumann2020Adaptive,
  author           = {Tobias Baumann and Thore Graepel and John Shawe-Taylor},
  booktitle        = {2020 International Joint Conference on Neural Networks (IJCNN)},
  title            = {Adaptive Mechanism Design: Learning to Promote Cooperation},
  year             = {2020},
  month            = jul,
  publisher        = {{IEEE}},
  abstract         = {In the future, artificial learning agents are likely to become increasingly widespread in our society. They will interact with both other learning agents and humans in a variety of complex settings including social dilemmas. We consider the problem of how an external agent can promote cooperation between artificial learners by distributing additional rewards and punishments based on observing the learners’ actions. We propose a rule for automatically learning how to create the right incentives by considering the players’ anticipated parameter updates. Using this learning rule leads to cooperation with high social welfare in matrix games in which the agents would otherwise learn to defect with high probability. We show that the resulting cooperative outcome is stable in certain games even if the planning agent is turned off after a given number of episodes, while other games require ongoing intervention to maintain mutual cooperation. However, even in the latter case, the amount of necessary additional incentives decreases over time.},
  creationdate     = {2022-02-22T13:01:29},
  doi              = {10.1109/ijcnn48605.2020.9207690},
  file             = {:Baumann_2020_Adaptive Mechanism Design_ Learning to Promote Cooperation.pdf:PDF},
  modificationdate = {2023-01-07T21:46:03},
}

@Misc{Li2021Nonstationary,
  author           = {Wenhao Li and Xiangfeng Wang and Bo Jin and Junjie Sheng and Hongyuan Zha},
  month            = feb,
  title            = {Dealing with Non-Stationarity in MARL via Trust-Region Decomposition},
  year             = {2021},
  abstract         = {Non-stationarity is one thorny issue in cooperative multi-agent reinforcement learning (MARL). One of the reasons is the policy changes of agents during the learning process. Some existing works have discussed various consequences caused by non-stationarity with several kinds of measurement indicators. This makes the objectives or goals of existing algorithms are inevitably inconsistent and disparate. In this paper, we introduce a novel notion, the $\delta$-measurement, to explicitly measure the non-stationarity of a policy sequence, which can be further proved to be bounded by the KL-divergence of consecutive joint policies. A straightforward but highly non-trivial way is to control the joint policies' divergence, which is difficult to estimate accurately by imposing the trust-region constraint on the joint policy. Although it has lower computational complexity to decompose the joint policy and impose trust-region constraints on the factorized policies, simple policy factorization like mean-field approximation will lead to more considerable policy divergence, which can be considered as the trust-region decomposition dilemma. We model the joint policy as a pairwise Markov random field and propose a trust-region decomposition network (TRD-Net) based on message passing to estimate the joint policy divergence more accurately. The Multi-Agent Mirror descent policy algorithm with Trust region decomposition, called MAMT, is established by adjusting the trust-region of the local policies adaptively in an end-to-end manner. MAMT can approximately constrain the consecutive joint policies' divergence to satisfy $\delta$-stationarity and alleviate the non-stationarity problem. Our method can bring noticeable and stable performance improvement compared with baselines in cooperative tasks of different complexity.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-02-22T17:01:49},
  eprint           = {2102.10616},
  file             = {:Li_2021_Dealing with Non Stationarity in MARL Via Trust Region Decomposition.pdf:PDF},
  keywords         = {cs.LG, cs.GT, cs.MA},
  modificationdate = {2022-06-06T11:12:45},
  primaryclass     = {cs.LG},
}

@Article{Kraemer2016Rehearsal,
  author           = {Kraemer, Landon and Banerjee, Bikramjit},
  journal          = {Neurocomputing},
  title            = {Multi-agent reinforcement learning as a rehearsal for decentralized planning},
  year             = {2016},
  pages            = {82--94},
  volume           = {190},
  abstract         = {Decentralized partially observable Markov decision processes (Dec-POMDPs) are a powerful tool for modeling multi-agent planning and decision-making under uncertainty. Prevalent Dec-POMDP solution techniques require centralized computation given full knowledge of the underlying model. Multi-agent reinforcement learning (MARL) based approaches have been recently proposed for distributed solution of Dec-POMDPs without full prior knowledge of the model, but these methods assume that conditions during learning and policy execution are identical. In some practical scenarios this may not be the case. We propose a novel MARL approach in which agents are allowed to rehearse with information that will not be available during policy execution. The key is for the agents to learn policies that do not explicitly rely on these rehearsal features. We also establish a weak convergence result for our algorithm, RLaR, demonstrating that RLaR converges in probability when certain conditions are met. We show experimentally that incorporating rehearsal features can enhance the learning rate compared to non-rehearsal-based learners, and demonstrate fast, (near) optimal performance on many existing benchmark Dec-POMDP problems. We also compare RLaR against an existing approximate Dec-POMDP solver which, like RLaR, does not assume a priori knowledge of the model. While RLaR׳s policy representation is not as scalable, we show that RLaR produces higher quality policies for most problems and horizons studied.},
  creationdate     = {2022-02-23T10:33:07},
  file             = {:MARL/Kraemer_2016_Multi Agent Reinforcement Learning As a Rehearsal for Decentralized Planning.pdf:PDF},
  groups           = {MARL, Planning},
  modificationdate = {2022-03-29T21:57:13},
  publisher        = {Elsevier},
}

@InProceedings{Foerster2017Stabilising,
  author           = {Foerster, J and Nardelli, N and Farquhar, G and Afouras, T and Torr, P H S and Kohli, P and Whiteson, S},
  booktitle        = {34th International Conference on Machine Learning},
  title            = {{Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning}},
  year             = {2017},
  address          = {Univ Oxford, Oxford, England},
  month            = aug,
  publisher        = {INTERNATIONAL CONFERENCE ON MACHINE LEARNING},
  volume           = {70},
  abstract         = {Many real-world problems, such as network packet routing and urban traffic control, are naturally modeled as multi-agent reinforcement learning (RL) problems. However, existing multi-agent RL methods typically scale poorly in the problem size. Therefore, a key challenge is to translate the success of deep learning on single-agent RL to the multi-agent setting. A major stumbling block is that independent Q-learning, the most popular multi-agent RL method, introduces nonstationarity that makes it incompatible with the experience replay memory on which deep Q-learning relies. This paper proposes two methods that address this problem: 1) using a multi-agent variant of importance sampling to naturally decay obsolete data and 2) conditioning each agent's value function on a fingerprint that disambiguates the age of the data sampled from the replay memory. Results on a challenging decentralised variant of StarCraft unit micro-management confirm that these methods enable the successful combination of experience replay with multi-agent RL.},
  comment          = {ICML2017 在多agent场景下，非平稳性导致IDQN的replay buffer无法兼容},
  file             = {:MARL/Foerster_2017_Stabilising Experience Replay for Deep Multi Agent Reinforcement Learning.pdf:PDF},
  groups           = {MARL},
  modificationdate = {2022-05-03T10:27:42},
}

@Misc{Mirsky2022Survey,
  author           = {Reuth Mirsky and Ignacio Carlucho and Arrasy Rahman and Elliot Fosong and William Macke and Mohan Sridharan and Peter Stone and Stefano V. Albrecht},
  title            = {A Survey of Ad Hoc Teamwork: Definitions, Methods, and Open Problems},
  year             = {2022},
  abstract         = {Ad hoc teamwork is the well-established research problem of designing agents that can collaborate with new teammates without prior coordination. This survey makes a two-fold contribution. First, it provides a structured description of the different facets of the ad hoc teamwork problem. Second, it discusses the progress that has been made in the field so far, and identifies the immediate and long-term open problems that need to be addressed in the field of ad hoc teamwork.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-02-26T10:32:38},
  eprint           = {2202.10450},
  file             = {:Ad_Hoc_Teamwork/Mirsky_2022_A Survey of Ad Hoc Teamwork_ Definitions, Methods, and Open Problems.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  modificationdate = {2022-03-29T21:56:43},
  primaryclass     = {cs.MA},
  ranking          = {rank5},
}

@Misc{Kottinger2022ConflictBased,
  author           = {Justin Kottinger and Shaull Almagor and Morteza Lahijanian},
  title            = {Conflict-Based Search for Explainable Multi-Agent Path Finding},
  year             = {2022},
  abstract         = {In the Multi-Agent Path Finding (MAPF) problem, the goal is to find non-colliding paths for agents in an environment, such that each agent reaches its goal from its initial location. In safety-critical applications, a human supervisor may want to verify that the plan is indeed collision-free. To this end, a recent work introduces a notion of explainability for MAPF based on a visualization of the plan as a short sequence of images representing time segments, where in each time segment the trajectories of the agents are disjoint. Then, the explainable MAPF problem asks for a set of non-colliding paths that admits a short-enough explanation. Explainable MAPF adds a new difficulty to MAPF, in that it is NP-hard with respect to the size of the environment, and not just the number of agents. Thus, traditional MAPF algorithms are not equipped to directly handle explainable-MAPF. In this work, we adapt Conflict Based Search (CBS), a well-studied algorithm for MAPF, to handle explainable MAPF. We show how to add explainability constraints on top of the standard CBS tree and its underlying A* search. We examine the usefulness of this approach and, in particular, the tradeoff between planning time and explainability.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-02-26T10:35:49},
  eprint           = {2202.09930},
  file             = {:Kottinger_2022_Conflict Based Search for Explainable Multi Agent Path Finding.pdf:PDF},
  modificationdate = {2022-03-29T21:56:10},
  primaryclass     = {cs.AI},
}

@Misc{chen2022CommunicationEfficient,
  author           = {Dingyang Chen and Yile Li and Qi Zhang},
  title            = {Communication-Efficient Actor-Critic Methods for Homogeneous Markov Games},
  year             = {2022},
  abstract         = {Recent success in cooperative multi-agent reinforcement learning (MARL) relies on centralized training and policy sharing. Centralized training eliminates the issue of non-stationarity MARL yet induces large communication costs, and policy sharing is empirically crucial to efficient learning in certain tasks yet lacks theoretical justification. In this paper, we formally characterize a subclass of cooperative Markov games where agents exhibit a certain form of homogeneity such that policy sharing provably incurs no suboptimality. This enables us to develop the first consensus-based decentralized actor-critic method where the consensus update is applied to both the actors and the critics while ensuring convergence. We also develop practical algorithms based on our decentralized actor-critic method to reduce the communication cost during training, while still yielding policies comparable with centralized training.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-02-26T10:36:32},
  eprint           = {2202.09422},
  file             = {:Chen_2022_Communication Efficient Actor Critic Methods for Homogeneous Markov Games.pdf:PDF},
  groups           = {MARL},
  modificationdate = {2022-03-29T21:55:55},
  primaryclass     = {cs.MA},
}

@Article{Xu2021MetaWeight,
  author           = {Zhixiong Xu and Xiliang Chen and Wei Tang and Jun Lai and Lei Cao},
  journal          = {Neurocomputing},
  title            = {Meta weight learning via model-agnostic meta-learning},
  year             = {2021},
  month            = apr,
  pages            = {124--132},
  volume           = {432},
  abstract         = {While meta learning approaches have achieved remarkable success, obtaining a stable and unbiased meta-learner remains a significant challenge, since the initial model of a meta-learner could be too biased towards existing tasks to adapt to new tasks. In order to avoid a biased meta-learner and improve its generalizability, this paper proposes a generic meta learning method that aims to learn an unbiased meta-learner towards a variety of tasks before its initial model is adapted to unseen tasks. Specifically, this paper presents a meta weight learning method for minimizing the inequality of performance across different training tasks. An end-to-end training approach is introduced for the proposed algorithm that allows for effectively learning weight and initializing the network model. Alternatively, a variety of measurement methods of weight is also designed to test the effectiveness of different weight learning methods on the improvement of model-agnostic meta-learning algorithm. The simulation results show that the proposed meta weight learning method not only outperforms state-of-the-art meta learning algorithms, but also is superior to other manually designed measurement methods of weight on discrete and continuous control problems.},
  creationdate     = {2022-02-26T10:45:28},
  doi              = {10.1016/j.neucom.2020.08.034},
  file             = {:Meta_Learning/Xu_2021_Meta Weight Learning Via Model Agnostic Meta Learning.pdf:PDF},
  groups           = {Meta_NeuroComputing},
  modificationdate = {2022-06-15T16:05:01},
  publisher        = {Elsevier {BV}},
}

@Article{Liu2020Meta,
  author           = {Xiaoqian Liu and Fengyu Zhou and Jin Liu and Lianjie Jiang},
  journal          = {Neurocomputing},
  title            = {Meta-Learning based prototype-relation network for few-shot classification},
  year             = {2020},
  month            = mar,
  pages            = {224--234},
  volume           = {383},
  abstract         = {Pattern recognition has made great progress under large amount of labeled data, while performs poorly on a very few examples obtained, named few-shot classification, where a classifier can identify new classes not encountered during training. In this paper, a simple framework named Prototype-Relation Network is presented for the few-shot classification. Moreover, a novel loss function compared with prototype networks is proposed which takes both inter-class and intra-class distance into account. During meta-learning, the model is optimized by end-to-end episodes, each of which is to imitate the test few-shot setting. The trained model is used to classify new classes by computing min distance between query images and the prototype of each class. Extensive experimental results demonstrate that our proposed meta-learning model is competitive and effective, which achieves the state-of-the-art performance on Omniglot and miniImageNet datasets.},
  creationdate     = {2022-02-26T10:48:00},
  doi              = {10.1016/j.neucom.2019.12.034},
  file             = {:Meta_Learning/Liu_2020_Meta Learning Based Prototype Relation Network for Few Shot Classification.pdf:PDF},
  groups           = {Meta_NeuroComputing},
  modificationdate = {2022-06-15T16:10:41},
  publisher        = {Elsevier {BV}},
}

@Misc{Zhang2021MetaAdaptive,
  author           = {Mingyue Zhang and Jialong Li and Haiyan Zhao and Kenji Tei and Shinichi Honiden and Zhi Jin},
  title            = {A Meta Reinforcement Learning-based Approach for Self-Adaptive System},
  year             = {2021},
  abstract         = {A self-learning adaptive system (SLAS) uses machine learning to enable and enhance its adaptability. Such systems are expected to perform well in dynamic situations.
For learning high-performance adaptation policy, some assumptions must be made on the environment-system dynamics when information about the real situation is incomplete. However, these assumptions cannot be expected to be always correct, and yet it is difficult to enumerate all possible assumptions.
This leads to the problem of incomplete-information learning.
We consider this problem as multiple model problem in terms of finding the adaptation policy that can cope with multiple models of environment-system dynamics. This paper proposes a novel approach to engineering the online adaptation of SLAS. It separates three concerns that are related to the adaptation policy and presents the modeling and synthesis process, with the goal of achieving higher model construction efficiency. In addition, it designs a meta-reinforcement learning algorithm for learning the meta policy over the multiple models, so that the meta policy can quickly adapts to the real environment-system dynamics. At last, it reports the case study on a robotic system to evaluate the adaptability of the approach.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-02-26T10:50:01},
  eprint           = {2105.04986},
  file             = {:Meta_Learning/Zhang_2021_A Meta Reinforcement Learning Based Approach for Self Adaptive System.pdf:PDF},
  groups           = {Meta Learning},
  modificationdate = {2022-03-29T21:52:36},
  primaryclass     = {cs.SE},
}

@InProceedings{Xu2018MetaGradient,
  author           = {Xu, Zhongwen and van Hasselt, Hado and Silver, David},
  booktitle        = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  title            = {Meta-gradient reinforcement learning},
  year             = {2018},
  pages            = {2402--2413},
  abstract         = {The goal of reinforcement learning algorithms is to estimate and/or optimise the value function. However, unlike supervised learning, no teacher or oracle is available to provide the true value function. Instead, the majority of reinforcement learning algorithms estimate and/or optimise a proxy for the value function. This proxy is typically based on a sampled and bootstrapped approximation to the true value function, known as a return. The particular choice of return is one of the chief components determining the nature of the algorithm: the rate at which future rewards are discounted; when and how values should be bootstrapped; or even the nature of the rewards themselves. It is well-known that these decisions are crucial to the overall success of RL algorithms. We discuss a gradient-based meta-learning algorithm that is able to adapt the nature of the return, online, whilst interacting and learning from the environment. When applied to 57 games on the Atari 2600 environment over 200 million frames, our algorithm achieved a new state-of-the-art performance.},
  creationdate     = {2022-02-27T10:04:55},
  file             = {:Xu_2018_Meta Gradient Reinforcement Learning.pdf:PDF},
  groups           = {Meta Learning},
  modificationdate = {2022-03-29T20:50:26},
}

@Misc{Sung2017Learning,
  author           = {Flood Sung and Li Zhang and Tao Xiang and Timothy Hospedales and Yongxin Yang},
  title            = {Learning to Learn: Meta-Critic Networks for Sample Efficient Learning},
  year             = {2017},
  abstract         = {We propose a novel and flexible approach to meta-learning for learning-to-learn  from only a few examples. Our framework is motivated by actor-critic reinforcement  learning, but can be applied to both reinforcement and supervised learning. The key idea is to learn a meta-critic: an action-value function neural network that learns to  criticise any actor trying to solve any specified task. For supervised learning, this  corresponds to the novel idea of a trainable task-parametrised loss generator. This  meta-critic approach provides a route to knowledge transfer that can flexibly deal  with few-shot and semi-supervised conditions for both reinforcement and supervised  learning. Promising results are shown on both reinforcement and supervised learning  problems.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-02-27T11:47:10},
  eprint           = {1706.09529},
  file             = {:Sung_2017_Learning to Learn_ Meta Critic Networks for Sample Efficient Learning.pdf:PDF},
  groups           = {Meta Learning},
  modificationdate = {2022-03-29T20:40:16},
  primaryclass     = {cs.LG},
}

@InProceedings{Foerster2018Learning,
  author           = {Foerster, Jakob and Chen, Richard Y and {Al-Shedivat}, Maruan and Whiteson, Shimon and Abbeel, Pieter and Mordatch, Igor},
  booktitle        = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
  title            = {Learning with Opponent-Learning Awareness},
  year             = {2018},
  pages            = {122--130},
  abstract         = {Multi-agent settings are quickly gathering importance in machine learning. This includes a plethora of recent work on deep multi-agent reinforcement learning, but also can be extended to hierarchical reinforcement learning, generative adversarial networks and decentralised optimisation. In all these settings the presence of multiple learning agents renders the training problem non-stationary and often leads to unstable training or undesired final results. We present Learning with Opponent-Learning Awareness (LOLA), a method in which each agent shapes the anticipated learning of the other agents in the environment. The LOLA learning rule includes an additional term that accounts for the impact of one agent's policy on the anticipated parameter update of the other agents. Preliminary results show that the encounter of two LOLA agents leads to the emergence of tit-for-tat and therefore cooperation in the iterated prisoners' dilemma (IPD), while independent learning does not. In this domain, LOLA also receives higher payouts compared to a naive learner, and is robust against exploitation by higher order gradient-based methods. Applied to infinitely repeated matching pennies, LOLA agents converge to the Nash equilibrium. In a round robin tournament we show that LOLA agents can successfully shape the learning of a range of multi-agent learning algorithms from literature, resulting in the highest average returns on the IPD. We also show that the LOLA update rule can be efficiently calculated using an extension of the likelihood ratio policy gradient estimator, making the method suitable for model-free reinforcement learning. This method thus scales to large parameter and input spaces and nonlinear function approximators. We also apply LOLA to a grid world task with an embedded social dilemma using deep recurrent policies and opponent modelling. Again, by explicitly considering the learning of the other agent, LOLA agents learn to cooperate out of self-interest.},
  creationdate     = {2022-02-28T10:12:58},
  file             = {:Foerster_2018_Learning with Opponent Learning Awareness (LOLA).pdf:PDF},
  groups           = {MARL},
  modificationdate = {2022-04-02T15:57:03},
}

@InProceedings{Yao2019MetaCity,
  author           = {Huaxiu Yao and Yiding Liu and Ying Wei and Xianfeng Tang and Zhenhui Li},
  booktitle        = {The World Wide Web Conference on - {WWW} {\textquotesingle}19},
  title            = {Learning from Multiple Cities: A Meta-Learning Approach for Spatial-Temporal Prediction},
  year             = {2019},
  pages            = {2181--2191},
  publisher        = {{ACM} Press},
  abstract         = {Spatial-temporal prediction is a fundamental problem for constructing smart city, which is useful for tasks such as traffic control, taxi  dispatching, and environment policy making. Due to data collection  mechanism, it is common to see data collection with unbalanced  spatial distributions. For example, some cities may release taxi  data for multiple years while others only release a few days of  data; some regions may have constant water quality data monitored by sensors whereas some regions only have a small collection  of water samples. In this paper, we tackle the problem of spatialtemporal prediction for the cities with only a short period of data  collection. We aim to utilize the long-period data from other cities  via transfer learning. Different from previous studies that transfer  knowledge from one single source city to a target city, we are the  first to leverage information from multiple cities to increase the  stability of transfer. Specifically, our proposed model is designed  as a spatial-temporal network with a meta-learning paradigm. The  meta-learning paradigm learns a well-generalized initialization of  the spatial-temporal network, which can be effectively adapted to  target cities. In addition, a pattern-based spatial-temporal memory  is designed to distill long-term temporal information (i.e., periodicity). We conduct extensive experiments on two tasks: traffic (taxi  and bike) prediction and water quality prediction. The experiments  demonstrate the effectiveness of our proposed model over several  competitive baseline models.},
  creationdate     = {2022-02-28T15:50:18},
  doi              = {10.1145/3308558.3313577},
  file             = {:Yao_2019_Learning from Multiple Cities_ a Meta Learning Approach for Spatial Temporal Prediction.pdf:PDF},
  groups           = {Meta Learning},
  modificationdate = {2022-06-15T15:57:56},
}

@InProceedings{Shi2021MetaMarket,
  author           = {Jiatu Shi and Huaxiu Yao and Xian Wu and Tong Li and Zedong Lin and Tengfei Wang and Binqiang Zhao},
  booktitle        = {Proceedings of the 14th {ACM} International Conference on Web Search and Data Mining},
  title            = {Relation-aware Meta-learning for E-commerce Market Segment Demand Prediction with Limited Records},
  year             = {2021},
  month            = mar,
  pages            = {220--228},
  publisher        = {{ACM}},
  abstract         = {E-commerce business is revolutionizing our shopping experiences by providing convenient and straightforward services. One of the most fundamental problems is how to balance the demand and supply in market segments to build an efficient platform. While conventional machine learning models have achieved great success on data-sufficient segments, it may fail in a large-portion of segments in E-commerce platforms, where there are not sufficient records to learn well-trained models. In this paper, we tackle this problem in the context of market segment demand prediction. The goal is to facilitate the learning process in the target segments by leveraging the learned knowledge from data-sufficient source segments. Specifically, we propose a novel algorithm, RMLDP, to incorporate a multi-pattern fusion network (MPFN) with a meta-learning paradigm. The multi-pattern fusion network considers both local and seasonal temporal patterns for segment demand prediction. In the meta-learning paradigm, transferable knowledge is regarded as the model parameter initialization of MPFN, which are learned from diverse source segments. Furthermore, we capture the segment relations by combining data-driven segment representation and segment knowledge graph representation and tailor the segment-specific relations to customize transferable model parameter initialization. Thus, even with limited data, the target segment can quickly find the most relevant transferred knowledge and adapt to the optimal parameters. We conduct extensive experiments on two large-scale industrial datasets. The results justify that our RMLDP outperforms a set of state-of-the-art baselines. Besides, RMLDP has been deployed in Taobao, a real-world E-commerce platform. The online A/B testing results further demonstrate the practicality of RMLDP.},
  comment          = {Meta learning in E-Commerce Market},
  creationdate     = {2022-02-28T16:28:45},
  doi              = {10.1145/3437963.3441750},
  groups           = {Meta Learning},
  modificationdate = {2022-06-15T16:10:44},
}

@Misc{Fallah2021ConvergenceMAML,
  author           = {Alireza Fallah and Kristian Georgiev and Aryan Mokhtari and Asuman Ozdaglar},
  title            = {On the Convergence Theory of Debiased Model-Agnostic Meta-Reinforcement Learning},
  year             = {2021},
  abstract         = {We consider Model-Agnostic Meta-Learning (MAML) methods for Reinforcement
Learning (RL) problems, where the goal is to find a policy using data from several  tasks represented by Markov Decision Processes (MDPs) that can be updated by  one step of stochastic policy gradient for the realized MDP. In particular, using  stochastic gradients in MAML update steps is crucial for RL problems since  computation of exact gradients requires access to a large number of possible  trajectories. For this formulation, we propose a variant of the MAML method,  named Stochastic Gradient Meta-Reinforcement Learning (SG-MRL), and study  its convergence properties. We derive the iteration and sample complexity of SGMRL to find an -first-order stationary point, which, to the best of our knowledge,  provides the first convergence guarantee for model-agnostic meta-reinforcement  learning algorithms. We further show how our results extend to the case where  more than one step of stochastic policy gradient method is used at test time. Finally,  we empirically compare SG-MRL and MAML in several deep RL environments.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-02-28T17:51:56},
  eprint           = {2002.05135},
  file             = {:Fallah_2021_On the Convergence Theory of Debiased Model Agnostic Meta Reinforcement Learning.pdf:PDF},
  groups           = {Meta Learning},
  modificationdate = {2022-03-29T20:32:08},
  primaryclass     = {cs.LG},
}

@Article{Huisman2021MetaReview,
  author           = {Mike Huisman and Jan N. van Rijn and Aske Plaat},
  journal          = {Artificial Intelligence Review},
  title            = {A Survey of Deep Meta-Learning},
  year             = {2021},
  month            = apr,
  number           = {6},
  pages            = {4483--4541},
  volume           = {54},
  abstract         = {Deep neural networks can achieve great successes when presented  with large data sets and sufficient computational resources. However, their  ability to learn new concepts quickly is limited. Meta-learning is one approach  to address this issue, by enabling the network to learn how to learn. The  field of Deep Meta-Learning advances at great speed, but lacks a unified, indepth overview of current techniques. With this work, we aim to bridge this  gap. After providing the reader with a theoretical foundation, we investigate  and summarize key methods, which are categorized into i) metric-, ii) model-,  and iii) optimization-based techniques. In addition, we identify the main open  challenges, such as performance evaluations on heterogeneous benchmarks,  and reduction of the computational costs of meta-learning.},
  creationdate     = {2022-03-01T15:17:49},
  doi              = {10.1007/s10462-021-10004-4},
  file             = {:Huisman_2021_A Survey of Deep Meta Learning.pdf:PDF},
  groups           = {Meta Learning},
  keywords         = {Meta-learning; Learning to learn; Few-shot learning; Transfer learning; Deep learning},
  modificationdate = {2022-06-15T16:10:05},
  publisher        = {Springer Science and Business Media {LLC}},
  ranking          = {rank4},
}

@InProceedings{Bengio1991Synaptic,
  author           = {Y. Bengio and S. Bengio and J. Cloutier},
  booktitle        = {{IJCNN}-91-Seattle International Joint Conference on Neural Networks},
  title            = {Learning a synaptic learning rule},
  year             = {1991},
  month            = jul,
  pages            = {969},
  publisher        = {{IEEE}},
  abstract         = {Summary form only given, as follows. The authors discuss an original approach to neural modeling based on the idea of searching, with learning methods, for a synaptic learning rule which is biologically plausible and yields networks that are able to learn to perform difficult tasks. The proposed method of automatically finding the learning rule relies on the idea of considering the synaptic modification rule as a parametric function. This function has local inputs and is the same in many neurons. The parameters that define this function can be estimated with known learning methods. For this optimization, particular attention is given to gradient descent and genetic algorithms. In both cases, estimation of this function consists of a joint global optimization of the synaptic modification function and the networks that are learning to perform some tasks. Both network architecture and the learning function can be designed within constraints derived from biological knowledge.},
  creationdate     = {2022-03-01T21:38:27},
  doi              = {10.1109/ijcnn.1991.155621},
  groups           = {Meta Learning},
  modificationdate = {2022-03-30T12:40:49},
}

@Article{Schmidhuber1992Learning,
  author           = {Jürgen Schmidhuber},
  journal          = {Neural Computation},
  title            = {Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks},
  year             = {1992},
  month            = jan,
  number           = {1},
  pages            = {131--139},
  volume           = {4},
  abstract         = {Previous algorithms for supervised sequence learning are based on dynamic recurrent networks. This paper describes an alternative class of gradient-based systems consisting of two feedforward nets that learn to deal with temporal sequences using fast weights: The first net learns to produce context-dependent weight changes for the second net whose weights may vary very quickly. The method offers the potential for STM storage efficiency: A single weight (instead of a full-fledged unit) may be sufficient for storing temporal information. Various learning methods are derived. Two experiments with unknown time delays illustrate the approach. One experiment shows how the system can be used for adaptive temporary variable binding.},
  creationdate     = {2022-03-01T21:41:12},
  doi              = {10.1162/neco.1992.4.1.131},
  groups           = {Meta Learning},
  modificationdate = {2022-06-15T16:10:46},
  publisher        = {{MIT} Press - Journals},
}

@InProceedings{Santoro16Memory,
  author           = {Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
  booktitle        = {Proceedings of The 33rd International Conference on Machine Learning},
  title            = {Meta-Learning with Memory-Augmented Neural Networks},
  year             = {2016},
  address          = {New York, New York, USA},
  editor           = {Balcan, Maria Florina and Weinberger, Kilian Q.},
  month            = jun,
  pages            = {1842--1850},
  publisher        = {PMLR},
  series           = {Proceedings of Machine Learning Research},
  volume           = {48},
  abstract         = {Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of "one-shot learning." Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.},
  creationdate     = {2022-03-02T15:50:01},
  file             = {:Meta_Learning/Santoro_2016_Meta Learning with Memory Augmented Neural Networks.pdf:PDF},
  groups           = {Meta Learning},
  modificationdate = {2022-06-15T16:10:49},
  url              = {https://proceedings.mlr.press/v48/santoro16.html},
}

@InProceedings{Frans2018Meta,
  author           = {Frans, Kevin and Ho, Jonathan and Chen, Xi and Abbeel, Pieter and Schulman, John},
  booktitle        = {International Conference on Learning Representations},
  title            = {META LEARNING SHARED HIERARCHIES},
  year             = {2018},
  pages            = {468--475},
  abstract         = {We develop a metalearning approach for learning hierarchically structured policies, improving sample efficiency on unseen tasks through the use of shared  primitives—policies that are executed for large numbers of timesteps. Specifically, a set of primitives are shared within a distribution of tasks, and are switched  between by task-specific policies. We provide a concrete metric for measuring  the strength of such hierarchies, leading to an optimization problem for quickly  reaching high reward on unseen tasks. We then present an algorithm to solve this  problem end-to-end through the use of any off-the-shelf reinforcement learning  method, by repeatedly sampling new tasks and resetting task-specific policies. We  successfully discover meaningful motor primitives for the directional movement  of four-legged robots, solely by interacting with distributions of mazes. We also  demonstrate the transferability of primitives to solve long-timescale sparse-reward  obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl  with the same policy.},
  creationdate     = {2022-03-02T23:36:49},
  file             = {:Frans_2018_META LEARNING SHARED HIERARCHIES.pdf:PDF},
  groups           = {Meta Learning},
  modificationdate = {2022-03-29T20:30:43},
}

@InProceedings{Houthooft2018Evolved,
  author           = {Houthooft, Rein and Chen, Richard Y and Isola, Phillip and Stadie, Bradly C and Wolski, Filip and Ho, Jonathan and Abbeel, Pieter},
  booktitle        = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  title            = {Evolved policy gradients},
  year             = {2018},
  pages            = {5405--5414},
  abstract         = {We propose a metalearning approach for learning gradient-based reinforcement learning (RL) algorithms. The idea is to evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss is parametrized via temporal convolutions over the agent's experience. Because this loss is highly flexible in its ability to take into account the agent's history, it enables fast task learning. Empirical results show that our evolved policy gradient algorithm (EPG) achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method. We also demonstrate that EPG's learned loss can generalize to out-of-distribution test time tasks, and exhibits qualitatively different behavior from other popular metalearning algorithms.},
  creationdate     = {2022-03-03T14:59:55},
  file             = {:Houthooft_2018_Evolved Policy Gradients.pdf:PDF},
  groups           = {Meta Learning},
  modificationdate = {2022-03-03T15:05:54},
}

@Misc{Fakoor2019MetaQ,
  author           = {Rasool Fakoor and Pratik Chaudhari and Stefano Soatto and Alexander J. Smola},
  month            = sep,
  title            = {Meta-Q-Learning},
  year             = {2019},
  abstract         = {This paper introduces Meta-Q-Learning (MQL), a new off-policy algorithm for  meta-Reinforcement Learning (meta-RL). MQL builds upon three simple ideas.
First, we show that Q-learning is competitive with state-of-the-art meta-RL algorithms if given access to a context variable that is a representation of the past  trajectory. Second, a multi-task objective to maximize the average reward across  the training tasks is an effective method to meta-train RL policies. Third, past data  from the meta-training replay buffer can be recycled to adapt the policy on a new  task using off-policy updates. MQL draws upon ideas in propensity estimation to  do so and thereby amplifies the amount of available data for adaptation. Experiments on standard continuous-control benchmarks suggest that MQL compares  favorably with the state of the art in meta-RL.},
  archiveprefix    = {arXiv},
  comment          = {直接采用多任务目标(no finetuning)，而不是元目标(after finetuning)；与pearl不同，metaQ适应时不仅更新context，还会对策略参数进行更新},
  creationdate     = {2022-03-04T11:49:17},
  eprint           = {1910.00125v2},
  file             = {:Fakoor_2020_Meta Q Learning.pdf:PDF},
  groups           = {Meta Learning},
  keywords         = {cs.LG, stat.ML},
  modificationdate = {2022-10-20T09:47:05},
  primaryclass     = {cs.LG},
  url              = {https://openreview.net/forum?id=SJeD3CEFPH},
}

@Article{Hu2021MetaRL,
  author           = {Hangkai Hu and Gao Huang and Xiang Li and Shiji Song},
  journal          = {{IEEE} Transactions on Neural Networks and Learning Systems},
  title            = {Meta-Reinforcement Learning With Dynamic Adaptiveness Distillation},
  year             = {2021},
  pages            = {1--11},
  abstract         = {Deep reinforcement learning is confronted with  problems of sampling inefficiency and poor task migration capability. Meta-reinforcement learning (meta-RL) enables  meta-learners to utilize the task-solving skills trained on similar tasks and quickly adapt to new tasks. However, meta-RL  methods lack enough queries toward the relationship between  task-agnostic exploitation of data and task-related knowledge  introduced by latent context, limiting their effectiveness and  generalization ability. In this article, we develop an algorithm for  off-policy meta-RL that can provide the meta-learners with selforiented cognition toward how they adapt to the family of tasks.
In our approach, we perform dynamic task-adaptiveness distillation to describe how the meta-learners adjust the exploration  strategy in the meta-training process. Our approach also enables  the meta-learners to balance the influence of task-agnostic selforiented adaption and task-related information through latent  context reorganization. In our experiments, our method achieves 10%-20% higher asymptotic reward than probabilistic embeddings for actor-critic RL (PEARL).},
  comment          = {from HuYue},
  creationdate     = {2022-03-04T20:53:57},
  doi              = {10.1109/tnnls.2021.3105407},
  file             = {:Hu_2021_Meta Reinforcement Learning with Dynamic Adaptiveness Distillation.pdf:PDF},
  groups           = {Meta Learning},
  modificationdate = {2022-03-29T20:29:50},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@MastersThesis{Robles2019Learning,
  author           = {J. Gomez Robles},
  school           = {Eindhoven University of Technology},
  title            = {Learning to reinforcement learn for neural architecture search},
  year             = {2019},
  address          = {Eindhoven University of Technology},
  month            = oct,
  abstract         = {Reinforcement learning (RL) is a goal-oriented learning solution that has proven to be successful for Neural Architecture Search (NAS) on the CIFAR and ImageNet datasets. However, a limitation of this approach is its high computational cost, making it unfeasible to replay it on other datasets. Through meta-learning, we could bring this cost down by adapting previously learned policies instead of learning them from scratch. In this work, we propose a deep meta-RL algorithm that learns an adaptive policy over a set of environments, making it possible to transfer it to previously unseen tasks. The algorithm was applied to various proof-of-concept environments in the past, but we adapt it to the NAS problem. We empirically investigate the agent's behavior during training when challenged to design chain-structured neural architectures for three datasets with increasing levels of hardness, to later fix the policy and evaluate it on two unseen datasets of different difficulty. Our results show that, under resource constraints, the agent effectively adapts its strategy during training to design better architectures than the ones designed by a standard RL algorithm, and can design good architectures during the evaluation on previously unseen environments. We also provide guidelines on the applicability of our framework in a more complex NAS setting by studying the progress of the agent when challenged to design multi-branch architectures.},
  creationdate     = {2022-03-05T14:53:39},
  file             = {:Robles_2019_Learning to Reinforcement Learn for Neural Architecture Search.pdf:PDF},
  groups           = {Meta Learning},
  keywords         = {Neural Architecture Search; Deep Meta-Reinforcement Learning; Image Classification},
  modificationdate = {2022-03-05T14:57:36},
  url              = {https://research.tue.nl/en/studentTheses/learning-to-reinforcement-learn-for-neural-architecture-search},
}

@PhdThesis{SunHosoya2019Meta,
  author           = {Lisheng Sun-Hosoya},
  school           = {Universite Paris-Sud},
  title            = {Meta-Learning as a Markov Decision Process},
  year             = {2019},
  address          = {Orsay},
  month            = dec,
  type             = {phdthesis},
  abstract         = {Machine Learning (ML) has enjoyed huge successes in recent years and an ever- growing number of real-world applications rely on it. However, designing promising algorithms for a specific problem still requires huge human effort. Automated Machine Learning (AutoML) aims at taking the human out of the loop and develop machines that generate / recommend good algorithms for a given ML tasks. AutoML is usually treated as an algorithm / hyper-parameter selection problems, existing approaches include Bayesian optimization, evolutionary algorithms as well as reinforcement learning. Among them, auto-sklearn which incorporates meta-learning techniques in their search initialization, ranks consistently well in AutoML challenges. This observation oriented my research to the Meta-Learning domain. This direction led me to develop a novel framework based on Markov Decision Processes (MDP) and reinforcement learning (RL). After a general introduction (Chapter 1), my thesis work starts with an in-depth analysis of the results of the AutoML challenge (Chapter 2). This analysis oriented my work towards meta-learning, leading me first to propose a formulation of AutoML as a recommendation problem, and ultimately to formulate a novel conceptualisation of the problem as a MDP (Chapter 3). In the MDP setting, the problem is brought back to filling up, as quickly and efficiently as possible, a meta-learning matrix S, in which lines correspond to ML tasks and columns to ML algorithms. A matrix element S(i, j) is the performance of algorithm j applied to task i. Searching efficiently for the best values in S allows us to identify quickly algorithms best suited to given tasks. In Chapter 4 the classical hyper-parameter optimization framework (HyperOpt) is first reviewed. In Chapter 5 a first meta-learning approach is introduced along the lines of our paper ActivMetaL that combines active learning and collaborative filtering techniques to predict the missing values in S. Our latest research applies RL to the MDP problem we defined to learn an efficient policy to explore S. We call this approach REVEAL and propose an analogy with a series of toy games to help visualize agents' strategies to reveal information progressively, e. g. masked areas of images to be classified, or ship positions in a battleship game. This line of research is developed in Chapter 6. The main results of my PhD project are: 1) HP / model selection: I have explored the Freeze-Thaw method and optimized the algorithm to enter the first AutoML challenge, achieving 3rd place in the final round (Chapter 3). 2) ActivMetaL: I have designed a new algorithm for active meta-learning (ActivMetaL) and compared it with other baseline methods on real-world and artificial data. This study demonstrated that ActiveMetaL is generally able to discover the best algorithm faster than baseline methods. 3) REVEAL: I developed a new conceptualization of meta-learning as a Markov Decision Process and put it into the more general framework of REVEAL games. With a master student intern, I developed agents that learns (with reinforcement learning) to predict the next best algorithm to be tried. To develop this agent, we used surrogate toy tasks of REVEAL games. We then applied our methods to AutoML problems. The work presented in my thesis is empirical in nature. Several real world meta-datasets were used in this research. Artificial and semi-artificial meta-datasets are also used in my work. The results indicate that RL is a viable approach to this problem, although much work remains to be done to optimize algorithms to make them scale to larger meta-learning problems.},
  creationdate     = {2022-03-05T14:57:57},
  file             = {:Sun-Hosoya_2019_Meta Learning As a Markov Decision Process.pdf:PDF},
  groups           = {Meta Learning},
  keywords         = {AutoML; Meta-learning; Markov Decision Process},
  modificationdate = {2022-03-05T15:00:58},
}

@PhdThesis{Fagerblom2020Pathology,
  author           = {Fagerblom, Freja},
  school           = {Linkoping University},
  title            = {Model-Agnostic Meta-Learning for Digital Pathology},
  year             = {2020},
  abstract         = {The performance of conventional deep neural networks tends to degrade when a domain shift is introduced, such as collecting data from a new site. Model-Agnostic Meta-Learning, or MAML, has achieved state-of-the-art performance in few-shot learning by finding initial parameters that adapt easily for new tasks. This thesis studies MAML in a digital pathology setting. Experiments show that a conventional model generalises poorly to data collected from another site. By annotating a few samples during inference however, a model with initial parameters obtained through MAML training can adapt to achieve better generalisation performance. It is also demonstrated that a simple transfer learning approach using a kNN classifier on features extracted from a conventional model yields good generalisation, but the variance caused by random sampling is higher. The results indicate that meta learning can lead to a lower annotation effort for machine learning in digital pathology while maintaining accuracy.},
  creationdate     = {2022-03-05T15:30:52},
  groups           = {Meta Learning},
  keywords         = {machine learning; model-agnostic meta-learning; maml; digital pathology; histopathology; meta learning; her2; domain shift; Computer Vision and Robotics (Autonomous Systems);},
  modificationdate = {2022-03-05T15:37:43},
}

@InCollection{Thrun1998Lifelong,
  author           = {Sebastian Thrun},
  booktitle        = {Learning to Learn},
  publisher        = {Springer {US}},
  title            = {Lifelong Learning Algorithms},
  year             = {1998},
  pages            = {181--209},
  abstract         = {Machine learning has not yet succeeded in the design of robust learning algorithms that generalize well from very small datasets. In contrast, humans often generalize correctly from only a single training example, even if the number of potentially relevant features is large. To do so, they successfully exploit knowledge acquired in previous learning tasks, to bias subsequent learning.
This paper investigates learning in a lifelong context. In contrast to most machine learning approaches, which aim at learning a single function in isolation, lifelong learning addresses situations where a learner faces a stream of learning tasks. Such scenarios provide the opportunity for synergetic effects that arise if knowledge is transferred across multiple learning tasks. To study the utility of transfer, several approaches to lifelong learning are proposed and evaluated in an object recognition domain. It is shown that all these algorithms generalize consistently more accurately from scarce training data than comparable “single-task” approaches.},
  creationdate     = {2022-03-11T10:57:56},
  keywords         = {Learning Task; Concept Learning; Query Point; Hypothesis Space; Generalization Accuracy},
  modificationdate = {2022-03-11T11:16:45},
}

@InProceedings{Paleja2021Explainable,
  author           = {Rohan R Paleja and Muyleng Ghuy and Nadun Ranawaka Arachchige and Reed Jensen and Matthew Gombolay},
  booktitle        = {Advances in Neural Information Processing Systems},
  title            = {The Utility of Explainable {AI} in Ad Hoc Human-Machine Teaming},
  year             = {2021},
  editor           = {A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
  abstract         = {Recent advances in machine learning have led to growing interest in Explainable AI (xAI) to enable humans to gain insight into the decision-making of machine learning models. Despite this recent interest, the utility of xAI techniques has not yet been characterized in human-machine teaming. Importantly, xAI offers the promise of enhancing team situational awareness (SA) and shared mental model development, which are the key characteristics of effective human-machine teams. Rapidly developing such mental models is especially critical in ad hoc human-machine teaming, where agents do not have a priori knowledge of others' decision-making strategies. In this paper, we present two novel human-subject experiments quantifying the benefits of deploying xAI techniques within a human-machine teaming scenario. First, we show that xAI techniques can support SA (. Second, we examine how different SA levels induced via a collaborative AI policy abstraction affect ad hoc human-machine teaming performance. Importantly, we find that the benefits of xAI are not universal, as there is a strong dependence on the composition of the human-machine team. Novices benefit from xAI providing increased SA () but are susceptible to cognitive overhead (). On the other hand, expert performance degrades with the addition of xAI-based support (), indicating that the cost of paying attention to the xAI outweighs the benefits obtained from being provided additional information to enhance SA. Our results demonstrate that researchers must deliberately design and deploy the right xAI techniques in the right scenario by carefully considering human-machine team composition and how the xAI method augments SA.},
  comment          = {Code: https://github.com/CORE-Robotics-Lab/Utility-of-Explainable-AI-NeurIPS2021},
  creationdate     = {2022-03-11T11:43:40},
  file             = {:Paleja_2021_The Utility of Explainable AI in Ad Hoc Human Machine Teaming.pdf:PDF},
  groups           = {Human-Agent Ad Hoc},
  modificationdate = {2022-09-03T10:15:20},
  ranking          = {rank3},
  url              = {https://openreview.net/forum?id=w6U6g5Bvug},
}

@InProceedings{Stone2010Adhoc,
  author           = {Stone, Peter and Kaminka, Gal and Kraus, Sarit and Rosenschein, Jeffrey},
  booktitle        = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title            = {Ad Hoc Autonomous Agent Teams: Collaboration without Pre-Coordination},
  year             = {2010},
  number           = {1},
  pages            = {1504--1509},
  volume           = {24},
  abstract         = {As autonomous agents proliferate in the real world,  both in software and robotic settings, they will increasingly need to band together for cooperative activities  with previously unfamiliar teammates. In such ad hoc  team settings, team strategies cannot be developed a  priori. Rather, an agent must be prepared to cooperate with many types of teammates: it must collaborate  without pre-coordination. This paper challenges the AI  community to develop theory and to implement prototypes of ad hoc team agents. It defines the concept of  ad hoc team agents, specifies an evaluation paradigm,  and provides examples of possible theoretical and empirical approaches to challenge. The goal is to encourage progress towards this ambitious, newly realistic,  and increasingly important research goal.},
  creationdate     = {2022-03-14T10:00:28},
  file             = {:Stone_2010_Ad Hoc Autonomous Agent Teams_ Collaboration without Pre Coordination.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  modificationdate = {2022-03-29T20:29:06},
}

@Article{Albrecht2016Special,
  author           = {Stefano V. Albrecht and Somchaya Liemhetcharat and Peter Stone},
  journal          = {Autonomous Agents and Multi-Agent Systems},
  title            = {Special issue on multiagent interaction without prior coordination: guest editorial},
  year             = {2016},
  month            = dec,
  number           = {4},
  pages            = {765--766},
  volume           = {31},
  abstract         = {This special issue of the Journal of Autonomous Agents and Multi-Agent Systems sought research articles on the emerging topic of multiagent interaction without prior coordination. Topics of interest included empirical and theoretical investigations of issues arising from assumptions of prior coordination, as well as solutions in the form of novel models and algorithms for effective multiagent interaction without prior coordination.},
  creationdate     = {2022-03-14T10:06:11},
  doi              = {10.1007/s10458-016-9358-0},
  file             = {:Albrecht_2016_Special Issue on Multiagent Interaction without Prior Coordination_ Guest Editorial.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  modificationdate = {2022-06-15T16:03:58},
  publisher        = {Springer Science and Business Media {LLC}},
}

@InProceedings{Carroll2019Overcooked,
  author           = {Carroll, Micah and Shah, Rohin and Ho, Mark K and Griffiths, Tom and Seshia, Sanjit and Abbeel, Pieter and Dragan, Anca},
  booktitle        = {Advances in Neural Information Processing Systems},
  title            = {On the Utility of Learning about Humans for Human-AI Coordination},
  year             = {2019},
  editor           = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages            = {5174--5185},
  publisher        = {Curran Associates, Inc.},
  volume           = {32},
  abstract         = {While we would like agents that can coordinate with humans, current algorithms such as self-play and population-based training create agents that can coordinate with themselves. Agents that assume their partner to be optimal or similar to them can converge to coordination protocols that fail to understand and be understood by humans. To demonstrate this, we introduce a simple environment that requires challenging coordination, based on the popular game Overcooked, and learn a simple model that mimics human play. We evaluate the performance of agents trained via self-play and population-based training. These agents perform very well when paired with themselves, but when paired with our human model, they are significantly worse than agents designed to play with the human model. An experiment with a planning algorithm yields the same conclusion, though only when the human-aware planner is given the exact human model that it is playing with. A user study with real humans shows this pattern as well, though less strongly. Qualitatively, we find that the gains come from having the agent adapt to the human's gameplay. Given this result, we suggest several approaches for designing agents that learn about humans in order to better coordinate with them. Code is available at https://github.com/HumanCompatibleAI/overcooked_ai.},
  comment          = {Overcooked},
  creationdate     = {2022-03-14T11:21:47},
  file             = {:Carroll_2019_On the Utility of Learning about Humans for Human AI Coordination.pdf:PDF},
  groups           = {Human-Agent Ad Hoc},
  modificationdate = {2022-09-03T10:14:58},
  url              = {https://proceedings.neurips.cc/paper/2019/hash/f5b1b89d98b7286673128a5fb112cb9a-Abstract.html},
}

@Misc{Zintgraf2021Bayesian,
  author           = {Luisa Zintgraf and Sam Devlin and Kamil Ciosek and Shimon Whiteson and Katja Hofmann},
  month            = jan,
  title            = {Deep Interactive Bayesian Reinforcement Learning via Meta-Learning},
  year             = {2021},
  abstract         = {Agents that interact with other agents often do not know a priori what the other agents' strategies are, but have to maximise their own online return while interacting with and learning about others. The optimal adaptive behaviour under uncertainty over the other agents' strategies w.r.t. some prior can in principle be computed using the Interactive Bayesian Reinforcement Learning framework. Unfortunately, doing so is intractable in most settings, and existing approximation methods are restricted to small tasks. To overcome this, we propose to meta-learn approximate belief inference and Bayes-optimal behaviour for a given prior. To model beliefs over other agents, we combine sequential and hierarchical Variational Auto-Encoders, and meta-train this inference model alongside the policy. We show empirically that our approach outperforms existing methods that use a model-free approach, sample from the approximate posterior, maintain memory-free models of others, or do not fully utilise the known structure of the environment.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-03-14T20:30:54},
  eprint           = {2101.03864},
  file             = {:Zintgraf_2021_Deep Interactive Bayesian Reinforcement Learning Via Meta Learning.pdf:PDF},
  groups           = {Meta Learning},
  keywords         = {cs.LG, cs.MA},
  modificationdate = {2022-03-14T20:31:59},
  primaryclass     = {cs.LG},
}

@InProceedings{Li2020Generative,
  author           = {Li, Guangyu and Jiang, Bo and Zhu, Hao and Che, Zhengping and Liu, Yan},
  booktitle        = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title            = {Generative Attention Networks for Multi-Agent Behavioral Modeling},
  year             = {2020},
  month            = apr,
  number           = {05},
  pages            = {7195--7202},
  publisher        = {Association for the Advancement of Artificial Intelligence ({AAAI})},
  volume           = {34},
  abstract         = {Understanding and modeling behavior of multi-agent systems is a central step for artificial intelligence. Here we present a deep generative model which captures behavior generating process of multi-agent systems, supports accurate predictions and inference, infers how agents interact in a complex system, as well as identifies agent groups and interaction types. Built upon advances in deep generative models and a novel attention mechanism, our model can learn interactions in highly heterogeneous systems with linear complexity in the number of agents. We apply this model to three multi-agent systems in different domains and evaluate performance on a diverse set of tasks including behavior prediction, interaction analysis and system identification. Experimental results demonstrate its ability to model multi-agent systems, yielding improved performance over competitive baselines. We also show the model can successfully identify agent groups and interaction types in these systems. Our model offers new opportunities to predict complex multi-agent behaviors and takes a step forward in understanding interactions in multi-agent systems.},
  creationdate     = {2022-03-18T21:21:06},
  doi              = {10.1609/aaai.v34i05.6209},
  file             = {:Li_2020_Generative Attention Networks for Multi Agent Behavioral Modeling.pdf:PDF},
  modificationdate = {2022-06-15T16:10:52},
  ranking          = {rank4},
}

@Misc{Levine2020OfflineRLReview,
  author           = {Sergey Levine and Aviral Kumar and George Tucker and Justin Fu},
  month            = may,
  title            = {Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems},
  year             = {2020},
  abstract         = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},
  archiveprefix    = {arXiv},
  comment          = {离线强化学习 off-line RL},
  creationdate     = {2022-03-19T22:33:44},
  eprint           = {2005.01643},
  file             = {:Levine_2020_Offline Reinforcement Learning_ Tutorial, Review, and Perspectives on Open Problems.pdf:PDF},
  keywords         = {cs.LG, cs.AI, stat.ML},
  modificationdate = {2022-03-19T22:35:38},
  primaryclass     = {cs.LG},
}

@InProceedings{Liu2021CurriculumMeta,
  author           = {Jiangbo Liu and Zhenyong Fu},
  booktitle        = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
  title            = {Curriculum Meta Learning: Learning to Learn from Easy to Hard},
  year             = {2021},
  month            = oct,
  pages            = {1571--1576},
  publisher        = {Association for Computing Machinery},
  abstract         = {Meta-learning is a machine learning paradigm that extracts crosstask knowledge by learning a large number of subtasks, to fast adapt to new tasks. Many meta-learning methods are widely applied in few-shot classification. These methods adopt an episodic training strategy, and the learning subtasks are sampled uniformly from the task distribution. In this paper, we explore the effect of the order of training subtasks on the performance of different meta-learning algorithms and propose a curriculum learning framework to improve the generalization performance. We define the hardness of subtasks at the class level and guide the model to learn training subtasks from easy to hard. We evaluate our curriculum learning framework on two few-shot classification benchmarks (mini-ImageNet and FC100), and it achieves improvements across different meta-learning algorithms and datasets. In the cross-domain scenario, we compare the performance of different meta learning algorithms under three curriculum settings. The results show that our CL approach improves significantly the generalization performance of different meta-learning methods.},
  creationdate     = {2022-03-20T20:08:42},
  doi              = {10.1145/3501409.3501686},
  file             = {:Liu_2021_Curriculum Meta Learning_ Learning to Learn from Easy to Hard.pdf:PDF},
  groups           = {Meta Learning},
  keywords         = {Meta learning, Curriculum learning, Few-shot learning},
  modificationdate = {2022-06-15T16:10:58},
}

@Article{Bertsekas2021PolicyIteration,
  author           = {Dimitri Bertsekas},
  journal          = {{IEEE}/{CAA} Journal of Automatica Sinica},
  title            = {Multiagent Reinforcement Learning: Rollout and Policy Iteration},
  year             = {2021},
  month            = feb,
  number           = {2},
  pages            = {249--272},
  volume           = {8},
  abstract         = {We discuss the solution of complex multistage decision problems using methods that are based on the idea of policy iteration (PI), i.e., start from some base policy and generate an improved policy. Rollout is the simplest method of this type, where just one improved policy is generated. We can view PI as repeated application of rollout, where the rollout policy at each iteration serves as the base policy for the next iteration. In contrast with PI, rollout has a robustness property: it can be applied on-line and is suitable for on-line replanning. Moreover, rollout can use as base policy one of the policies produced by PI, thereby improving on that policy. This is the type of scheme underlying the prominently successful AlphaZero chess program. In this paper we focus on rollout and PI-like methods for problems where the control consists of multiple components each selected (conceptually) by a separate agent. This is the class of multiagent problems where the agents have a shared objective function, and a shared and perfect state information. Based on a problem reformulation that trades off control space complexity with state space complexity, we develop an approach, whereby at every stage, the agents sequentially (one-at-a-time) execute a local rollout algorithm that uses a base policy, together with some coordinating information from the other agents. The amount of total computation required at every stage grows linearly with the number of agents. By contrast, in the standard rollout algorithm, the amount of total computation grows exponentially with the number of agents. Despite the dramatic reduction in required computation, we show that our multiagent rollout algorithm has the fundamental cost improvement property of standard rollout: it guarantees an improved performance relative to the base policy. We also discuss autonomous multiagent rollout schemes that allow the agents to make decisions autonomously through the use of precomputed signaling information, which is sufficient to maintain the cost improvement property, without any on-line coordination of control selection between the agents. For discounted and other infinite horizon problems, we also consider exact and approximate PI algorithms involving a new type of one-agent-at-a-time policy improvement operation. For one of our PI algorithms, we prove convergence to an agent-by-agent optimal policy, thus establishing a connection with the theory of teams. For another PI algorithm, which is executed over a more complex state space, we prove convergence to an optimal policy. Approximate forms of these algorithms are also given, based on the use of policy and value neural networks. These PI algorithms, in both their exact and their approximate form are strictly off-line methods, but they can be used to provide a base policy for use in an on-line multiagent rollout scheme.},
  creationdate     = {2022-03-20T22:34:52},
  doi              = {10.1109/jas.2021.1003814},
  file             = {:Bertsekas_2021_Multiagent Reinforcement Learning_ Rollout and Policy Iteration.pdf:PDF},
  modificationdate = {2022-06-15T16:04:14},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Misc{Liu2021IntentionSharing,
  author           = {Zeyang Liu and Lipeng Wan and Xue sui and Kewu Sun and Xuguang Lan},
  month            = dec,
  title            = {Multi-Agent Intention Sharing via Leader-Follower Forest},
  year             = {2021},
  abstract         = {Intention sharing is crucial for efficient cooperation under partially observable environments in multi-agent reinforcement learning (MARL). However, message deceiving, i.e., a mismatch between the propagated intentions and the final decisions, may happen when agents change strategies simultaneously according to received intentions. Message deceiving leads to potential miscoordination and difficulty for policy learning. This paper proposes the leader-follower forest (LFF) to learn the hierarchical relationship between agents based on interdependencies, achieving one-sided intention sharing in multi-agent communication. By limiting the flowings of intentions through directed edges, intention sharing via LFF (IS-LFF) can eliminate message deceiving effectively and achieve better coordination. In addition, a twostage learning algorithm is proposed to train the forest and the agent network. We evaluate IS-LFF on multiple partially observable MARL benchmarks, and the experimental results show that our method outperforms state-of-the-art communication algorithms.},
  archiveprefix    = {arXiv},
  comment          = {在部分可观察环境下，意图共享是多智能体强化学习中有效协作的关键。然而，当agent根据接收到的意图同时改变策略时，可能会出现信息欺骗，即传播的意图与最终决策之间的不匹配。信息欺骗可能导致政策学习的不协调和困难。本文提出了leader-follower forest (LFF)，基于相互依赖关系来学习agent之间的层次关系，实现多agent通信中的单向意图共享。通过有向边限制意图的流动，通过LFF实现意图共享(IS-LFF)可以有效地消除消息欺骗，实现更好的协调。此外，提出了一种两阶段学习算法来训练森林和代理网络。我们在多个部分可观测MARL基准上评估了IS-LFF，实验结果表明，我们的方法优于目前最先进的通信算法。},
  creationdate     = {2022-03-22T20:34:49},
  eprint           = {2112.01078},
  file             = {:Liu_2021_Multi Agent Intention Sharing Via Leader Follower Forest.pdf:PDF},
  keywords         = {cs.MA},
  modificationdate = {2022-03-22T22:56:40},
  primaryclass     = {cs.MA},
}

@Article{Genter2016Flock,
  author           = {Katie Genter and Peter Stone},
  journal          = {Acta Polytechnica},
  title            = {{Ad} {Hoc} {Teamwork} {Behaviors} {for} {Influencing} a {Flock}},
  year             = {2016},
  month            = feb,
  number           = {1},
  pages            = {18--26},
  volume           = {56},
  abstract         = {Ad hoc teamwork refers to the challenge of designing agents that can influence the  behavior of a team, without prior coordination with its teammates. This paper considers influencing  a flock of simple robotic agents to adopt a desired behavior within the context of ad hoc teamwork.
Specifically, we examine how the ad hoc agents should behave in order to orient a flock towards a  target heading as quickly as possible when given knowledge of, but no direct control over, the behavior  of the flock. We introduce three algorithms which the ad hoc agents can use to influence the flock, and  we examine the relative importance of coordinating the ad hoc agents versus planning farther ahead  when given fixed computational resources. We present detailed experimental results for each of these  algorithms, concluding that in this setting, inter-agent coordination and deeper lookahead planning  are no more beneficial than short-term lookahead planning.},
  comment          = {relworks中涉及了steam一些传统方法},
  creationdate     = {2022-03-22T22:02:54},
  doi              = {10.14311/app.2016.56.0018},
  file             = {:Genter_2016_Ad Hoc Teamwork Behaviors for Influencing a Flock.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  modificationdate = {2022-10-31T11:04:27},
  publisher        = {Czech Technical University in Prague - Central Library},
  ranking          = {rank5},
}

@Misc{Yao2021POAsynchronous,
  author           = {Meng Yao and Qiyue Yin and Jun Yang and Tongtong Yu and Shengqi Shen and Junge Zhang and Bin Liang and Kaiqi Huang},
  month            = dec,
  title            = {The Partially Observable Asynchronous Multi-Agent Cooperation Challenge},
  year             = {2021},
  abstract         = {Multi-agent reinforcement learning (MARL) has received increasing attention for its applications in various domains. Researchers have paid much attention on its partially observable and cooperative settings for meeting real-world requirements. For testing performance of different algorithms, standardized environments are designed such as the StarCraft Multi-Agent Challenge, which is one of the most successful MARL benchmarks. To our best knowledge, most of current environments are synchronous, where agents execute actions in the same pace. However, heterogeneous agents usually have their own action spaces and there is no guarantee for actions from different agents to have the same executed cycle, which leads to asynchronous multi-agent cooperation. Inspired from the Wargame, a confrontation game between two armies abstracted from real world environment, we propose the first Partially Observable Asynchronous multi-agent Cooperation challenge (POAC) for the MARL community. Specifically, POAC supports two teams of heterogeneous agents to fight with each other, where an agent selects actions based on its own observations and cooperates asynchronously with its allies. Moreover, POAC is a light weight, flexible and easy to use environment, which can be configured by users to meet different experimental requirements such as self-play model, human-AI model and so on. Along with our benchmark, we offer six game scenarios of varying difficulties with the built-in rule-based AI as opponents. Finally, since most MARL algorithms are designed for synchronous agents, we revise several representatives to meet the asynchronous setting, and the relatively poor experimental results validate the challenge of POAC. Source code is released in \url{http://turingai.ia.ac.cn/data\_center/show}.},
  archiveprefix    = {arXiv},
  comment          = {兵棋推演 http://turingai.ia.ac.cn/data_center/show},
  creationdate     = {2022-03-22T22:57:00},
  eprint           = {2112.03809},
  file             = {:Yao_2021_The Partially Observable Asynchronous Multi Agent Cooperation Challenge.pdf:PDF},
  keywords         = {cs.MA},
  modificationdate = {2022-03-22T23:01:28},
  primaryclass     = {cs.MA},
}

@Misc{Mahmud2021Learning,
  author           = {Rafid Ameer Mahmud and Fahim Faisal and Saaduddin Mahmud and Md. Mosaddek Khan},
  month            = oct,
  title            = {Learning Cooperation and Online Planning Through Simulation and Graph Convolutional Network},
  year             = {2021},
  abstract         = {Multi-agent Markov Decision Process (MMDP) has been an effective way of modelling sequential decision making algorithms for multi-agent cooperative environments. A number of algorithms based on centralized and decentralized planning have been developed in this domain. However, dynamically changing environment, coupled with exponential size of the state and joint action space, make it difficult for these algorithms to provide both efficiency and scalability. Recently, Centralized planning algorithm FV-MCTS-MP and decentralized planning algorithm \textit{Alternate maximization with Behavioural Cloning} (ABC) have achieved notable performance in solving MMDPs. However, they are not capable of adapting to dynamically changing environments and accounting for the lack of communication among agents, respectively. Against this background, we introduce a simulation based online planning algorithm, that we call SiCLOP, for multi-agent cooperative environments. Specifically, SiCLOP tailors Monte Carlo Tree Search (MCTS) and uses Coordination Graph (CG) and Graph Neural Network (GCN) to learn cooperation and provides real time solution of a MMDP problem. It also improves scalability through an effective pruning of action space. Additionally, unlike FV-MCTS-MP and ABC, SiCLOP supports transfer learning, which enables learned agents to operate in different environments. We also provide theoretical discussion about the convergence property of our algorithm within the context of multi-agent settings. Finally, our extensive empirical results show that SiCLOP significantly outperforms the state-of-the-art online planning algorithms.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-03-23T12:13:06},
  eprint           = {2110.08480},
  file             = {:Mahmud_2021_Learning Cooperation and Online Planning through Simulation and Graph Convolutional Network.pdf:PDF},
  groups           = {Planning},
  keywords         = {cs.AI},
  modificationdate = {2022-03-23T12:13:46},
  primaryclass     = {cs.AI},
}

@InProceedings{Zerbel2019MAMCTS,
  author           = {Zerbel, Nicholas and Yliniemi, Logan},
  booktitle        = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
  title            = {Multiagent monte carlo tree search},
  year             = {2019},
  pages            = {2309--2311},
  abstract         = {Monte Carlo Tree Search (MCTS) is a best-first search which is efficient in large search spaces and is effective at balancing exploration versus exploitation. In this work, we introduce a novel extension for MCTS, called Multiagent Monte Carlo Tree Search (MAMCTS), which pairs MCTS with difference evaluations. We demonstrate the performance of MAMCTS in a cooperative, multiagent path-planning domain called Multiagent Gridworld. We show that MAMCTS using difference evaluations outperforms MAMCTS using local rewards by up to 31.4% and MAMCTS using the global reward by up to 88.9% for a system with 1,000 agents.},
  creationdate     = {2022-03-23T12:17:23},
  file             = {:Zerbel_2019_Multiagent Monte Carlo Tree Search.pdf:PDF},
  groups           = {Planning},
  modificationdate = {2022-03-23T12:25:08},
}

@MastersThesis{Zerbel2018ThesisMAMCTS,
  author           = {Zerbel, Nicholas},
  school           = {University of Nevada, Reno},
  title            = {Multiagent monte carlo tree search with difference evaluations and evolved rollout policy},
  year             = {2018},
  abstract         = {Monte Carlo Tree Search (MCTS) is a best-first search algorithm that has produced  many breakthroughs in AI research. MCTS has been applied to a wide variety of domains including turn-based board games, real-time strategy games, multiagent systems, and optimization problems. In addition to its ability to function in a wide  variety of domains, MCTS is also a suitable candidate for performance improving  modifications such as the improvement of its default rollout policy. In this work,  we propose an enhancement to MCTS called Multiagent Monte Carlo Tree Search
(MAMCTS) which incorporates multiagent credit evaluations in the form of Difference Evaluations. We show that MAMCTS can be successfully applied to a cooperative system called Multiagent Gridworld. We then show that the use of Difference
Evaluations in MAMCTS offers superior control over agent decision making compared  with other forms of multiagent credit evaluations, namely Global Evaluations. Furthermore, we show that the default rollout policy can be improved using a Genetic
Algorithm, with (µ + λ) selection, resulting in a 37.6% increase in overall system  performance within the training domain. Finally, we show that the trained rollout  policy can be transferred to more complex multiagent systems resulting in as high as  a 14.6% increase in system performance compared to the default rollout policy.},
  creationdate     = {2022-03-23T12:19:10},
  file             = {:Zerbel_2018_Multiagent Monte Carlo Tree Search with Difference Evaluations and Evolved Rollout Policy.pdf:PDF},
  groups           = {Planning},
  modificationdate = {2022-03-29T20:28:13},
}

@Misc{Choudhury2021AnytimePlanning,
  author           = {Shushman Choudhury and Jayesh K. Gupta and Peter Morales and Mykel J. Kochenderfer},
  month            = jan,
  title            = {Scalable Anytime Planning for Multi-Agent MDPs},
  year             = {2021},
  abstract         = {We present a scalable tree search planning algorithm for large multi-agent sequential decision problems that require dynamic collaboration. Teams of agents need to coordinate decisions in many domains, but naive approaches fail due to the exponential growth of the joint action space with the number of agents. We circumvent this complexity through an anytime approach that allows us to trade computation for approximation quality and also dynamically coordinate actions. Our algorithm comprises three elements: online planning with Monte Carlo Tree Search (MCTS), factored representations of local agent interactions with coordination graphs, and the iterative Max-Plus method for joint action selection. We evaluate our approach on the benchmark SysAdmin domain with static coordination graphs and achieve comparable performance with much lower computation cost than our MCTS baselines. We also introduce a multi-drone delivery domain with dynamic, i.e., state-dependent coordination graphs, and demonstrate how our approach scales to large problems on this domain that are intractable for other MCTS methods. We provide an open-source implementation of our algorithm at https://github.com/JuliaPOMDP/FactoredValueMCTS.jl.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-03-23T12:30:17},
  eprint           = {2101.04788},
  file             = {:Choudhury_2021_Scalable Anytime Planning for Multi Agent MDPs.pdf:PDF},
  groups           = {Planning},
  keywords         = {cs.AI, cs.MA},
  modificationdate = {2022-03-23T12:30:57},
  primaryclass     = {cs.AI},
}

@Article{Zhang2018SafePathPlanning,
  author           = {Hong-Mei Zhang and Ming-Long Li and Le Yang},
  journal          = {Algorithms},
  title            = {Safe Path Planning of Mobile Robot Based on Improved A{\ast} Algorithm in Complex Terrains},
  year             = {2018},
  month            = apr,
  number           = {4},
  pages            = {44},
  volume           = {11},
  abstract         = {The A* algorithm has been widely investigated and applied in path planning problems,  but it does not fully consider the safety and smoothness of the path. Therefore, an improved A*  algorithm is presented in this paper. Firstly, a new environment modeling method is proposed in  which the evaluation function of A* algorithm is improved by taking the safety cost into account.
This results in a safer path which can stay farther away from obstacles. Then a new path smoothing  method is proposed, which introduces a path evaluation mechanism into the smoothing process.
This method is then applied to smoothing the path without safety reduction. Secondly, with respect to  path planning problems in complex terrains, a complex terrain environment model is established in  which the distance and safety cost of the evaluation function of the A* algorithm are converted into  time cost. This results in a unification of units as well as a clarity in their physical meanings.
The simulation results show that the improved A* algorithm can greatly improve the safety  and smoothness of the planned path and the movement time of the robot in complex terrain is  greatly reduced.},
  creationdate     = {2022-03-23T12:31:53},
  doi              = {10.3390/a11040044},
  file             = {:Zhang_2018_Safe Path Planning of Mobile Robot Based on Improved A_ Algorithm in Complex Terrains.pdf:PDF},
  groups           = {Planning},
  modificationdate = {2022-06-15T16:04:42},
  publisher        = {{MDPI} {AG}},
}

@InProceedings{Liu2021Coach,
  author           = {Liu, Bo and Liu, Qiang and Stone, Peter and Garg, Animesh and Zhu, Yuke and Anandkumar, Anima},
  booktitle        = {Proceedings of the 38th International Conference on Machine Learning},
  title            = {Coach-Player Multi-agent Reinforcement Learning for Dynamic Team Composition},
  year             = {2021},
  editor           = {Meila, Marina and Zhang, Tong},
  month            = jul,
  pages            = {6860--6870},
  publisher        = {PMLR},
  series           = {Proceedings of Machine Learning Research},
  volume           = {139},
  abstract         = {In real-world multi-agent systems, agents with different capabilities may join or leave without altering the team’s overarching goals. Coordinating teams with such dynamic composition is challenging: the optimal team strategy varies with the composition. We propose COPA, a coach-player framework to tackle this problem. We assume the coach has a global view of the environment and coordinates the players, who only have partial views, by distributing individual strategies. Specifically, we 1) adopt the attention mechanism for both the coach and the players; 2) propose a variational objective to regularize learning; and 3) design an adaptive communication method to let the coach decide when to communicate with the players. We validate our methods on a resource collection task, a rescue game, and the StarCraft micromanagement tasks. We demonstrate zero-shot generalization to new team compositions. Our method achieves comparable or better performance than the setting where all players have a full view of the environment. Moreover, we see that the performance remains high even when the coach communicates as little as 13% of the time using the adaptive communication strategy.},
  file             = {:Liu_2021_Coach Player Multi Agent Reinforcement Learning for Dynamic Team Composition.pdf:PDF;:Liu_2021_Coach Player Multi Agent Reinforcement Learning for Dynamic Team Composition-supp.pdf:PDF},
  modificationdate = {2023-01-31T20:02:55},
  url              = {https://proceedings.mlr.press/v139/liu21m.html},
}

@Misc{Kim2019Schednet,
  author           = {Daewoo Kim and Sangwoo Moon and David Hostallero and Wan Ju Kang and Taeyoung Lee and Kyunghwan Son and Yung Yi},
  month            = feb,
  title            = {Learning to Schedule Communication in Multi-agent Reinforcement Learning},
  year             = {2019},
  abstract         = {Many real-world reinforcement learning tasks require multiple agents to make sequential decisions under the agents' interaction, where well-coordinated actions among the agents are crucial to achieve the target goal better at these tasks. One way to accelerate the coordination effect is to enable multiple agents to communicate with each other in a distributed manner and behave as a group. In this paper, we study a practical scenario when (i) the communication bandwidth is limited and (ii) the agents share the communication medium so that only a restricted number of agents are able to simultaneously use the medium, as in the state-of-the-art wireless networking standards. This calls for a certain form of communication scheduling. In that regard, we propose a multi-agent deep reinforcement learning framework, called SchedNet, in which agents learn how to schedule themselves, how to encode the messages, and how to select actions based on received messages. SchedNet is capable of deciding which agents should be entitled to broadcasting their (encoded) messages, by learning the importance of each agent's partially observed information. We evaluate SchedNet against multiple baselines under two different applications, namely, cooperative communication and navigation, and predator-prey. Our experiments show a non-negligible performance gap between SchedNet and other mechanisms such as the ones without communication and with vanilla scheduling methods, e.g., round robin, ranging from 32% to 43%.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-03-23T16:26:59},
  eprint           = {1902.01554},
  file             = {:Kim_2019_Learning to Schedule Communication in Multi Agent Reinforcement Learning.pdf:PDF},
  groups           = {MARL},
  keywords         = {cs.AI, cs.LG, cs.MA},
  modificationdate = {2022-03-23T16:27:37},
  primaryclass     = {cs.AI},
}

@InProceedings{Du2021Learning,
  author           = {Du, Yali and Liu, Bo and Moens, Vincent and Liu, Ziqi and Ren, Zhicheng and Wang, Jun and Chen, Xu and Zhang, Haifeng},
  booktitle        = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  title            = {Learning correlated communication topology in multi-agent reinforcement learning},
  year             = {2021},
  address          = {Richland, SC},
  pages            = {456--464},
  publisher        = {International Foundation for Autonomous Agents and Multiagent Systems},
  abstract         = {Communication improves the efficiency and convergence of multi-agent learning. Existing study of agent communication has been limited on predefined fixed connections. While an attention mechanism exists and is useful for scheduling the communication between agents, it, however, largely ignores the dynamical nature of communication and thus the correlation between agents' connections. In this work, we adopt a normalizing flow to encode correlation between agents interactions. The dynamical communication topology is directly learned by maximizing the agent rewards. In our end-to-end formulation, the communication structure is learned by considering it as a hidden dynamical variable. We realize centralized training of critics and graph reasoning policy, and decentralized execution from local observation and message that are received through the learned dynamical communication topology. Experiments on cooperative navigation in the particle world and adaptive traffic control tasks demonstrate the effectiveness of our method.},
  creationdate     = {2022-03-23T16:29:55},
  file             = {:Du_2021_Learning Correlated Communication Topology in Multi Agent Reinforcement Learning.pdf:PDF},
  groups           = {MARL},
  keywords         = {multi-agent systems, reinforcement learning, communication topology},
  modificationdate = {2023-01-05T22:05:24},
  ranking          = {rank5},
}

@Misc{Wang2021QPLEX,
  author           = {Wang, Jianhao and Ren, Zhizhou and Liu, Terry and Yu, Yang and Zhang, Chongjie},
  month            = oct,
  title            = {{QPLEX}: {Duplex} {Dueling} {Multi}-{Agent} {Q}-{Learning}},
  year             = {2021},
  abstract         = {We explore value-based multi-agent reinforcement learning (MARL) in the popular paradigm of centralized training with decentralized execution (CTDE). CTDE has an important concept, Individual-Global-Max (IGM) principle, which requires the consistency between joint and local action selections to support efficient local decision-making. However, in order to achieve scalability, existing MARL methods either limit representation expressiveness of their value function classes or relax the IGM consistency, which may suffer from instability risk or may not perform well in complex domains. This paper presents a novel MARL approach, called duPLEX dueling multi-agent Q-learning (QPLEX), which takes a duplex dueling network architecture to factorize the joint value function. This duplex dueling structure encodes the IGM principle into the neural network architecture and thus enables efficient value function learning. Theoretical analysis shows that QPLEX achieves a complete IGM function class. Empirical experiments on StarCraft II micromanagement tasks demonstrate that QPLEX significantly outperforms state-of-the-art baselines in both online and offline data collection settings, and also reveal that QPLEX achieves high sample efficiency and can benefit from offline datasets without additional online exploration.},
  creationdate     = {2022-03-23T16:46:31},
  eprint           = {2008.01062},
  file             = {:Wang_2021_QPLEX_ Duplex Dueling Multi Agent Q Learning.pdf:PDF},
  keywords         = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems, Statistics - Machine Learning},
  modificationdate = {2022-03-23T16:48:40},
  ranking          = {rank2},
  shorttitle       = {{QPLEX}},
  url              = {http://arxiv.org/abs/2008.01062},
  urldate          = {2022-03-23},
}

@Article{Tampuu2017Multiagent,
  author           = {Tampuu, Ardi and Matiisen, Tambet and Kodelja, Dorian and Kuzovkin, Ilya and Korjus, Kristjan and Aru, Juhan and Aru, Jaan and Vicente, Raul},
  journal          = {PLoS ONE},
  title            = {Multiagent Cooperation and Competition with Deep Reinforcement Learning},
  year             = {2017},
  number           = {4},
  volume           = {12},
  abstract         = {Evolution of cooperation and competition can appear when multiple adaptive agents share a  biological, social, or technological niche. In the present work we study how cooperation and  competition emerge between autonomous agents that learn by reinforcement while using  only their raw visual input as the state representation. In particular, we extend the Deep QLearning framework to multiagent environments to investigate the interaction between two  learning agents in the well-known video game Pong. By manipulating the classical rewarding scheme of Pong we show how competitive and collaborative behaviors emerge. We also  describe the progression from competitive to collaborative behavior when the incentive to  cooperate is increased. Finally we show how learning by playing against another adaptive  agent, instead of against a hard-wired algorithm, results in more robust strategies. The present work shows that Deep Q-Networks can become a useful tool for studying decentralized  learning of multiagent systems coping with high-dimensional environments.},
  comment          = {只使用原始视觉输入，考虑合作和竞争，尝试将DQL扩展到多agent},
  copyright        = {Creative Commons Attribution 4.0 International},
  creationdate     = {2022-03-23T21:07:36},
  doi              = {10.3929/ETHZ-B-000130290},
  file             = {:Tampuu_2017_Multiagent Cooperation and Competition with Deep Reinforcement Learning.pdf:PDF},
  language         = {en},
  modificationdate = {2022-05-01T13:34:39},
  publisher        = {ETH Zurich},
  url              = {https://www.research-collection.ethz.ch/handle/20.500.11850/130290},
}

@Misc{Rashid2018QMIX,
  author           = {Tabish Rashid and Mikayel Samvelyan and Christian Schroeder de Witt and Gregory Farquhar and Jakob Foerster and Shimon Whiteson},
  month            = mar,
  title            = {QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning},
  year             = {2018},
  abstract         = {In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-03-23T21:52:01},
  eprint           = {1803.11485},
  file             = {:Rashid_2018_QMIX_ Monotonic Value Function Factorisation for Deep Multi Agent Reinforcement Learning.pdf:PDF},
  groups           = {MARL, RTS-MARL},
  keywords         = {cs.LG, cs.MA, stat.ML},
  modificationdate = {2022-06-06T16:56:52},
  primaryclass     = {cs.LG},
}

@InProceedings{Ye2020Honor,
  author           = {Ye, Deheng and Chen, Guibin and Zhang, Wen and Chen, Sheng and Yuan, Bo and Liu, Bo and Chen, Jia and Liu, Zhao and Qiu, Fuhao and Yu, Hongsheng and others},
  booktitle        = {NeurIPS},
  title            = {Towards Playing Full MOBA Games with Deep Reinforcement Learning},
  year             = {2020},
  pages            = {621--632},
  abstract         = {MOBA games, e.g., Honor of Kings, League of Legends, and Dota 2, pose grand challenges to AI systems such as multi-agent, enormous state-action space, complex action control, etc. Developing AI for playing MOBA games has raised much attention accordingly. However, existing work falls short in handling the raw game complexity caused by the explosion of agent combinations, i.e., lineups, when expanding the hero pool in case that OpenAI's Dota AI limits the play to a pool of only 17 heroes. As a result, full MOBA games without restrictions are far from being mastered by any existing AI system. In this paper, we propose a MOBA AI learning paradigm that methodologically enables playing full MOBA games with deep reinforcement learning. Specifically, we develop a combination of novel and existing learning techniques, including curriculum self-play learning, policy distillation, off-policy adaption, multi-head value estimation, and Monte-Carlo tree-search, in training and playing a large pool of heroes, meanwhile addressing the scalability issue skillfully. Tested on Honor of Kings, a popular MOBA game, we show how to build superhuman AI agents that can defeat top esports players. The superiority of our AI is demonstrated by the first large-scale performance test of MOBA AI agent in the literature.},
  comment          = {王者荣耀 moba 腾讯 tencent},
  creationdate     = {2022-03-23T23:09:03},
  file             = {:Ye_2020_Towards Playing Full MOBA Games with Deep Reinforcement Learning.pdf:PDF},
  groups           = {RTS-MARL},
  modificationdate = {2022-06-08T11:05:44},
}

@Misc{Sukhbaatar2016CommNet,
  author           = {Sainbayar Sukhbaatar and Arthur Szlam and Rob Fergus},
  month            = may,
  title            = {Learning Multiagent Communication with Backpropagation},
  year             = {2016},
  abstract         = {Many tasks in AI require the collaboration of multiple agents. Typically, the communication protocol between agents is manually specified and not altered during training. In this paper we explore a simple neural model, called CommNet, that uses continuous communication for fully cooperative tasks. The model consists of multiple agents and the communication between them is learned alongside their policy. We apply this model to a diverse set of tasks, demonstrating the ability of the agents to learn to communicate amongst themselves, yielding improved performance over non-communicative agents and baselines. In some cases, it is possible to interpret the language devised by the agents, revealing simple but effective strategies for solving the task at hand.},
  archiveprefix    = {arXiv},
  comment          = {CommNet},
  creationdate     = {2022-03-24T10:17:52},
  eprint           = {1605.07736},
  file             = {:Sukhbaatar_2016_Learning Multiagent Communication with Backpropagation (CommNet).pdf:PDF},
  groups           = {MARL},
  keywords         = {cs.LG, cs.AI},
  modificationdate = {2022-06-10T20:18:09},
  primaryclass     = {cs.LG},
}

@Misc{Hospedales2020MetaSurvey,
  author           = {Timothy Hospedales and Antreas Antoniou and Paul Micaelli and Amos Storkey},
  month            = apr,
  title            = {Meta-Learning in Neural Networks: A Survey},
  year             = {2020},
  abstract         = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-03-24T11:07:14},
  eprint           = {2004.05439},
  file             = {:Hospedales_2020_Meta Learning in Neural Networks_ a Survey.pdf:PDF},
  groups           = {Meta Learning},
  keywords         = {cs.LG, stat.ML},
  modificationdate = {2022-03-24T11:08:32},
  primaryclass     = {cs.LG},
}

@Article{Chen2021MetaMAPPO,
  author           = {Long Chen and Bin Hu and Zhi-Hong Guan and Lian Zhao and Xuemin Shen},
  journal          = {{IEEE} Transactions on Neural Networks and Learning Systems},
  title            = {Multiagent Meta-Reinforcement Learning for Adaptive Multipath Routing Optimization},
  year             = {2021},
  month            = apr,
  pages            = {1--13},
  abstract         = {In this article, we investigate the routing problem of packet networks through multiagent reinforcement learning (RL), which is a very challenging topic in distributed and autonomous networked systems. In specific, the routing problem is modeled as a networked multiagent partially observable Markov decision process (MDP). Since the MDP of a network node is not only affected by its neighboring nodes' policies but also the network traffic demand, it becomes a multitask learning problem. Inspired by recent success of RL and metalearning, we propose two novel model-free multiagent RL algorithms, named multiagent proximal policy optimization (MAPPO) and multiagent metaproximal policy optimization (meta-MAPPO), to optimize the network performances under fixed and time-varying traffic demand, respectively. A practicable distributed implementation framework is designed based on the separability of exploration and exploitation in training MAPPO. Compared with the existing routing optimization policies, our simulation results demonstrate the excellent performances of the proposed algorithms.},
  comment          = {MAPPO; meta-MAPPO},
  creationdate     = {2022-03-25T10:20:18},
  doi              = {10.1109/tnnls.2021.3070584},
  file             = {:Chen_2021_Multiagent Meta Reinforcement Learning for Adaptive Multipath Routing Optimization.pdf:PDF},
  groups           = {MARL, Multi_Agent_Meta_Learning},
  modificationdate = {2022-03-28T21:54:57},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
  ranking          = {rank3},
}

@Misc{Yu2021MAPPO,
  author           = {Chao Yu and Akash Velu and Eugene Vinitsky and Yu Wang and Alexandre Bayen and Yi Wu},
  month            = mar,
  title            = {The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games},
  year             = {2021},
  abstract         = {Proximal Policy Optimization (PPO) is a popular on-policy reinforcement learning algorithm but is significantly less utilized than off-policy learning algorithms in multi-agent settings. This is often due the belief that on-policy methods are significantly less sample efficient than their off-policy counterparts in multi-agent problems. In this work, we investigate Multi-Agent PPO (MAPPO), a variant of PPO which is specialized for multi-agent settings. Using a 1-GPU desktop, we show that MAPPO achieves surprisingly strong performance in three popular multi-agent testbeds: the particle-world environments, the Starcraft multi-agent challenge, and the Hanabi challenge, with minimal hyperparameter tuning and without any domain-specific algorithmic modifications or architectures. In the majority of environments, we find that compared to off-policy baselines, MAPPO achieves strong results while exhibiting comparable sample efficiency. Finally, through ablation studies, we present the implementation and algorithmic factors which are most influential to MAPPO's practical performance.},
  archiveprefix    = {arXiv},
  comment          = {MAPPO;https://github.com/marlbenchmark/on-policy; 官方代码对环境的要求可能比较高，更加轻量版，对环境没有依赖的版本，更好方便移植到自己项目的代码为：https://github.com/tinyzqh/light_mappo。},
  creationdate     = {2022-03-25T10:31:00},
  eprint           = {2103.01955},
  file             = {:Yu_2021_The Surprising Effectiveness of PPO in Cooperative, Multi Agent Games.pdf:PDF},
  groups           = {MARL},
  keywords         = {cs.LG, cs.AI, cs.MA},
  modificationdate = {2022-05-04T21:04:10},
  primaryclass     = {cs.LG},
}

@InProceedings{Zhou2020MetaCritic,
  author           = {Zhou, Wei and Li, Yiying and Yang, Yongxin and Wang, Huaimin and Hospedales, Timothy},
  booktitle        = {Advances in Neural Information Processing Systems},
  title            = {Online Meta-Critic Learning for Off-Policy Actor-Critic Methods},
  year             = {2020},
  editor           = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages            = {17662--17673},
  publisher        = {Curran Associates, Inc.},
  volume           = {33},
  abstract         = {Off-Policy Actor-Critic (OffP-AC) methods have proven successful in a variety of continuous control tasks. Normally, the critic's action-value function is updated using temporal-difference, and the critic in turn provides a loss for the actor that trains it to take actions with higher expected return. In this paper, we introduce a flexible and augmented meta-critic that observes the learning process and meta-learns an additional loss for the actor that accelerates and improves actor-critic learning. Compared to existing meta-learning algorithms, meta-critic is rapidly learned online for a single task, rather than slowly over a family of tasks. Crucially, our meta-critic is designed for off-policy based learners, which currently provide state-of-the-art reinforcement learning sample efficiency. We demonstrate that online meta-critic learning benefits to a variety of continuous control tasks when combined with contemporary OffP-AC methods DDPG, TD3 and SAC.},
  creationdate     = {2022-03-26T17:07:43},
  file             = {:Zhou_2020_Online Meta Critic Learning for off Policy Actor Critic Methods.pdf:PDF},
  groups           = {Meta Learning},
  modificationdate = {2022-03-26T20:33:54},
  url              = {https://proceedings.neurips.cc/paper/2020/file/cceff8faa855336ad53b3325914caea2-Paper.pdf},
}

@InProceedings{Antoniou2018TrainMAML,
  author           = {Antreas Antoniou and Harrison Edwards and Amos Storkey},
  booktitle        = {International Conference on Learning Representations},
  title            = {How to train your {MAML}},
  year             = {2019},
  abstract         = {The field of few-shot learning has recently seen substantial advancements. Most of these advancements came from casting few-shot learning as a meta-learning problem.Model Agnostic Meta Learning or MAML is currently one of the best approaches for few-shot learning via meta-learning. MAML is simple, elegant and very powerful, however, it has a variety of issues, such as being very sensitive to neural network architectures, often leading to instability during training, requiring arduous hyperparameter searches to stabilize training and achieve high generalization and being very computationally expensive at both training and inference times. In this paper, we propose various modifications to MAML that not only stabilize the system, but also substantially improve the generalization performance, convergence speed and computational overhead of MAML, which we call MAML++.},
  code             = {https://github.com/AntreasAntoniou/HowToTrainYourMAMLPytorch},
  creationdate     = {2022-03-26T21:17:04},
  file             = {:Antoniou_2019_How to Train Your MAML.pdf:PDF},
  groups           = {Meta Learning},
  keywords         = {meta-learning, deep-learning, few-shot learning, supervised learning, neural-networks, stochastic optimization},
  modificationdate = {2022-03-29T11:26:22},
  ranking          = {rank3},
  url              = {https://openreview.net/forum?id=HJGven05Y7},
}

@Misc{Schulman2017PPO,
  author           = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
  month            = jul,
  title            = {Proximal Policy Optimization Algorithms},
  year             = {2017},
  abstract         = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix    = {arXiv},
  comment          = {提出PPO算法},
  creationdate     = {2022-03-28T13:17:48},
  eprint           = {1707.06347},
  file             = {:Schulman_2017_Proximal Policy Optimization Algorithms (PPO).pdf:PDF},
  keywords         = {cs.LG},
  modificationdate = {2022-06-23T11:27:32},
  primaryclass     = {cs.LG},
}

@Article{Botvinick2019FastSlow,
  author           = {Matthew Botvinick and Sam Ritter and Jane X. Wang and Zeb Kurth-Nelson and Charles Blundell and Demis Hassabis},
  journal          = {Trends in Cognitive Sciences},
  title            = {Reinforcement Learning, Fast and Slow},
  year             = {2019},
  month            = may,
  number           = {5},
  pages            = {408--422},
  volume           = {23},
  abstract         = {Deep reinforcement learning (RL) methods have driven impressive advances in artificial intelligence in recent years, exceeding human performance in domains ranging from Atari to Go to no-limit poker. This progress has drawn the attention of cognitive scientists interested in understanding human learning. However, the concern has been raised that deep RL may be too sample-inefficient - that is, it may simply be too slow - to provide a plausible model of how humans learn. In the present review, we counter this critique by describing recently developed techniques that allow deep RL to operate more nimbly, solving problems much more quickly than previous methods. Although these techniques were developed in an AI context, we propose that they may have rich implications for psychology and neuroscience. A key insight, arising from these AI methods, concerns the fundamental connection between fast RL and slower, more incremental forms of learning.},
  creationdate     = {2022-03-28T15:05:06},
  doi              = {10.1016/j.tics.2019.02.006},
  file             = {:Botvinick_2019_Reinforcement Learning, Fast and Slow.pdf:PDF},
  groups           = {Meta Learning},
  modificationdate = {2022-06-15T16:04:17},
  publisher        = {Elsevier {BV}},
  ranking          = {rank5},
}

@InProceedings{Mordatch2018Emergence,
  author           = {Mordatch, Igor and Abbeel, Pieter},
  booktitle        = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title            = {Emergence of grounded compositional language in multi-agent populations},
  year             = {2018},
  number           = {1},
  pages            = {1495--1502},
  volume           = {32},
  abstract         = {By capturing statistical patterns in large corpora, machine learning has enabled significant advances in natural language processing, including in machine translation, question answering, and sentiment analysis. However, for agents to intelligently interact with humans, simply capturing the statistical patterns is insufficient. In this paper we investigate if, and how, grounded compositional language can emerge as a means to achieve goals in multi-agent populations. Towards this end, we propose a multi-agent learning environment and learning methods that bring about emergence of a basic compositional language. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. We also observe emergence of non-verbal communication such as pointing and guiding when language communication is unavailable.},
  comment          = {李艺颖meta-MADDPG中用的是文中的环境},
  creationdate     = {2022-03-28T21:53:06},
  file             = {:Mordatch_2018_Emergence of Grounded Compositional Language in Multi Agent Populations.pdf:PDF},
  groups           = {MARL},
  modificationdate = {2022-03-28T23:11:57},
}

@Misc{Kia2018TutorialDAC,
  author           = {Solmaz S. Kia and Bryan Van Scoy and Jorge Cortes and Randy A. Freeman and Kevin M. Lynch and Sonia Martinez},
  month            = mar,
  title            = {Tutorial on dynamic average consensus: the problem, its applications, and the algorithms},
  year             = {2018},
  abstract         = {This paper considers the problem of dynamic average consensus algorithm design for a group of communicating agents. This problem consists of designing a distributed algorithm that enables a group of agents with communication and computation capabilities to use local interactions to track the average of locally time-varying reference signals at each agent. The objective of this article is to provide an overview of the dynamic average consensus problem that serves as a comprehensive introduction to the problem definition, its applications, and the distributed methods available to solve them. Our primary intention, rather than providing a full account of all the available literature, is to introduce the reader, in a tutorial fashion, to the main ideas behind dynamic average consensus algorithms, the performance trade-offs considered in their design, and the requirements needed for their analysis and convergence guarantees.},
  archiveprefix    = {arXiv},
  comment          = {动态平均共识算法 参见Sun2021MAMRL},
  creationdate     = {2022-03-29T09:27:15},
  eprint           = {1803.04628},
  file             = {:Kia_2018_Tutorial on Dynamic Average Consensus_ the Problem, Its Applications, and the Algorithms.pdf:PDF},
  keywords         = {cs.SY},
  modificationdate = {2022-03-29T09:28:23},
  primaryclass     = {cs.SY},
}

@Misc{Sheng2020Structured,
  author           = {Junjie Sheng and Xiangfeng Wang and Bo Jin and Junchi Yan and Wenhao Li and Tsung-Hui Chang and Jun Wang and Hongyuan Zha},
  month            = feb,
  title            = {Learning Structured Communication for Multi-agent Reinforcement Learning},
  year             = {2020},
  abstract         = {This work explores the large-scale multi-agent communication mechanism under a multi-agent reinforcement learning (MARL) setting. We summarize the general categories of topology for communication structures in MARL literature, which are often manually specified. Then we propose a novel framework termed as Learning Structured Communication (LSC) by using a more flexible and efficient communication topology. Our framework allows for adaptive agent grouping to form different hierarchical formations over episodes, which is generated by an auxiliary task combined with a hierarchical routing protocol. Given each formed topology, a hierarchical graph neural network is learned to enable effective message information generation and propagation among inter- and intra-group communications. In contrast to existing communication mechanisms, our method has an explicit while learnable design for hierarchical communication. Experiments on challenging tasks show the proposed LSC enjoys high communication efficiency, scalability, and global cooperation capability.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-03-29T20:03:19},
  eprint           = {2002.04235},
  file             = {:Sheng_2020_Learning Structured Communication for Multi Agent Reinforcement Learning.pdf:PDF},
  groups           = {MARL},
  keywords         = {cs.LG, stat.ML},
  modificationdate = {2022-04-23T19:17:50},
  primaryclass     = {cs.LG},
  url              = {https://openreview.net/forum?id=BklWt24tvH},
}

@InProceedings{Macke2021Expected,
  author           = {Macke, William and Mirsky, Reuth and Stone, Peter},
  booktitle        = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title            = {Expected Value of Communication for Planning in Ad Hoc Teamwork},
  year             = {2021},
  number           = {13},
  pages            = {11290--11298},
  volume           = {35},
  abstract         = {A desirable goal for autonomous agents is to be able to coordinate on the fly with previously unknown teammates. Known as “ad hoc teamwork”, enabling such a capability has been receiving increasing attention in the research community. One of the central challenges in ad hoc teamwork is quickly recognizing the current plans of other agents and planning accordingly. In this paper, we focus on the scenario in which teammates can communicate with one another, but only at a cost. Thus, they must carefully balance plan recognition based on observations vs. that based on communication. This paper proposes a new metric for evaluating how similar are two policies that a teammate may be following - the Expected Divergence Point (EDP). We then present a novel planning algorithm for ad hoc teamwork, determining which query to ask and planning accordingly. We demonstrate the effectiveness of this algorithm in a range of increasingly general communication in ad hoc teamwork problems.},
  creationdate     = {2022-03-29T20:08:14},
  file             = {:Macke_2021_Expected Value of Communication for Planning in Ad Hoc Teamwork.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  modificationdate = {2022-03-29T20:26:00},
}

@InProceedings{Mirsky2020Penny,
  author           = {Reuth Mirsky and William Macke and Andy Wang and Harel Yedidsion and Peter Stone},
  booktitle        = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
  title            = {A Penny for Your Thoughts: The Value of Communication in Ad Hoc Teamwork},
  year             = {2020},
  month            = jul,
  pages            = {254--260},
  publisher        = {International Joint Conferences on Artificial Intelligence Organization},
  abstract         = {In ad hoc teamwork, multiple agents need to collaborate without having knowledge about their teammates or their plans a priori. A common assumption in this research area is that the agents cannot communicate. However, just as two random people may speak the same language, autonomous teammates may also happen to share a communication protocol. This paper considers how such a shared protocol can be leveraged, introducing a means to reason about Communication in Ad Hoc Teamwork (CAT). The goal of this work is enabling improved ad hoc teamwork by judiciously leveraging the ability of the team to communicate. We situate our study within a novel CAT scenario, involving tasks with multiple steps, where teammates' plans are unveiled over time. In this context, the paper proposes methods to reason about the timing and value of communication and introduces an algorithm for an ad hoc agent to leverage these methods. Finally, we introduces a new multiagent domain, the tool fetching domain, and we study how varying this domain's properties affects the usefulness of communication. Empirical results show the benefits of explicit reasoning about communication content and timing in ad hoc teamwork.},
  creationdate     = {2022-03-29T20:10:17},
  doi              = {10.24963/ijcai.2020/36},
  file             = {:Mirsky_2020_A Penny for Your Thoughts_ the Value of Communication in Ad Hoc Teamwork.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  modificationdate = {2022-06-15T16:11:04},
}

@Misc{Wang2020Cooks,
  author           = {Rose E. Wang and Sarah A. Wu and James A. Evans and Joshua B. Tenenbaum and David C. Parkes and Max Kleiman-Weiner},
  month            = mar,
  title            = {Too many cooks: Bayesian inference for coordinating multi-agent collaboration},
  year             = {2020},
  abstract         = {Collaboration requires agents to coordinate their behavior on the fly, sometimes cooperating to solve a single task together and other times dividing it up into sub-tasks to work on in parallel. Underlying the human ability to collaborate is theory-of-mind, the ability to infer the hidden mental states that drive others to act. Here, we develop Bayesian Delegation, a decentralized multi-agent learning mechanism with these abilities. Bayesian Delegation enables agents to rapidly infer the hidden intentions of others by inverse planning. We test Bayesian Delegation in a suite of multi-agent Markov decision processes inspired by cooking problems. On these tasks, agents with Bayesian Delegation coordinate both their high-level plans (e.g. what sub-task they should work on) and their low-level actions (e.g. avoiding getting in each other's way). In a self-play evaluation, Bayesian Delegation outperforms alternative algorithms. Bayesian Delegation is also a capable ad-hoc collaborator and successfully coordinates with other agent types even in the absence of prior experience. Finally, in a behavioral experiment, we show that Bayesian Delegation makes inferences similar to human observers about the intent of others. Together, these results demonstrate the power of Bayesian Delegation for decentralized multi-agent collaboration.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-03-29T20:18:12},
  eprint           = {2003.11778},
  file             = {:Wang_2020_Too Many Cooks_ Bayesian Inference for Coordinating Multi Agent Collaboration.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  keywords         = {cs.AI, cs.LG, cs.MA},
  modificationdate = {2022-04-23T19:18:13},
  primaryclass     = {cs.AI},
  ranking          = {rank5},
}

@InProceedings{Rahman2021OpenAdHoc,
  author           = {Rahman, Muhammad A and Hopner, Niklas and Christianos, Filippos and Albrecht, Stefano V},
  booktitle        = {Proceedings of the 38th International Conference on Machine Learning},
  title            = {Towards Open Ad Hoc Teamwork Using Graph-based Policy Learning},
  year             = {2021},
  month            = jul,
  organization     = {PMLR},
  pages            = {8776--8786},
  publisher        = {PMLR},
  series           = {Proceedings of Machine Learning Research},
  volume           = {139},
  abstract         = {Ad hoc teamwork is the challenging problem of designing an autonomous agent which can adapt quickly to collaborate with teammates without prior coordination mechanisms, including joint training. Prior work in this area has focused on closed teams in which the number of agents is fixed. In this work, we consider open teams by allowing agents with different fixed policies to enter and leave the environment without prior notification. Our solution builds on graph neural networks to learn agent models and joint-action value models under varying team compositions. We contribute a novel action-value computation that integrates the agent model and joint-action value model to produce action-value estimates. We empirically demonstrate that our approach successfully models the effects other agents have on the learner, leading to policies that robustly adapt to dynamic team compositions and significantly outperform several alternative methods.},
  creationdate     = {2022-03-29T20:21:44},
  file             = {:Rahman_2021_Towards Open Ad Hoc Teamwork Using Graph Based Policy Learning.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  modificationdate = {2022-03-29T20:24:35},
}

@Misc{Jaafra2018ReviewNAS,
  author           = {Yesmina Jaafra and Jean Luc Laurent and Aline Deruyver and Mohamed Saber Naceur},
  month            = dec,
  title            = {A Review of Meta-Reinforcement Learning for Deep Neural Networks Architecture Search},
  year             = {2018},
  abstract         = {Deep Neural networks are efficient and flexible models that perform well for a variety of tasks such as image, speech recognition and natural language understanding. In particular, convolutional neural networks (CNN) generate a keen interest among researchers in computer vision and more specifically in classification tasks. CNN architecture and related hyperparameters are generally correlated to the nature of the processed task as the network extracts complex and relevant characteristics allowing the optimal convergence. Designing such architectures requires significant human expertise, substantial computation time and doesn't always lead to the optimal network. Model configuration topic has been extensively studied in machine learning without leading to a standard automatic method. This survey focuses on reviewing and discussing the current progress in automating CNN architecture search.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-03-30T10:08:44},
  eprint           = {1812.07995},
  file             = {:Jaafra_2018_A Review of Meta Reinforcement Learning for Deep Neural Networks Architecture Search.pdf:PDF},
  groups           = {Meta Learning},
  keywords         = {cs.LG, cs.AI},
  modificationdate = {2022-03-31T19:56:07},
  primaryclass     = {cs.LG},
}

@InProceedings{Finn2019OnlineMeta,
  author           = {Finn, Chelsea and Rajeswaran, Aravind and Kakade, Sham and Levine, Sergey},
  booktitle        = {Proceedings of the 36th International Conference on Machine Learning},
  title            = {Online Meta-Learning},
  year             = {2019},
  editor           = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  month            = jun,
  pages            = {1920--1930},
  publisher        = {PMLR},
  series           = {Proceedings of Machine Learning Research},
  volume           = {97},
  abstract         = {A central capability of intelligent systems is the ability to continuously build upon previous experiences to speed up and enhance learning of new tasks. Two distinct research paradigms have studied this question. Meta-learning views this problem as learning a prior over model parameters that is amenable for fast adaptation on a new task, but typically assumes the tasks are available together as a batch. In contrast, online (regret based) learning considers a setting where tasks are revealed one after the other, but conventionally trains a single model without task-specific adaptation. This work introduces an online meta-learning setting, which merges ideas from both paradigms to better capture the spirit and practice of continual lifelong learning. We propose the follow the meta leader (FTML) algorithm which extends the MAML algorithm to this setting. Theoretically, this work provides an O(log T) regret guarantee with one additional higher order smoothness assumption (in comparison to the standard online setting). Our experimental evaluation on three different large-scale problems suggest that the proposed algorithm significantly outperforms alternatives based on traditional online learning approaches.},
  creationdate     = {2022-03-30T10:35:22},
  file             = {:Finn_2019_Online Meta Learning.pdf:PDF},
  groups           = {Meta Learning},
  modificationdate = {2022-06-15T16:09:44},
  pdf              = {http://proceedings.mlr.press/v97/finn19a/finn19a.pdf},
  url              = {https://proceedings.mlr.press/v97/finn19a.html},
}

@InProceedings{Du2019LIIR,
  author           = {Du, Yali and Han, Lei and Fang, Meng and Liu, Ji and Dai, Tianhong and Tao, Dacheng},
  booktitle        = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
  title            = {LIIR: Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning},
  year             = {2019},
  pages            = {4403--4414},
  volume           = {32},
  abstract         = {A great challenge in cooperative decentralized multi-agent reinforcement learning (MARL) is generating diversified behaviors for each individual agent when receiving only a team reward. Prior studies have paid much effort on reward shaping or designing a centralized critic that can discriminatively credit the agents. In this paper, we propose to merge the two directions and learn each agent an intrinsic reward function which diversely stimulates the agents at each time step. Specifically, the intrinsic reward for a specific agent will be involved in computing a distinct proxy critic for the agent to direct the updating of its individual policy. Meanwhile, the parameterized intrinsic reward function will be updated towards maximizing the expected accumulated team reward from the environment so that the objective is consistent with the original MARL problem. The proposed method is referred to as learning individual intrinsic reward (LIIR) in MARL. We compare LIIR with a number of state-of-the-art MARL methods on battle games in StarCraft II. The results demonstrate the effectiveness of LIIR, and we show LIIR can assign each individual agent an insightful intrinsic reward per time step.},
  comment          = {内在奖励函数用元参数表示，策略函数用基础参数表示，用类似MAML双层优化的思想，求解一个较优的内在奖励函数，使其能够生成较好的策略。

StarCraft},
  creationdate     = {2022-03-30T22:20:33},
  file             = {:Du_2019_LIIR_ Learning Individual Intrinsic Reward in Multi Agent Reinforcement Learning.pdf:PDF},
  groups           = {MARL, RTS-MARL},
  modificationdate = {2022-06-06T16:56:16},
}

@Misc{Weng2019MetaRL,
  author           = {Lilian Weng},
  howpublished     = {\url{https://lilianweng.github.io/posts/2019-06-23-meta-rl/}},
  month            = jun,
  title            = {Meta Reinforcement Learning},
  year             = {2019},
  creationdate     = {2022-03-31T08:45:03},
  groups           = {Meta Learning},
  modificationdate = {2022-03-31T09:47:46},
  url              = {https://lilianweng.github.io/posts/2019-06-23-meta-rl/},
}

@Misc{Plappert2017ParameterNoise,
  author           = {Matthias Plappert and Rein Houthooft and Prafulla Dhariwal and Szymon Sidor and Richard Y. Chen and Xi Chen and Tamim Asfour and Pieter Abbeel and Marcin Andrychowicz},
  month            = jun,
  title            = {Parameter Space Noise for Exploration},
  year             = {2017},
  abstract         = {Deep reinforcement learning (RL) methods generally engage in exploratory behavior through noise injection in the action space. An alternative is to add noise directly to the agent's parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require significantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both worlds. We demonstrate that both off- and on-policy methods benefit from this approach through experimental comparison of DQN, DDPG, and TRPO on high-dimensional discrete action environments as well as continuous control tasks. Our results show that RL with parameter noise learns more efficiently than traditional RL with action space noise and evolutionary strategies individually.},
  archiveprefix    = {arXiv},
  comment          = {策略参数噪声扰动，探索 Exploration},
  creationdate     = {2022-03-31T15:15:37},
  eprint           = {1706.01905},
  file             = {:Plappert_2017_Parameter Space Noise for Exploration.pdf:PDF},
  keywords         = {cs.LG, cs.AI, cs.NE, cs.RO, stat.ML},
  modificationdate = {2022-03-31T15:19:35},
  primaryclass     = {cs.LG},
}

@InProceedings{Ruckstiess2008Exploration,
  author           = {R{\"u}ckstie{\ss}, Thomas and Felder, Martin and Schmidhuber, J{\"u}rgen},
  booktitle        = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  title            = {State-dependent exploration for policy gradient methods},
  year             = {2008},
  organization     = {Springer},
  pages            = {234--249},
  publisher        = {Springer},
  volume           = {5212},
  abstract         = {Policy Gradient methods are model-free reinforcement learning algorithms which in recent years have been successfully applied to many real-world problems. Typically, Likelihood Ratio (LR) methods are used to estimate the gradient, but they suffer from high variance due to random exploration at every time step of each training episode. Our solution to this problem is to introduce a state-dependent exploration function (SDE) which during an episode returns the same action for any given state. This results in less variance per episode and faster convergence. SDE also finds solutions overlooked by other methods, and even improves upon state-of-the-art gradient estimators such as Natural Actor-Critic. We systematically derive SDE and apply it to several illustrative toy problems and a challenging robotics simulation task, where SDE greatly outperforms random exploration.},
  creationdate     = {2022-03-31T15:20:48},
  doi              = {10.1007/978-3-540-87481-2_16},
  file             = {:Rückstieß_2008_State Dependent Exploration for Policy Gradient Methods.pdf:PDF},
  modificationdate = {2022-03-31T15:25:44},
}

@InProceedings{Grover2018Learning,
  author           = {Grover, Aditya and Al-Shedivat, Maruan and Gupta, Jayesh and Burda, Yuri and Edwards, Harrison},
  booktitle        = {Proceedings of the 35th International Conference on Machine Learning},
  title            = {Learning Policy Representations in Multiagent Systems},
  year             = {2018},
  editor           = {Dy, Jennifer and Krause, Andreas},
  month            = jul,
  pages            = {1802--1811},
  publisher        = {PMLR},
  series           = {Proceedings of Machine Learning Research},
  volume           = {80},
  abstract         = {Modeling agent behavior is central to understanding the emergence of complex phenomena in multiagent systems. Prior work in agent modeling has largely been task-specific and driven by hand-engineering domain-specific prior knowledge. We propose a general learning framework for modeling agent behavior in any multiagent system using only a handful of interaction data. Our framework casts agent modeling as a representation learning problem. Consequently, we construct a novel objective inspired by imitation learning and agent identification and design an algorithm for unsupervised learning of representations of agent policies. We demonstrate empirically the utility of the proposed framework in (i) a challenging high-dimensional competitive environment for continuous control and (ii) a cooperative environment for communication, on supervised predictive tasks, unsupervised clustering, and policy optimization using deep reinforcement learning.},
  creationdate     = {2022-03-31T19:38:22},
  file             = {:Grover_2018_Learning Policy Representations in Multiagent Systems.pdf:PDF},
  modificationdate = {2022-06-15T16:09:54},
  pdf              = {http://proceedings.mlr.press/v80/grover18a/grover18a.pdf},
  url              = {https://proceedings.mlr.press/v80/grover18a.html},
}

@Misc{Flennerhag2021Bootstrapped,
  author           = {Sebastian Flennerhag and Yannick Schroecker and Tom Zahavy and Hado van Hasselt and David Silver and Satinder Singh},
  month            = sep,
  title            = {Bootstrapped Meta-Learning},
  year             = {2021},
  abstract         = {Meta-learning empowers artificial intelligence to increase its efficiency by learning how to learn. Unlocking this potential involves overcoming a challenging meta-optimisation problem. We propose an algorithm that tackles this problem by letting the meta-learner teach itself. The algorithm first bootstraps a target from the meta-learner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. Focusing on meta-learning with gradients, we establish conditions that guarantee performance improvements and show that the metric can control meta-optimisation. Meanwhile, the bootstrapping mechanism can extend the effective meta-learning horizon without requiring backpropagation through all updates. We achieve a new state-of-the art for model-free agents on the Atari ALE benchmark and demonstrate that it yields both performance and efficiency gains in multi-task meta-learning. Finally, we explore how bootstrapping opens up new possibilities and find that it can meta-learn efficient exploration in an epsilon-greedy Q-learning agent, without backpropagating through the update rule.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-03-31T19:55:04},
  eprint           = {2109.04504},
  file             = {:Flennerhag_2021_Bootstrapped Meta Learning.pdf:PDF},
  groups           = {Meta Learning},
  keywords         = {cs.LG, cs.AI, stat.ML},
  modificationdate = {2022-03-31T19:55:46},
  primaryclass     = {cs.LG},
}

@InProceedings{Veeriah2019Discovery,
  author           = {Veeriah, Vivek and Hessel, Matteo and Xu, Zhongwen and Rajendran, Janarthanan and Lewis, Richard L and Oh, Junhyuk and van Hasselt, Hado P and Silver, David and Singh, Satinder},
  booktitle        = {Advances in Neural Information Processing Systems},
  title            = {Discovery of Useful Questions as Auxiliary Tasks},
  year             = {2019},
  editor           = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages            = {9310--9321},
  publisher        = {Curran Associates, Inc.},
  volume           = {32},
  abstract         = {Arguably, intelligent agents ought to be able to discover their own questions so that in learning answers for them they learn unanticipated useful knowledge and skills; this departs from the focus in much of machine learning on agents learning answers to externally defined questions. We present a novel method for a reinforcement learning (RL) agent to discover questions formulated as general value functions or GVFs, a fairly rich form of knowledge representation. Specifically, our method uses non-myopic meta-gradients to learn GVF-questions such that learning answers to them, as an auxiliary task, induces useful representations for the main task faced by the RL agent. We demonstrate that auxiliary tasks based on the discovered GVFs are sufficient, on their own, to build representations that support main task learning, and that they do so better than popular hand-designed auxiliary tasks from the literature. Furthermore, we show, in the context of Atari2600 videogames, how such auxiliary tasks, meta-learned alongside the main task, can improve the data efficiency of an actor-critic agent.},
  creationdate     = {2022-03-31T20:21:54},
  file             = {:Veeriah_2019_Discovery of Useful Questions As Auxiliary Tasks.pdf:PDF},
  modificationdate = {2022-03-31T20:31:34},
  url              = {https://proceedings.neurips.cc/paper/2019/file/10ff0b5e85e5b85cc3095d431d8c08b4-Paper.pdf},
}

@InProceedings{Rakelly2019PEARL,
  author           = {Rakelly, Kate and Zhou, Aurick and Finn, Chelsea and Levine, Sergey and Quillen, Deirdre},
  booktitle        = {Proceedings of the 36th International Conference on Machine Learning},
  title            = {Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables},
  year             = {2019},
  editor           = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  month            = jun,
  pages            = {5331--5340},
  publisher        = {PMLR},
  series           = {Proceedings of Machine Learning Research},
  volume           = {97},
  abstract         = {Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While meta-reinforcement learning (meta-RL) algorithms can enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Current methods rely heavily on on-policy experience, limiting their sample efficiency. They also lack mechanisms to reason about task uncertainty when adapting to new tasks, limiting their effectiveness on sparse reward problems. In this paper, we address these challenges by developing an off-policy meta-RL algorithm that disentangles task inference and control. In our approach, we perform online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience. This probabilistic interpretation enables posterior sampling for structured and efficient exploration. We demonstrate how to integrate these task variables with off-policy RL algorithms to achieve both meta-training and adaptation efficiency. Our method outperforms prior algorithms in sample efficiency by 20-100X as well as in asymptotic performance on several meta-RL benchmarks.},
  code             = {https://github.com/katerakelly/oyster},
  creationdate     = {2022-04-01T09:15:14},
  file             = {:Rakelly_2019_Efficient off Policy Meta Reinforcement Learning Via Probabilistic Context Variables (PEARL).pdf:PDF},
  groups           = {Meta Learning},
  modificationdate = {2022-04-02T11:09:29},
  url              = {https://proceedings.mlr.press/v97/rakelly19a.html},
}

@InProceedings{Gupta2018MAESN,
  author           = {Gupta, Abhishek and Mendonca, Russell and Liu, YuXuan and Abbeel, Pieter and Levine, Sergey},
  booktitle        = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  title            = {Meta-Reinforcement Learning of Structured Exploration Strategies},
  year             = {2018},
  address          = {Red Hook, NY, USA},
  pages            = {5307--5316},
  publisher        = {Curran Associates Inc.},
  volume           = {31},
  abstract         = {Exploration is a fundamental challenge in reinforcement learning (RL). Many current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we study how prior tasks can inform an agent about how to explore effectively in new situations. We introduce a novel gradient-based fast adaptation algorithm – model agnostic exploration with structured noise (MAESN) – to learn exploration strategies from prior experience. The prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy, producing exploration strategies that are informed by prior knowledge and are more effective than random action-space noise. We show that MAESN is more effective at learning exploration strategies when compared to prior meta-RL methods, RL without learned exploration strategies, and task-agnostic exploration methods. We evaluate our method on a variety of simulated tasks: locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.},
  creationdate     = {2022-04-02T11:31:19},
  file             = {:Gupta_2018_Meta Reinforcement Learning of Structured Exploration Strategies (MAESN).pdf:PDF},
  groups           = {Meta Learning},
  location         = {Montr\'{e}al, Canada},
  modificationdate = {2022-04-09T16:05:53},
  numpages         = {10},
}

@Misc{Wang2019Bottleneck,
  author           = {Rundong Wang and Xu He and Runsheng Yu and Wei Qiu and Bo An and Zinovi Rabinovich},
  month            = nov,
  title            = {Learning Efficient Multi-agent Communication: An Information Bottleneck Approach},
  year             = {2019},
  abstract         = {Many real-world multi-agent reinforcement learning applications require agents to communicate, assisted by a communication protocol. These applications face a common and critical issue of communication's limited bandwidth that constrains agents' ability to cooperate successfully. In this paper, rather than proposing a fixed communication protocol, we develop an Informative Multi-Agent Communication (IMAC) method to learn efficient communication protocols. Our contributions are threefold. First, we notice a fact that a limited bandwidth translates into a constraint on the communicated message entropy, thus paving the way of controlling the bandwidth. Second, we introduce a customized batch-norm layer, which controls the messages' entropy to simulate the limited bandwidth constraint. Third, we apply the information bottleneck method to discover the optimal communication protocol, which can satisfy a bandwidth constraint via training with the prior distribution in the method. To demonstrate the efficacy of our method, we conduct extensive experiments in various cooperative and competitive multi-agent tasks across two dimensions: the number of agents and different bandwidths. We show that IMAC converges fast, and leads to efficient communication among agents under the limited-bandwidth constraint as compared to many baseline methods.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-04-04T22:01:40},
  eprint           = {1911.06992},
  file             = {:Wang_2019_Learning Efficient Multi Agent Communication_ an Information Bottleneck Approach.pdf:PDF},
  groups           = {MARL},
  keywords         = {cs.AI, cs.MA},
  modificationdate = {2022-04-04T22:02:32},
  primaryclass     = {cs.AI},
}

@Article{Li2021Individualized,
  author           = {Li, Huao and Ni, Tianwei and Agrawal, Siddharth and Jia, Fan and Raja, Suhas and Gui, Yikang and Hughes, Dana and Lewis, Michael and Sycara, Katia},
  journal          = {IEEE Transactions on Human-Machine Systems},
  title            = {Individualized Mutual Adaptation in Human-Agent Teams},
  year             = {2021},
  number           = {6},
  pages            = {706--714},
  volume           = {51},
  abstract         = {The ability to collaborate with previously unseen human teammates is crucial for artificial agents to be effective in human-agent teams (HATs). Due to individual differences and complex team dynamics, it is hard to develop a single agent policy to match all potential teammates. In this article, we study both human-human and HAT in a dyadic cooperative task, Team Space Fortress. Results show that the team performance is influenced by both players’ individual skill level and their ability to collaborate with different teammates by adopting complementary policies. Based on human-human team results, we propose an adaptive agent that identifies different human policies and assigns a complementary partner policy to optimize team performance. The adaptation method relies on a novel similarity metric to infer human policy and then selects the most complementary policy from a pretrained library of exemplar policies. We conducted human-agent experiments to evaluate the adaptive agent and examine mutual adaptation in HAT. Results show that both human adaptation and agent adaptation contribute to team performance.},
  creationdate     = {2022-04-16T23:10:16},
  doi              = {10.1109/thms.2021.3107675},
  file             = {:Li_2021_Individualized Mutual Adaptation in Human Agent Teams.pdf:PDF},
  groups           = {Human-Agent Ad Hoc},
  modificationdate = {2022-07-08T11:29:15},
}

@Misc{Zhang2017Replay,
  author           = {Shangtong Zhang and Richard S. Sutton},
  month            = dec,
  title            = {A Deeper Look at Experience Replay},
  year             = {2017},
  abstract         = {Recently experience replay is widely used in various deep reinforcement learning (RL) algorithms, in this paper we rethink the utility of experience replay. It introduces a new hyper-parameter, the memory buffer size, which needs carefully tuning. However unfortunately the importance of this new hyper-parameter has been underestimated in the community for a long time. In this paper we did a systematic empirical study of experience replay under various function representations. We showcase that a large replay buffer can significantly hurt the performance. Moreover, we propose a simple O(1) method to remedy the negative influence of a large replay buffer. We showcase its utility in both simple grid world and challenging domains like Atari games.},
  archiveprefix    = {arXiv},
  comment          = {Replay Buffer 经验回放池大小的影响},
  creationdate     = {2022-04-19T11:10:17},
  eprint           = {1712.01275},
  file             = {:Zhang_2017_A Deeper Look at Experience Replay.pdf:PDF},
  keywords         = {cs.LG, cs.AI},
  modificationdate = {2022-04-19T11:11:42},
  primaryclass     = {cs.LG},
}

@Misc{Fickinger2021Scalable,
  author           = {Arnaud Fickinger and Hengyuan Hu and Brandon Amos and Stuart Russell and Noam Brown},
  month            = sep,
  title            = {Scalable Online Planning via Reinforcement Learning Fine-Tuning},
  year             = {2021},
  abstract         = {Lookahead search has been a critical component of recent AI successes, such as in the games of chess, go, and poker. However, the search methods used in these games, and in many other settings, are tabular. Tabular search methods do not scale well with the size of the search space, and this problem is exacerbated by stochasticity and partial observability. In this work we replace tabular search with online model-based fine-tuning of a policy neural network via reinforcement learning, and show that this approach outperforms state-of-the-art search algorithms in benchmark settings. In particular, we use our search algorithm to achieve a new state-of-the-art result in self-play Hanabi, and show the generality of our algorithm by also showing that it outperforms tabular search in the Atari game Ms. Pacman.},
  archiveprefix    = {arXiv},
  comment          = {Hanabi},
  creationdate     = {2022-04-23T19:14:58},
  eprint           = {2109.15316},
  file             = {:Fickinger_2021_Scalable Online Planning Via Reinforcement Learning Fine Tuning.pdf:PDF},
  keywords         = {cs.AI},
  modificationdate = {2022-04-23T19:16:30},
  primaryclass     = {cs.AI},
  ranking          = {rank1},
}

@Misc{Sunehag2017VDN,
  author           = {Peter Sunehag and Guy Lever and Audrunas Gruslys and Wojciech Marian Czarnecki and Vinicius Zambaldi and Max Jaderberg and Marc Lanctot and Nicolas Sonnerat and Joel Z. Leibo and Karl Tuyls and Thore Graepel},
  month            = jun,
  title            = {Value-Decomposition Networks For Cooperative Multi-Agent Learning},
  year             = {2017},
  abstract         = {We study the problem of cooperative multi-agent reinforcement learning with a single joint reward signal. This class of learning problems is difficult because of the often large combined action and observation spaces. In the fully centralized and decentralized approaches, we find the problem of spurious rewards and a phenomenon we call the "lazy agent" problem, which arises due to partial observability. We address these problems by training individual agents with a novel value decomposition network architecture, which learns to decompose the team value function into agent-wise value functions. We perform an experimental evaluation across a range of partially-observable multi-agent domains and show that learning such value-decompositions leads to superior results, in particular when combined with weight sharing, role information and information channels.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-04-24T11:19:54},
  eprint           = {1706.05296},
  file             = {:Sunehag_2017_Value Decomposition Networks for Cooperative Multi Agent Learning (VDN).pdf:PDF},
  groups           = {MARL},
  keywords         = {cs.AI, I.2.11},
  modificationdate = {2022-04-25T14:59:42},
  primaryclass     = {cs.AI},
}

@Misc{Weber2017Imagination,
  author           = {Théophane Weber and Sébastien Racanière and David P. Reichert and Lars Buesing and Arthur Guez and Danilo Jimenez Rezende and Adria Puigdomènech Badia and Oriol Vinyals and Nicolas Heess and Yujia Li and Razvan Pascanu and Peter Battaglia and Demis Hassabis and David Silver and Daan Wierstra},
  month            = jul,
  title            = {Imagination-Augmented Agents for Deep Reinforcement Learning},
  year             = {2017},
  abstract         = {We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-04-25T14:58:54},
  eprint           = {1707.06203},
  file             = {:Weber_2017_Imagination Augmented Agents for Deep Reinforcement Learning.pdf:PDF},
  keywords         = {cs.LG, cs.AI, stat.ML},
  modificationdate = {2022-04-25T15:12:46},
  primaryclass     = {cs.LG},
}

@Misc{Mendonca2020Meta,
  author           = {Russell Mendonca and Xinyang Geng and Chelsea Finn and Sergey Levine},
  month            = jun,
  title            = {Meta-Reinforcement Learning Robust to Distributional Shift via Model Identification and Experience Relabeling},
  year             = {2020},
  abstract         = {Reinforcement learning algorithms can acquire policies for complex tasks autonomously. However, the number of samples required to learn a diverse set of skills can be prohibitively large. While meta-reinforcement learning methods have enabled agents to leverage prior experience to adapt quickly to new tasks, their performance depends crucially on how close the new task is to the previously experienced tasks. Current approaches are either not able to extrapolate well, or can do so at the expense of requiring extremely large amounts of data for on-policy meta-training. In this work, we present model identification and experience relabeling (MIER), a meta-reinforcement learning algorithm that is both efficient and extrapolates well when faced with out-of-distribution tasks at test time. Our method is based on a simple insight: we recognize that dynamics models can be adapted efficiently and consistently with off-policy data, more easily than policies and value functions. These dynamics models can then be used to continue training policies and value functions for out-of-distribution tasks without using meta-reinforcement learning at all, by generating synthetic experience for the new task.},
  archiveprefix    = {arXiv},
  comment          = {本文附录提到了MAML为什么不适合Value-based
https://www.zhihu.com/question/482495955},
  creationdate     = {2022-04-28T10:32:30},
  eprint           = {2006.07178},
  file             = {:Mendonca_2020_Meta Reinforcement Learning Robust to Distributional Shift Via Model Identification and Experience Relabeling.pdf:PDF},
  groups           = {Meta Learning},
  keywords         = {cs.LG, stat.ML},
  modificationdate = {2022-04-29T11:10:31},
  primaryclass     = {cs.LG},
}

@Misc{Lee2020Context,
  author           = {Lee, Kimin and Seo, Younggyo and Lee, Seunghyun and Lee, Honglak and Shin, Jinwoo},
  month            = jun,
  title            = {Context-aware {Dynamics} {Model} for {Generalization} in {Model}-{Based} {Reinforcement} {Learning}},
  year             = {2020},
  abstract         = {Model-based reinforcement learning (RL) enjoys several benefits, such as data-efficiency and planning, by learning a model of the environment's dynamics. However, learning a global model that can generalize across different dynamics is a challenging task. To tackle this problem, we decompose the task of learning a global dynamics model into two stages: (a) learning a context latent vector that captures the local dynamics, then (b) predicting the next state conditioned on it. In order to encode dynamics-specific information into the context latent vector, we introduce a novel loss function that encourages the context latent vector to be useful for predicting both forward and backward dynamics. The proposed method achieves superior generalization ability across various simulated robotics and control tasks, compared to existing RL schemes.},
  archiveprefix    = {arXiv},
  comment          = {Comment: Accepted in ICML2020. First two authors contributed equally, website: https://sites.google.com/view/cadm code: https://github.com/younggyoseo/CaDM},
  creationdate     = {2022-04-29T11:08:18},
  eprint           = {2005.06800},
  file             = {:Lee_2020_Context Aware Dynamics Model for Generalization in Model Based Reinforcement Learning.pdf:PDF},
  groups           = {Meta Learning},
  keywords         = {Computer Science - Machine Learning, Statistics - Machine Learning},
  modificationdate = {2022-05-03T09:08:29},
  url              = {http://arxiv.org/abs/2005.06800},
  urldate          = {2022-04-29},
}

@Misc{Foerster2017COMA,
  author           = {Jakob Foerster and Gregory Farquhar and Triantafyllos Afouras and Nantas Nardelli and Shimon Whiteson},
  month            = may,
  title            = {Counterfactual Multi-Agent Policy Gradients},
  year             = {2017},
  abstract         = {Cooperative multi-agent systems can be naturally used to model many real world problems, such as network packet routing and the coordination of autonomous vehicles. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.},
  archiveprefix    = {arXiv},
  comment          = {StarCraft Micromanagement},
  creationdate     = {2022-05-01T10:36:51},
  eprint           = {1705.08926},
  file             = {:Foerster_2017_Counterfactual Multi Agent Policy Gradients (COMA).pdf:PDF},
  groups           = {MARL, RTS-MARL},
  keywords         = {cs.AI, cs.MA},
  modificationdate = {2022-06-06T16:57:54},
  primaryclass     = {cs.AI},
}

@InCollection{Gupta2017Cooperative,
  author           = {Jayesh K. Gupta and Maxim Egorov and Mykel Kochenderfer},
  booktitle        = {Autonomous Agents and Multiagent Systems},
  publisher        = {Springer International Publishing},
  title            = {Cooperative Multi-agent Control Using Deep Reinforcement Learning},
  year             = {2017},
  pages            = {66--83},
  abstract         = {This work considers the problem of learning cooperative policies in complex, partially observable domains without explicit communication. We extend three classes of single-agent deep reinforcement learning algorithms based on policy gradient, temporal-difference error, and actor-critic methods to cooperative multi-agent systems. To effectively scale these algorithms beyond a trivial number of agents, we combine them with a multi-agent variant of curriculum learning. The algorithms are benchmarked on a suite of cooperative control tasks, including tasks with discrete and continuous actions, as well as tasks with dozens of cooperating agents. We report the performance of the algorithms using different neural architectures, training procedures, and reward structures. We show that policy gradient methods tend to outperform both temporal-difference and actor-critic methods and that curriculum learning is vital to scaling reinforcement learning algorithms in complex multi-agent domains.},
  comment          = {同构智能体，共享一套网络参数},
  creationdate     = {2022-05-01T13:38:00},
  doi              = {10.1007/978-3-319-71682-4_5},
  file             = {:Gupta_2017_Cooperative Multi Agent Control Using Deep Reinforcement Learning.pdf:PDF},
  groups           = {MARL},
  modificationdate = {2022-05-01T13:39:18},
}

@Misc{Foerster2016DIAL,
  author           = {Jakob N. Foerster and Yannis M. Assael and Nando de Freitas and Shimon Whiteson},
  month            = may,
  title            = {Learning to Communicate with Deep Multi-Agent Reinforcement Learning},
  year             = {2016},
  abstract         = {We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.},
  archiveprefix    = {arXiv},
  comment          = {DIAL RIAL NuerIPS},
  creationdate     = {2022-05-03T16:34:49},
  eprint           = {1605.06676},
  file             = {:Foerster_2016_Learning to Communicate with Deep Multi Agent Reinforcement Learning (DIAL).pdf:PDF},
  groups           = {MARL},
  keywords         = {cs.AI, cs.LG, cs.MA},
  modificationdate = {2022-05-03T16:37:21},
  primaryclass     = {cs.AI},
}

@Misc{Papoudakis2019Dealing,
  author           = {Georgios Papoudakis and Filippos Christianos and Arrasy Rahman and Stefano V. Albrecht},
  month            = jun,
  title            = {Dealing with Non-Stationarity in Multi-Agent Deep Reinforcement Learning},
  year             = {2019},
  abstract         = {Recent developments in deep reinforcement learning are concerned with creating decision-making agents which can perform well in various complex domains. A particular approach which has received increasing attention is multi-agent reinforcement learning, in which multiple agents learn concurrently to coordinate their actions. In such multi-agent environments, additional learning problems arise due to the continually changing decision-making policies of agents. This paper surveys recent works that address the non-stationarity problem in multi-agent deep reinforcement learning. The surveyed methods range from modifications in the training procedure, such as centralized training, to learning representations of the opponent's policy, meta-learning, communication, and decentralized learning. The survey concludes with a list of open problems and possible lines of future research.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-05-08T16:26:48},
  eprint           = {1906.04737},
  file             = {:Papoudakis_2019_Dealing with Non Stationarity in Multi Agent Deep Reinforcement Learning.pdf:PDF},
  groups           = {MARL},
  keywords         = {cs.LG, cs.AI, cs.MA, stat.ML},
  modificationdate = {2022-05-08T16:27:38},
  primaryclass     = {cs.LG},
}

@InProceedings{Zang2020MetaLight,
  author           = {Xinshi Zang and Huaxiu Yao and Guanjie Zheng and Nan Xu and Kai Xu and Zhenhui Li},
  booktitle        = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
  title            = {{MetaLight}: Value-Based Meta-Reinforcement Learning for Traffic Signal Control},
  year             = {2020},
  month            = apr,
  number           = {01},
  pages            = {1153--1160},
  publisher        = {Association for the Advancement of Artificial Intelligence ({AAAI})},
  volume           = {34},
  abstract         = {Using reinforcement learning for traffic signal control has attracted increasing interests recently. Various value-based reinforcement learning methods have been proposed to deal with this classical transportation problem and achieved better performances compared with traditional transportation methods. However, current reinforcement learning models rely on tremendous training data and computational resources, which may have bad consequences (e.g., traffic jams or accidents) in the real world. In traffic signal control, some algorithms have been proposed to empower quick learning from scratch, but little attention is paid to learning by transferring and reusing learned experience. In this paper, we propose a novel framework, named as MetaLight, to speed up the learning process in new scenarios by leveraging the knowledge learned from existing scenarios. MetaLight is a value-based meta-reinforcement learning workflow based on the representative gradient-based meta-learning algorithm (MAML), which includes periodically alternate individual-level adaptation and global-level adaptation. Moreover, MetaLight improves the-state-of-the-art reinforcement learning model FRAP in traffic signal control by optimizing its model structure and updating paradigm. The experiments on four real-world datasets show that our proposed MetaLight not only adapts more quickly and stably in new traffic scenarios, but also achieves better performance.},
  creationdate     = {2022-05-08T22:34:37},
  doi              = {10.1609/aaai.v34i01.5467},
  file             = {:Zang_2020_MetaLight_ Value Based Meta Reinforcement Learning for Traffic Signal Control.pdf:PDF},
  groups           = {Meta Learning},
  modificationdate = {2022-06-15T16:04:51},
}

@InProceedings{Rix2022Review,
  author           = {Jennifer Rix},
  booktitle        = {Proceedings of the Annual Hawaii International Conference on System Sciences},
  title            = {From Tools to Teammates: Conceptualizing Humans' Perception of Machines as Teammates with a Systematic Literature Review},
  year             = {2022},
  publisher        = {Hawaii International Conference on System Sciences},
  abstract         = {The accelerating capabilities of systems brought about by advances in Artificial Intelligence challenge the traditional notion of systems as tools. Systems’ increasingly agentic and collaborative character offers the potential for a new user-system interaction paradigm: Teaming replaces unidirectional system use. Yet, extant literature addresses the prerequisites for this new interaction paradigm inconsistently, often not even considering the foundations established in human teaming literature. To address this, this study utilizes a systematic literature review to conceptualize the drivers of the perception of systems as teammates instead of tools. Hereby, it integrates insights from the dispersed and interdisciplinary field of human-machine teaming with established human teaming principles. The creation of a team setting and a social entity, as well as specific configurations of the machine teammate’s collaborative behaviors, are identified as main drivers of the formation of impactful human-machine teams.},
  creationdate     = {2022-05-08T22:38:39},
  doi              = {10.24251/hicss.2022.048},
  file             = {:Rix_2022_From Tools to Teammates_ Conceptualizing Humans' Perception of Machines As Teammates with a Systematic Literature Review.pdf:PDF;:Rix_2022_From Tools to Teammates_ Conceptualizing Humans' Perception of Machines As Teammates with a Systematic Literature Review (1).pdf:PDF},
  modificationdate = {2022-05-08T22:42:07},
}

@InProceedings{Lowe2019Learning,
  author           = {Lowe, Ryan and Gupta, Abhinav and Foerster, Jakob and Kiela, Douwe and Pineau, Joelle},
  booktitle        = {Proceedings of the 1st Adaptive \& Multitask Learning Workshop},
  title            = {Learning to learn to communicate},
  year             = {2019},
  creationdate     = {2022-05-10T15:34:55},
  file             = {:Lowe_2019_Learning to Learn to Communicate.pdf:PDF},
  groups           = {Meta Learning},
  modificationdate = {2022-05-10T15:36:13},
}

@Misc{Li2021Celebrating,
  author           = {Chenghao Li and Tonghan Wang and Chengjie Wu and Qianchuan Zhao and Jun Yang and Chongjie Zhang},
  month            = jun,
  title            = {Celebrating Diversity in Shared Multi-Agent Reinforcement Learning},
  year             = {2021},
  abstract         = {Recently, deep multi-agent reinforcement learning (MARL) has shown the promise to solve complex cooperative tasks. Its success is partly because of parameter sharing among agents. However, such sharing may lead agents to behave similarly and limit their coordination capacity. In this paper, we aim to introduce diversity in both optimization and representation of shared multi-agent reinforcement learning. Specifically, we propose an information-theoretical regularization to maximize the mutual information between agents' identities and their trajectories, encouraging extensive exploration and diverse individualized behaviors. In representation, we incorporate agent-specific modules in the shared neural network architecture, which are regularized by L1-norm to promote learning sharing among agents while keeping necessary diversity. Empirical results show that our method achieves state-of-the-art performance on Google Research Football and super hard StarCraft II micromanagement tasks.},
  archiveprefix    = {arXiv},
  comment          = {https://zhuanlan.zhihu.com/p/463634515
MARL中，各智能体共享参数可以减少搜索空间，但导致趋同；因此将参数分为共享和不共享两部分},
  creationdate     = {2022-05-11T12:19:21},
  eprint           = {2106.02195},
  file             = {:Li_2021_Celebrating Diversity in Shared Multi Agent Reinforcement Learning.pdf:PDF},
  groups           = {MARL},
  keywords         = {cs.LG},
  modificationdate = {2023-01-07T21:59:37},
  primaryclass     = {cs.LG},
}

@InProceedings{Tan1993IQL,
  author           = {Tan, Ming},
  booktitle        = {Proceedings of the Tenth International Conference on International Conference on Machine Learning},
  title            = {Multi-agent reinforcement learning: independent vs. cooperative agents},
  year             = {1993},
  pages            = {330--337},
  creationdate     = {2022-05-12T17:24:14},
  groups           = {MARL},
  modificationdate = {2022-05-12T17:25:03},
}

@Article{Mnih2015DQN,
  author           = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal          = {Nature},
  title            = {Human-level control through deep reinforcement learning},
  year             = {2015},
  number           = {7540},
  pages            = {529--533},
  volume           = {518},
  abstract         = {The theory of reinforcement learning provides a normative account1, deeply rooted in psychological2 and neuroscientific3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems4,5, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms3. While reinforcement learning agents have achieved some successes in a variety of domains6,7,8, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks9,10,11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games12. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
  creationdate     = {2022-05-12T17:39:23},
  file             = {:Mnih_2015_Human Level Control through Deep Reinforcement Learning (DQN).pdf:PDF;:(slides) Mnih_2015_Human Level Control through Deep Reinforcement Learning (DQN).pdf.pdf:PDF},
  modificationdate = {2022-06-15T16:07:08},
  publisher        = {Nature Publishing Group},
}

@InProceedings{Chouhan2012MultiAgentPlanning,
  author           = {Chouhan, Satyendra Singh and Niyogi, Rajdeep},
  booktitle        = {2012 Second International Conference on Advanced Computing \& Communication Technologies},
  title            = {Multi-agent Planning in Grid World Domain},
  year             = {2012},
  organization     = {IEEE},
  pages            = {117--122},
  creationdate     = {2022-05-18T15:32:21},
  doi              = {10.1109/ACCT.2012.75},
  file             = {:Chouhan_2012_Multi Agent Planning in Grid World Domain.pdf:PDF},
  groups           = {Planning},
  modificationdate = {2022-05-18T15:33:15},
}

@InProceedings{Jin2018Advertising,
  author           = {Junqi Jin and Chengru Song and Han Li and Kun Gai and Jun Wang and Weinan Zhang},
  booktitle        = {Proceedings of the 27th {ACM} International Conference on Information and Knowledge Management},
  title            = {Real-Time Bidding with Multi-Agent Reinforcement Learning in Display Advertising},
  year             = {2018},
  month            = oct,
  publisher        = {{ACM}},
  comment          = {B会 MARL在互联网广告投放上的应用},
  creationdate     = {2022-06-06T11:29:20},
  doi              = {10.1145/3269206.3272021},
  groups           = {ApplicationsInMARL},
  modificationdate = {2022-06-15T16:11:08},
}

@Misc{ShalevShwartz2016Driving,
  author           = {Shai Shalev-Shwartz and Shaked Shammah and Amnon Shashua},
  month            = oct,
  title            = {Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving},
  year             = {2016},
  abstract         = {Autonomous driving is a multi-agent setting where the host vehicle must apply sophisticated negotiation skills with other road users when overtaking, giving way, merging, taking left and right turns and while pushing ahead in unstructured urban roadways. Since there are many possible scenarios, manually tackling all possible cases will likely yield a too simplistic policy. Moreover, one must balance between unexpected behavior of other drivers/pedestrians and at the same time not to be too defensive so that normal traffic flow is maintained. In this paper we apply deep reinforcement learning to the problem of forming long term driving strategies. We note that there are two major challenges that make autonomous driving different from other robotic tasks. First, is the necessity for ensuring functional safety - something that machine learning has difficulty with given that performance is optimized at the level of an expectation over many instances. Second, the Markov Decision Process model often used in robotics is problematic in our case because of unpredictable behavior of other agents in this multi-agent scenario. We make three contributions in our work. First, we show how policy gradient iterations can be used without Markovian assumptions. Second, we decompose the problem into a composition of a Policy for Desires (which is to be learned) and trajectory planning with hard constraints (which is not learned). The goal of Desires is to enable comfort of driving, while hard constraints guarantees the safety of driving. Third, we introduce a hierarchical temporal abstraction we call an "Option Graph" with a gating mechanism that significantly reduces the effective horizon and thereby reducing the variance of the gradient estimation even further.},
  archiveprefix    = {arXiv},
  comment          = {MARL 自动驾驶领域},
  creationdate     = {2022-06-06T14:59:39},
  eprint           = {1610.03295},
  file             = {:http\://arxiv.org/pdf/1610.03295v1:PDF},
  groups           = {AutonomousDriving-MARL},
  keywords         = {cs.AI, cs.LG, stat.ML},
  modificationdate = {2022-06-06T16:35:29},
  primaryclass     = {cs.AI},
}

@Article{Ye2022Honor,
  author           = {Deheng Ye and Guibin Chen and Peilin Zhao and Fuhao Qiu and Bo Yuan and Wen Zhang and Sheng Chen and Mingfei Sun and Xiaoqian Li and Siqin Li and Jing Liang and Zhenjie Lian and Bei Shi and Liang Wang and Tengfei Shi and Qiang Fu and Wei Yang and Lanxiao Huang},
  journal          = {{IEEE} Transactions on Neural Networks and Learning Systems},
  title            = {Supervised Learning Achieves Human-Level Performance in {MOBA} Games: A Case Study of Honor of Kings},
  year             = {2022},
  month            = mar,
  number           = {3},
  pages            = {908--918},
  volume           = {33},
  comment          = {王者荣耀 腾讯 tencent moba},
  creationdate     = {2022-06-06T15:20:28},
  doi              = {10.1109/tnnls.2020.3029475},
  file             = {:Ye_2022_Supervised Learning Achieves Human Level Performance in MOBA Games_ a Case Study of Honor of Kings.pdf:PDF},
  groups           = {RTS-MARL},
  modificationdate = {2022-06-15T16:04:58},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InProceedings{Liu2017Medical,
  author           = {Ying Liu and Brent Logan and Ning Liu and Zhiyuan Xu and Jian Tang and Yangzhi Wang},
  booktitle        = {2017 {IEEE} International Conference on Healthcare Informatics ({ICHI})},
  title            = {Deep Reinforcement Learning for Dynamic Treatment Regimes on Medical Registry Data},
  year             = {2017},
  month            = aug,
  publisher        = {{IEEE}},
  creationdate     = {2022-06-06T15:25:42},
  doi              = {10.1109/ichi.2017.45},
  groups           = {ApplicationsInMARL},
  modificationdate = {2022-06-15T16:11:11},
}

@InProceedings{Wang2021Power,
  author           = {Wang, Jianhong and Xu, Wangkun and Gu, Yunjie and Song, Wenbin and Green, Tim C},
  booktitle        = {Advances in Neural Information Processing Systems},
  title            = {Multi-Agent Reinforcement Learning for Active Voltage Control on Power Distribution Networks},
  year             = {2021},
  editor           = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages            = {3271--3284},
  publisher        = {Curran Associates, Inc.},
  volume           = {34},
  abstract         = {This paper presents a problem in power networks that creates an exciting and yet challenging real-world scenario for application of multi-agent reinforcement learning (MARL). The emerging trend of decarbonisation is placing excessive stress on power distribution networks. Active voltage control is seen as a promising solution to relieve power congestion and improve voltage quality without extra hardware investment, taking advantage of the controllable apparatuses in the network, such as roof-top photovoltaics (PVs) and static var compensators (SVCs). These controllable apparatuses appear in a vast number and are distributed in a wide geographic area, making MARL a natural candidate. This paper formulates the active voltage control problem in the framework of Dec-POMDP and establishes an open-source environment. It aims to bridge the gap between the power community and the MARL community and be a drive force towards real-world applications of MARL algorithms. Finally, we analyse the special characteristics of the active voltage control problems that cause challenges (e.g. interpretability) for state-of-the-art MARL approaches, and summarise the potential directions.},
  comment          = {MARL 电力系统},
  creationdate     = {2022-06-06T16:02:18},
  groups           = {Power-MARL},
  modificationdate = {2022-06-06T16:39:42},
  url              = {https://proceedings.neurips.cc/paper/2021/file/1a6727711b84fd1efbb87fc565199d13-Paper.pdf},
}

@Article{Glavic2017PowerRL,
  author           = {Mevludin Glavic and Raphaël Fonteneau and Damien Ernst},
  journal          = {IFAC-PapersOnLine},
  title            = {Reinforcement Learning for Electric Power System Decision and Control: Past Considerations and Perspectives},
  year             = {2017},
  issn             = {2405-8963},
  note             = {20th IFAC World Congress},
  number           = {1},
  pages            = {6918--6927},
  volume           = {50},
  abstract         = {In this paper, we review past (including very recent) research considerations in using reinforcement learning (RL) to solve electric power system decision and control problems. The RL considerations are reviewed in terms of specific electric power system problems, type of control and RL method used. We also provide observations about past considerations based on a comprehensive review of available publications. The review reveals the RL is considered as viable solutions to many decision and control problems across different time scales and electric power system states. Furthermore, we analyse the perspectives of RL approaches in light of the emergence of new-generation, communications, and instrumentation technologies currently in use, or available for future use, in power systems. The perspectives are also analysed in terms of recent breakthroughs in RL algorithms (Safe RL, Deep RL and path integral control for RL) and other, not previously considered, problems for RL considerations (most notably restorative, emergency controls together with so-called system integrity protection schemes, fusion with existing robust controls, and combining preventive and emergency control).},
  comment          = {single RL?},
  creationdate     = {2022-06-06T16:03:42},
  doi              = {https://doi.org/10.1016/j.ifacol.2017.08.1217},
  groups           = {ApplicationsInRL},
  keywords         = {Electric power system, reinforcement learning, control, decision},
  modificationdate = {2022-06-15T15:59:42},
  url              = {https://www.sciencedirect.com/science/article/pii/S2405896317317238},
}

@Article{Cao2020ReviewPowerRL,
  author           = {Di Cao and Weihao Hu and Junbo Zhao and Guozhou Zhang and Bin Zhang and Zhou Liu and Zhe Chen and Frede Blaabjerg},
  journal          = {Journal of Modern Power Systems and Clean Energy},
  title            = {Reinforcement Learning and Its Applications in Modern Power and Energy Systems: A Review},
  year             = {2020},
  number           = {6},
  pages            = {1029--1042},
  volume           = {8},
  creationdate     = {2022-06-06T16:05:28},
  doi              = {10.35833/mpce.2020.000552},
  modificationdate = {2022-06-06T16:05:48},
  publisher        = {Journal of Modern Power Systems and Clean Energy},
}

@InProceedings{Jestel2021Robot,
  author           = {Christian Jestel and Harmtmut Surmann and Jonas Stenzel and Oliver Urbann and Marius Brehler},
  booktitle        = {2021 7th International Conference on Automation, Robotics and Applications ({ICARA})},
  title            = {Obtaining Robust Control and Navigation Policies for Multi-robot Navigation via Deep Reinforcement Learning},
  year             = {2021},
  month            = feb,
  pages            = {48--54},
  publisher        = {{IEEE}},
  abstract         = {Multi-robot navigation is a challenging task in which multiple robots must be coordinated simultaneously within dynamic environments. We apply deep reinforcement learning (DRL) to learn a decentralized end-to-end policy which maps raw sensor data to the command velocities of the agent. In order to enable the policy to generalize, the training is performed in different environments and scenarios. The learned policy is tested and evaluated in common multi-robot scenarios like switching a place, an intersection and a bottleneck situation. This policy allows the agent to recover from dead ends and to navigate through complex environments.},
  comment          = {MARL in robotics},
  creationdate     = {2022-06-06T16:26:21},
  doi              = {10.1109/icara51699.2021.9376457},
  groups           = {Robotics-MARL},
  modificationdate = {2022-06-15T16:11:14},
}

@InProceedings{Wei2021RobotPath,
  author           = {Yongyong Wei and Rong Zheng},
  booktitle        = {{IEEE} {INFOCOM} 2021 - {IEEE} Conference on Computer Communications},
  title            = {Multi-Robot Path Planning for Mobile Sensing through Deep Reinforcement Learning},
  year             = {2021},
  month            = may,
  pages            = {1--10},
  publisher        = {{IEEE}},
  abstract         = {Mobile sensing is an effective way to collect environmental data such as air quality, humidity and temperature at low costs. However, mobile robots are typically battery powered and have limited travel distances. To accelerate data collection in large geographical areas, it is beneficial to deploy multiple robots to perform tasks in parallel. In this paper, we investigate the Multi-Robot Informative Path Planning (MIPP) problem, namely, to plan the most informative paths in a target area subject to the budget constraints of multiple robots. We develop two deep reinforcement learning (RL) based cooperative strategies: independent learning through credit assignment and sequential rollout based learning for MIPP. Both strategies are highly scalable with the number of robots. Extensive experiments are conducted to evaluate the performance of the proposed and baseline approaches using real-world WiFi Received Signal Strength (RSS) data. In most cases, the RL based solutions achieve superior or similar performance as a baseline genetic algorithm (GA)-based solution but at only a fraction of running time during inference. Furthermore, when the budgets and initial positions of the robots change, the pre-trained policies can be applied directly.},
  creationdate     = {2022-06-06T16:32:56},
  doi              = {10.1109/infocom42981.2021.9488669},
  groups           = {Robotics-MARL},
  modificationdate = {2022-06-15T16:11:18},
}

@Article{Liu2021Visu,
  author           = {Zhe Liu and Qiming Liu and Ling Tang and Kefan Jin and Hongye Wang and Ming Liu and Hesheng Wang},
  journal          = {{IEEE} Transactions on Automation Science and Engineering},
  title            = {Visuomotor Reinforcement Learning for Multirobot Cooperative Navigation},
  year             = {2021},
  pages            = {1--12},
  abstract         = {This article investigates the multirobot cooperative navigation problem based on raw visual observations. A fully end-to-end learning framework is presented, which leverages graph neural networks to learn local motion coordination and utilizes deep reinforcement learning to generate visuomotor policy that enables each robot to move to its goal without the need of environment map and global positioning information. Experimental results show that, with a few tens of robots, our approach achieves comparable performance with the state-of-the-art imitation learning-based approaches with bird-view state inputs. We also illustrate our generalizability to crowded and large environments and our scalability to ten times number of the training robots. In addition, we demonstrate that our model trained for multirobot case can also improve the success rate in the single-robot navigation task in unseen environments.},
  creationdate     = {2022-06-06T16:33:41},
  doi              = {10.1109/tase.2021.3114327},
  groups           = {Robotics-MARL},
  modificationdate = {2022-10-29T09:22:58},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InCollection{Bhalla2020Deep,
  author           = {Sushrut Bhalla and Sriram Ganapathi Subramanian and Mark Crowley},
  booktitle        = {Advances in Artificial Intelligence},
  publisher        = {Springer International Publishing},
  title            = {Deep Multi Agent Reinforcement Learning for Autonomous Driving},
  year             = {2020},
  pages            = {67--78},
  abstract         = {Deep Learning and back-propagation have been successfully used to perform centralized training with communication protocols among multiple agents in a cooperative environment. In this work, we present techniques for centralized training of Multi-Agent Deep Reinforcement Learning (MARL) using the model-free Deep Q-Network (DQN) as the baseline model and communication between agents. We present two novel, scalable and centralized MARL training techniques (MA-MeSN, MA-BoN), which achieve faster convergence and higher cumulative reward in complex domains like autonomous driving simulators. Subsequently, we present a memory module to achieve a decentralized cooperative policy for execution and thus addressing the challenges of noise and communication bottlenecks in real-time communication channels. This work theoretically and empirically compares our centralized and decentralized training algorithms to current research in the field of MARL. We also present and release a new OpenAI-Gym environment which can be used for multi-agent research as it simulates multiple autonomous cars driving on a highway. We compare the performance of our centralized algorithms to existing state-of-the-art algorithms, DIAL and IMS based on cumulative reward achieved per episode. MA-MeSN and MA-BoN achieve a cumulative reward of at-least of the reward achieved by the DIAL and IMS. We also present an ablation study of the scalability of MA-BoN showing that it has a linear time and space complexity compared to quadratic for DIAL in the number of agents.},
  comment          = {MARL 自动驾驶},
  creationdate     = {2022-06-06T16:34:31},
  doi              = {10.1007/978-3-030-47358-7_7},
  groups           = {AutonomousDriving-MARL},
  modificationdate = {2022-10-29T09:22:42},
}

@Article{Yu2020Driving,
  author           = {Chao Yu and Xin Wang and Xin Xu and Minjie Zhang and Hongwei Ge and Jiankang Ren and Liang Sun and Bingcai Chen and Guozhen Tan},
  journal          = {{IEEE} Transactions on Intelligent Transportation Systems},
  title            = {Distributed Multiagent Coordinated Learning for Autonomous Driving in Highways Based on Dynamic Coordination Graphs},
  year             = {2020},
  month            = feb,
  number           = {2},
  pages            = {735--748},
  volume           = {21},
  abstract         = {Autonomous driving is one of the most important AI applications and has attracted extensive interest in recent years. A large number of studies have successfully applied reinforcement learning techniques in various aspects of autonomous driving, ranging from low-level control of driving maneuvers to higher level of strategic decision-making. However, comparatively less progress has been made in investigating how co-existing autonomous vehicles would interact with each other in a common environment and how reinforcement learning can be helpful in such situations by applying multiagent reinforcement learning techniques in the high-level strategic decision-making of the following or overtaking for a group of autonomous vehicles in highway scenarios. Learning to achieve coordination among vehicles in such situations is challenging due to the unique feature of vehicular mobility, which renders it infeasible to directly apply the existing coordinated learning approaches. To solve this problem, we propose using dynamic coordination graph to model the continuously changing topology during vehicles' interactions and come up with two basic learning approaches to coordinate the driving maneuvers for a group of vehicles. Several extension mechanisms are then presented to make these approaches workable in a more complex and realistic setting with any number of vehicles. The experimental evaluation has verified the benefits of the proposed coordinated learning approaches, compared with other approaches that learn without coordination or rely on some traditional mobility models based on some expert driving rules.},
  comment          = {MARL 自动驾驶},
  creationdate     = {2022-06-06T16:34:31},
  doi              = {10.1109/tits.2019.2893683},
  groups           = {AutonomousDriving-MARL},
  modificationdate = {2022-06-15T16:04:55},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Kiran2022Survey,
  author           = {B Ravi Kiran and Ibrahim Sobh and Victor Talpaert and Patrick Mannion and Ahmad A. Al Sallab and Senthil Yogamani and Patrick Perez},
  journal          = {{IEEE} Transactions on Intelligent Transportation Systems},
  title            = {Deep Reinforcement Learning for Autonomous Driving: A Survey},
  year             = {2022},
  month            = jun,
  number           = {6},
  pages            = {4909--4926},
  volume           = {23},
  abstract         = {With the development of deep representation learning, the domain of reinforcement learning (RL) has become a powerful learning framework now capable of learning complex policies in high dimensional environments. This review summarises deep reinforcement learning (DRL) algorithms and provides a taxonomy of automated driving tasks where (D)RL methods have been employed, while addressing key computational challenges in real world deployment of autonomous driving agents. It also delineates adjacent domains such as behavior cloning, imitation learning, inverse reinforcement learning that are related but are not classical RL algorithms. The role of simulators in training agents, methods to validate, test and robustify existing solutions in RL are discussed.},
  comment          = {强化学习在自动驾驶领域的应用},
  creationdate     = {2022-06-06T16:34:58},
  doi              = {10.1109/tits.2021.3054625},
  file             = {:Kiran_2022_Deep Reinforcement Learning for Autonomous Driving_ a Survey.pdf:PDF},
  groups           = {AutonomousDriving-MARL},
  modificationdate = {2022-06-15T16:12:11},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Yan2020Load,
  author           = {Ziming Yan and Yan Xu},
  journal          = {{IEEE} Transactions on Power Systems},
  title            = {A Multi-Agent Deep Reinforcement Learning Method for Cooperative Load Frequency Control of a Multi-Area Power System},
  year             = {2020},
  month            = nov,
  number           = {6},
  pages            = {4599--4608},
  volume           = {35},
  creationdate     = {2022-06-06T16:44:11},
  doi              = {10.1109/tpwrs.2020.2999890},
  groups           = {Power-MARL},
  modificationdate = {2022-06-15T16:03:07},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Wang2020Voltage,
  author           = {Shengyi Wang and Jiajun Duan and Di Shi and Chunlei Xu and Haifeng Li and Ruisheng Diao and Zhiwei Wang},
  journal          = {{IEEE} Transactions on Power Systems},
  title            = {A Data-Driven Multi-Agent Autonomous Voltage Control Framework Using Deep Reinforcement Learning},
  year             = {2020},
  month            = nov,
  number           = {6},
  pages            = {4644--4654},
  volume           = {35},
  creationdate     = {2022-06-06T16:44:38},
  doi              = {10.1109/tpwrs.2020.2990179},
  groups           = {Power-MARL},
  modificationdate = {2022-06-15T16:12:27},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InProceedings{Ng1999Shaping,
  author           = {Andrew Y. Ng and Daishi Harada and Stuart Russell},
  booktitle        = {In Proceedings of the Sixteenth International Conference on Machine Learning},
  title            = {Policy invariance under reward transformations: Theory and application to reward shaping},
  year             = {1999},
  pages            = {278--287},
  publisher        = {Morgan Kaufmann},
  comment          = {reward shaping},
  creationdate     = {2022-06-08T19:52:31},
  file             = {:Ng_1999_Policy Invariance under Reward Transformations_ Theory and Application to Reward Shaping.pdf:PDF},
  modificationdate = {2022-06-08T19:53:52},
}

@Misc{Zou2019Shaping,
  author           = {Haosheng Zou and Tongzheng Ren and Dong Yan and Hang Su and Jun Zhu},
  month            = jan,
  title            = {Reward Shaping via Meta-Learning},
  year             = {2019},
  abstract         = {Reward shaping is one of the most effective methods to tackle the crucial yet challenging problem of credit assignment in Reinforcement Learning (RL). However, designing shaping functions usually requires much expert knowledge and hand-engineering, and the difficulties are further exacerbated given multiple similar tasks to solve. In this paper, we consider reward shaping on a distribution of tasks, and propose a general meta-learning framework to automatically learn the efficient reward shaping on newly sampled tasks, assuming only shared state space but not necessarily action space. We first derive the theoretically optimal reward shaping in terms of credit assignment in model-free RL. We then propose a value-based meta-learning algorithm to extract an effective prior over the optimal reward shaping. The prior can be applied directly to new tasks, or provably adapted to the task-posterior while solving the task within few gradient updates. We demonstrate the effectiveness of our shaping through significantly improved learning efficiency and interpretable visualizations across various settings, including notably a successful transfer from DQN to DDPG.},
  archiveprefix    = {arXiv},
  comment          = {Reward shaping},
  creationdate     = {2022-06-08T19:54:05},
  eprint           = {1901.09330},
  file             = {:http\://arxiv.org/pdf/1901.09330v1:PDF},
  keywords         = {cs.LG, stat.ML},
  modificationdate = {2022-06-08T19:54:20},
  primaryclass     = {cs.LG},
}

@InCollection{Schurr2004Machinetta,
  author           = {Nathan Schurr and Steven Okamoto and Rajiv T. Maheswaran and Paul Scerri and Milind Tambe},
  booktitle        = {Cognition and Multi-Agent Interaction},
  publisher        = {Cambridge University Press},
  title            = {From STEAM to Machinetta: Evolution of a Teamwork Model},
  year             = {2004},
  month            = dec,
  pages            = {307--327},
  creationdate     = {2022-06-10T10:39:12},
  doi              = {10.1017/cbo9780511610721.013},
  file             = {:Schurr_2004_From STEAM to Machinetta_ Evolution of a Teamwork Model.pdf:PDF},
  groups           = {Teamwork},
  modificationdate = {2022-06-15T16:12:30},
}

@Article{Tambe1997STEAM,
  author           = {M. Tambe},
  journal          = {Journal of Artificial Intelligence Research},
  title            = {Towards Flexible Teamwork},
  year             = {1997},
  month            = sep,
  pages            = {83--124},
  volume           = {7},
  abstract         = {Many AI researchers are today striving to build agent teams for complex, dynamic multi-agent domains, with intended applications in arenas such as education, training, entertainment, information integration, and collective robotics. Unfortunately, uncertainties in these complex, dynamic domains obstruct coherent teamwork. In particular, team members often encounter differing, incomplete, and possibly inconsistent views of their environment. Furthermore, team members can unexpectedly fail in fulfilling responsibilities or discover unexpected opportunities. Highly flexible coordination and communication is key in addressing such uncertainties. Simply fitting individual agents with precomputed coordination plans will not do, for their inflexibility can cause severe failures in teamwork, and their domain-specificity hinders reusability. Our central hypothesis is that the key to such flexibility and reusability is providing agents with general models of teamwork. Agents exploit such models to autonomously reason about coordination and communication, providing requisite flexibility. Furthermore, the models enable reuse across domains, both saving implementation effort and enforcing consistency. This article presents one general, implemented model of teamwork, called STEAM. The basic building block of teamwork in STEAM is joint intentions (Cohen & Levesque, 1991b); teamwork in STEAM is based on agents' building up a (partial) hierarchy of joint intentions (this hierarchy is seen to parallel Grosz & Kraus's partial SharedPlans, 1996). Furthermore, in STEAM, team members monitor the team's and individual members' performance, reorganizing the team as necessary. Finally, decision-theoretic communication selectivity in STEAM ensures reduction in communication overheads of teamwork, with appropriate sensitivity to the environmental conditions. This article describes STEAM's application in three different complex domains, and presents detailed empirical results.},
  creationdate     = {2022-06-10T10:42:18},
  doi              = {10.1613/jair.433},
  file             = {:Tambe_1997_Towards Flexible Teamwork.pdf:PDF},
  groups           = {Teamwork},
  modificationdate = {2022-06-15T16:12:34},
  publisher        = {{AI} Access Foundation},
}

@InProceedings{McCarthy2018Formation,
  author           = {Mc Carthy, Sara Marie},
  booktitle        = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title            = {Adaptive and dynamic team formation for strategic and tactical planning},
  year             = {2018},
  number           = {1},
  pages            = {8024--8025},
  volume           = {32},
  abstract         = {Past work in security games has mainly focused on the problem static resource allocation; how to optimally deploy a given fixed team of resources. My research aims to address the challenge of integrating operational planning into security games, where resources are heterogeneous and the defender is tasked with optimizing over both the investment into these resources, as well as their deployment in the field. This allows the defender to design more adaptive strategies, reason about the efficiency of their use of these resources as well as their effectiveness in their deployment. This thesis explores the challenges in integrating these two optimization problems in both the single stage and multi-stage setting and provides a formal model of this problem, which we refer to as the Simultaneous Optimization of Resource Teams and Tactics (SORT) as a new fundamental research problem in security games that combines strategic and tactical decision making. The main contributions of this work are solution methods to the SORT problem under various settings as well as exploring various types of tradeoffs that can arise in these settings. These include managing budget for investment in resources as well as capacity constraints on use of resources. My work addresses scenarios when the tactical decision problem (optimal deployment) is difficult, and thus evaluating the performance of any given team is difficult. Additionally, I address domains where we are tasked with making repeated strategic level decision and where, due to changing domain features, fluctuations in time dependent processes or the realization of uncertain parameters in the problem, it becomes necessary to re-evaluate and adapt to new information.},
  creationdate     = {2022-06-10T10:51:23},
  file             = {:Mc Carthy_2018_Adaptive and Dynamic Team Formation for Strategic and Tactical Planning.pdf:PDF},
  groups           = {Teamwork},
  modificationdate = {2022-06-15T15:59:39},
}

@Article{ElGibreen2019Dynamic,
  author           = {Hebah ElGibreen and Kamal Youcef-Toumi},
  journal          = {Autonomous Robots},
  title            = {Dynamic task allocation in an uncertain environment with heterogeneous multi-agents},
  year             = {2019},
  month            = jan,
  number           = {7},
  pages            = {1639--1664},
  volume           = {43},
  creationdate     = {2022-06-10T10:54:32},
  doi              = {10.1007/s10514-018-09820-5},
  file             = {:ElGibreen_2019_Dynamic Task Allocation in an Uncertain Environment with Heterogeneous Multi Agents.pdf:PDF},
  groups           = {Teamwork},
  modificationdate = {2022-06-15T16:09:38},
  publisher        = {Springer Science and Business Media {LLC}},
}

@InProceedings{Jiang2018ATOC,
  author           = {Jiang, Jiechuan and Lu, Zongqing},
  booktitle        = {Advances in Neural Information Processing Systems},
  title            = {Learning Attentional Communication for Multi-Agent Cooperation},
  year             = {2018},
  editor           = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  pages            = {7254--7264},
  publisher        = {Curran Associates, Inc.},
  volume           = {31},
  abstract         = {Communication could potentially be an effective way for multi-agent cooperation. However, information sharing among all agents or in predefined communication architectures that existing methods adopt can be problematic. When there is a large number of agents, agents cannot differentiate valuable information that helps cooperative decision making from globally shared information. Therefore, communication barely helps, and could even impair the learning of multi-agent cooperation. Predefined communication architectures, on the other hand, restrict communication among agents and thus restrain potential cooperation. To tackle these difficulties, in this paper, we propose an attentional communication model that learns when communication is needed and how to integrate shared information for cooperative decision making. Our model leads to efficient and effective communication for large-scale multi-agent cooperation. Empirically, we show the strength of our model in a variety of cooperative scenarios, where agents are able to develop more coordinated and sophisticated strategies than existing methods.},
  creationdate     = {2022-06-10T20:43:11},
  file             = {:Jiang_2018_Learning Attentional Communication for Multi Agent Cooperation (ATOC).pdf:PDF},
  groups           = {MARL},
  modificationdate = {2022-06-10T21:51:13},
  url              = {https://proceedings.neurips.cc/paper/2018/file/6a8018b3a00b69c008601b8becae392b-Paper.pdf},
}

@InProceedings{Iqbal2019MAAC,
  author           = {Iqbal, Shariq and Sha, Fei},
  booktitle        = {Proceedings of the 36th International Conference on Machine Learning},
  title            = {Actor-Attention-Critic for Multi-Agent Reinforcement Learning},
  year             = {2019},
  editor           = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  month            = jun,
  pages            = {2961--2970},
  publisher        = {PMLR},
  series           = {Proceedings of Machine Learning Research},
  volume           = {97},
  abstract         = {Reinforcement learning in multi-agent scenarios is important for real-world applications but presents challenges beyond those seen in single-agent settings. We present an actor-critic algorithm that trains decentralized policies in multi-agent settings, using centrally computed critics that share an attention mechanism which selects relevant information for each agent at every timestep. This attention mechanism enables more effective and scalable learning in complex multi-agent environments, when compared to recent approaches. Our approach is applicable not only to cooperative settings with shared rewards, but also individualized reward settings, including adversarial settings, as well as settings that do not provide global states, and it makes no assumptions about the action spaces of the agents. As such, it is flexible enough to be applied to most multi-agent learning problems.},
  creationdate     = {2022-06-10T21:25:18},
  file             = {:Iqbal_2019_Actor Attention Critic for Multi Agent Reinforcement Learning （MAAC).pdf:PDF},
  groups           = {MARL},
  modificationdate = {2022-09-15T08:52:48},
  pdf              = {http://proceedings.mlr.press/v97/iqbal19a/iqbal19a.pdf},
  url              = {https://proceedings.mlr.press/v97/iqbal19a.html},
}

@InProceedings{Sunehag2018Team,
  author           = {Sunehag, Peter and Lever, Guy and Gruslys, Audrunas and Czarnecki, Wojciech Marian and Zambaldi, Vinicius and Jaderberg, Max and Lanctot, Marc and Sonnerat, Nicolas and Leibo, Joel Z. and Tuyls, Karl and Graepel, Thore},
  booktitle        = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
  title            = {Value-Decomposition Networks For Cooperative Multi-Agent Learning Based On Team Reward},
  year             = {2018},
  address          = {Richland, SC},
  pages            = {2085--2087},
  publisher        = {International Foundation for Autonomous Agents and Multiagent Systems},
  series           = {AAMAS '18},
  abstract         = {We study the problem of cooperative multi-agent reinforcement learning with a single joint reward signal. This class of learning problems is difficult because of the often large combined action and observation spaces. In the fully centralized and decentralized approaches, we find the problem of spurious rewards and a phenomenon we call the "lazy agent'' problem, which arises due to partial observability. We address these problems by training individual agents with a novel value-decomposition network architecture, which learns to decompose the team value function into agent-wise value functions.},
  creationdate     = {2022-06-10T21:36:58},
  groups           = {MARL},
  keywords         = {multi-agent, neural networks, reinforcement learning, dqn, q-learning, collaborative, value-decomposition},
  location         = {Stockholm, Sweden},
  modificationdate = {2022-06-15T15:57:46},
  numpages         = {3},
}

@InProceedings{Tobin2017Domain,
  author           = {Josh Tobin and Rachel Fong and Alex Ray and Jonas Schneider and Wojciech Zaremba and Pieter Abbeel},
  booktitle        = {2017 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
  title            = {Domain randomization for transferring deep neural networks from simulation to the real world},
  year             = {2017},
  month            = sep,
  pages            = {23--30},
  publisher        = {{IEEE}},
  abstract         = {Bridging the ‘reality gap’ that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.},
  creationdate     = {2022-06-14T10:14:12},
  doi              = {10.1109/iros.2017.8202133},
  file             = {:Tobin_2017_Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World.pdf:PDF},
  groups           = {DomainRandomization},
  modificationdate = {2022-06-15T16:12:40},
}

@InProceedings{Grun2019Evaluation,
  author           = {Silas Grun and Simon Honinger and Paul Maria Scheikl and Bjorn Hein and Torsten Kroger},
  booktitle        = {2019 19th International Conference on Advanced Robotics ({ICAR})},
  title            = {Evaluation of Domain Randomization Techniques for Transfer Learning},
  year             = {2019},
  month            = dec,
  pages            = {481--486},
  publisher        = {{IEEE}},
  abstract         = {To address the challenge of resource-intensive data collection from real robotic environments, many deep learning applications use synthetic data to train their networks. This creates new problems when transferring the obtained knowledge from the simulated to the real world domain. Various aspects of the simulation, which do not influence the learning objective, can be randomized to enhance generalization to new domains. In this paper, we analyze the effect of these domain randomization techniques. To get an insight into their benefits, we apply them while training a grasp success classifier based on state-of-the-art CNN for an industrial robot as a showcase. We generated a large synthetic data set containing 1.44M RGB images with 48 permutations of 6 different randomizations and a base scenario as training data. The resulting networks, each trained on a different subset of this data set, are evaluated on 3k real world images of the robot performing grasps. We observed the effectiveness of randomization of perspective, distractors, lighting and the grasped box. Notably, we show that pretrained networks benefit from these techniques in particular.},
  creationdate     = {2022-06-14T10:17:50},
  doi              = {10.1109/icar46387.2019.8981654},
  file             = {:Grun_2019_Evaluation of Domain Randomization Techniques for Transfer Learning.pdf:PDF},
  groups           = {DomainRandomization},
  modificationdate = {2022-06-15T16:10:01},
}

@InProceedings{Chen2021Zeroshot,
  author           = {Fanfei Chen and Paul Szenher and Yewei Huang and Jinkun Wang and Tixiao Shan and Shi Bai and Brendan Englot},
  booktitle        = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
  title            = {Zero-Shot Reinforcement Learning on Graphs for Autonomous Exploration Under Uncertainty},
  year             = {2021},
  month            = may,
  pages            = {5193--5199},
  publisher        = {{IEEE}},
  abstract         = {This paper studies the problem of autonomous exploration under localization uncertainty for a mobile robot with 3D range sensing. We present a framework for self-learning a high-performance exploration policy in a single simulation environment, and transferring it to other environments, which may be physical or virtual. Recent work in transfer learning achieves encouraging performance by domain adaptation and domain randomization to expose an agent to scenarios that fill the inherent gaps in sim2sim and sim2real approaches. However, it is inefficient to train an agent in environments with randomized conditions to learn the important features of its current state. An agent can use domain knowledge provided by human experts to learn efficiently. We propose a novel approach that uses graph neural networks in conjunction with deep reinforcement learning, enabling decision-making over graphs containing relevant exploration information provided by human experts to predict a robot's optimal sensing action in belief space. The policy, which is trained only in a single simulation environment, offers a real-time, scalable, and transferable decision-making strategy, resulting in zero-shot transfer to other simulation environments and even real-world environments.},
  creationdate     = {2022-06-14T11:21:49},
  doi              = {10.1109/icra48506.2021.9561917},
  file             = {:Chen_2021_Zero Shot Reinforcement Learning on Graphs for Autonomous Exploration under Uncertainty.pdf:PDF},
  modificationdate = {2023-01-08T19:33:29},
  ranking          = {rank3},
}

@Misc{Slaoui2019Robust,
  author           = {Reda Bahi Slaoui and William R. Clements and Jakob N. Foerster and Sébastien Toth},
  month            = oct,
  title            = {Robust Visual Domain Randomization for Reinforcement Learning},
  year             = {2019},
  abstract         = {Producing agents that can generalize to a wide range of visually different environments is a significant challenge in reinforcement learning. One method for overcoming this issue is visual domain randomization, whereby at the start of each training episode some visual aspects of the environment are randomized so that the agent is exposed to many possible variations. However, domain randomization is highly inefficient and may lead to policies with high variance across domains. Instead, we propose a regularization method whereby the agent is only trained on one variation of the environment, and its learned state representations are regularized during training to be invariant across domains. We conduct experiments that demonstrate that our technique leads to more efficient and robust learning than standard domain randomization, while achieving equal generalization scores.},
  archiveprefix    = {arXiv},
  comment          = {Motivation: 使agent能够泛化到不同视觉的环境，但普通的域随机化效率太低，且可能导致不同域的策略差异太大
本文提出一种正则化方法，使得状态表示在训练期间正则化为跨域不变},
  creationdate     = {2022-06-14T11:40:34},
  eprint           = {1910.10537},
  file             = {:http\://arxiv.org/pdf/1910.10537v2:PDF},
  groups           = {DomainRandomization},
  keywords         = {cs.LG, cs.AI},
  modificationdate = {2022-06-14T11:44:18},
  primaryclass     = {cs.LG},
}

@InProceedings{CarmoAlves2022AdLeapMAS,
  author           = {do Carmo Alves, Matheus Aparecido and Varma, Amokh and Elkhatib, Yehia and Soriano Marcolino, Leandro},
  booktitle        = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  title            = {AdLeap-MAS: An Open-Source Multi-Agent Simulator for Ad-Hoc Reasoning},
  year             = {2022},
  address          = {Richland, SC},
  pages            = {1893--1895},
  publisher        = {International Foundation for Autonomous Agents and Multiagent Systems},
  series           = {AAMAS '22},
  abstract         = {Ad-hoc reasoning models are recurrently used to solve some of our daily tasks. Intending to avoid worthless investments or spend valuable resources, these smart systems requires a proper evaluation before acting in the real-world. In this paper, we demonstrate AdLeap-MAS, a novel framework focused on enabling quick and easy testing of smart algorithms in ad-hoc reasoning domains.},
  comment          = {ad hoc 测试环境，可以用来做一些标准化测试},
  creationdate     = {2022-06-19T01:46:40},
  doi              = {10.5555/3535850.3536143},
  file             = {:Carmo Alves_2022_AdLeap MAS_ an Open Source Multi Agent Simulator for Ad Hoc Reasoning.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  isbn             = {9781450392136},
  keywords         = {autonomous systems, open-source, simulation framework, ad-hoc reasoning, online planning},
  location         = {Virtual Event, New Zealand},
  modificationdate = {2022-12-08T16:54:49},
  numpages         = {3},
  url              = {https://github.com/lsmcolab/adleap-mas/},
}

@InProceedings{Gini2017Multi,
  author           = {Gini, Maria},
  booktitle        = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
  title            = {Multi-Robot Allocation of Tasks with Temporal and Ordering Constraints},
  year             = {2017},
  address          = {San Francisco, California, USA},
  pages            = {4863–4869},
  publisher        = {AAAI Press},
  series           = {AAAI'17},
  abstract         = {Task allocation is ubiquitous in computer science and robotics, yet some problems have received limited attention in the computer science and AI community. Specifically, we will focus on multi-robot task allocation problems when tasks have time windows or ordering constraints. We will outline the main lines of research and open problems.},
  comment          = {带时间窗口和时序约束的任务分配
消防救火背景},
  creationdate     = {2022-06-19T02:11:15},
  file             = {:Gini_2017_Multi Robot Allocation of Tasks with Temporal and Ordering Constraints.pdf:PDF},
  modificationdate = {2022-06-19T02:15:36},
  numpages         = {7},
  ranking          = {rank3},
}

@Article{Liang2022Continuous,
  author           = {Liang, Wenqian and Wang, Ji and Bao, Weidong and Zhu, Xiaomin and Wang, Qingyong and Han, Beibei},
  journal          = {Complex \& Intelligent Systems},
  title            = {Continuous self-adaptive optimization to learn multi-task multi-agent},
  year             = {2022},
  number           = {2},
  pages            = {1355--1367},
  volume           = {8},
  abstract         = {Multi-agent reinforcement learning (MARL) methods have shown superior performance to solve a variety of real-world problems focusing on learning distinct policies for individual tasks. These approaches face problems when applied to the non-stationary real-world: agents trained in specialized tasks cannot achieve satisfied generalization performance across multiple tasks; agents have to learn and store specialized policies for individual task and reliable identities of tasks are hardly observable in practice. To address the challenge continuously adapting to multiple tasks in MARL, we formalize the problem into a two-stage curriculum. Single-task policies are learned with MARL approaches, after that we develop a gradient-based Self-Adaptive Meta-Learning algorithm, SAML, that cannot only distill single-task policies into a unified policy but also can facilitate the unified policy to continuously adapt to new incoming tasks. In addition, to validate the continuous adaptation performance on complex task, we extend the widely adopted StarCraft benchmark SMAC and develop a new multi-task multi-agent StarCraft environment, Meta-SMAC, for testing various aspects of continuous adaptation method. Our experiments with a population of agents show that our method enables significantly more efficient adaptation than reactive baselines across different scenarios.},
  creationdate     = {2022-06-19T11:08:18},
  doi              = {10.1007/s40747-021-00591-8},
  file             = {:Liang_2022_Continuous Self Adaptive Optimization to Learn Multi Task Multi Agent.pdf:PDF},
  groups           = {Meta Learning},
  modificationdate = {2022-06-19T11:11:50},
  publisher        = {Springer},
  ranking          = {rank5},
}

@Article{Wen2021Multirobot,
  author           = {Shuhuan Wen and Zeteng Wen and Di Zhang and Hong Zhang and Tao Wang},
  journal          = {Applied Soft Computing},
  title            = {A multi-robot path-planning algorithm for autonomous navigation using meta-reinforcement learning based on transfer learning},
  year             = {2021},
  issn             = {1568-4946},
  pages            = {1--15},
  volume           = {110},
  abstract         = {The adaptability of multi-robot systems in complex environments is a hot topic. Aiming at static and dynamic obstacles in complex environments, this paper presents dynamic proximal meta policy optimization with covariance matrix adaptation evolutionary strategies (dynamic-PMPO-CMA) to avoid obstacles and realize autonomous navigation. Firstly, we propose dynamic proximal policy optimization with covariance matrix adaptation evolutionary strategies (dynamic-PPO-CMA) based on original proximal policy optimization (PPO) to obtain a valid policy of obstacles avoidance. The simulation results show that the proposed dynamic-PPO-CMA can avoid obstacles and reach the designated target position successfully. Secondly, in order to improve the adaptability of multi-robot systems in different environments, we integrate meta-learning with dynamic-PPO-CMA to form the dynamic-PMPO-CMA algorithm. In training process, we use the proposed dynamic-PMPO-CMA to train robots to learn multi-task policy. Finally, in testing process, transfer learning is introduced to the proposed dynamic-PMPO-CMA algorithm. The trained parameters of meta policy are transferred to new environments and regarded as the initial parameters. The simulation results show that the proposed algorithm can have faster convergence rate and arrive the destination more quickly than PPO, PMPO and dynamic-PPO-CMA.},
  comment          = {meta RL in multi-agent path finding},
  creationdate     = {2022-06-21T11:39:54},
  doi              = {https://doi.org/10.1016/j.asoc.2021.107605},
  file             = {:Wen_2021_A Multi Robot Path Planning Algorithm for Autonomous Navigation Using Meta Reinforcement Learning Based on Transfer Learning.pdf:PDF},
  groups           = {Meta Learning},
  keywords         = {Multi-robot system, Path planning, Deep reinforcement learning, Meta learning, Transfer learning},
  modificationdate = {2022-06-21T11:43:29},
  url              = {https://www.sciencedirect.com/science/article/pii/S1568494621005263},
}

@InProceedings{Pynadath2002Teamwork,
  author           = {David V. Pynadath and Milind Tambe},
  booktitle        = {Proceedings of the first international joint conference on Autonomous agents and multiagent systems part 2 - {AAMAS} {\textquotesingle}02},
  title            = {Multiagent Teamwork: Analyzing the Optimality and Complexity of Key Theories and Models},
  year             = {2002},
  publisher        = {{ACM} Press},
  creationdate     = {2022-06-27T00:39:59},
  doi              = {10.1145/544862.544946},
  groups           = {Teamwork},
  modificationdate = {2022-06-27T00:40:29},
}

@InProceedings{Kwak2010Teamwork,
  author           = {Kwak, Jun-young and Yang, Rong and Yin, Zhengyu and Taylor, Matthew E and Tambe, Milind},
  booktitle        = {Workshops at the Twenty-Fourth AAAI Conference on Artificial Intelligence},
  title            = {Teamwork and coordination under model uncertainty in DEC-POMDPs},
  year             = {2010},
  creationdate     = {2022-06-27T00:41:35},
  groups           = {Teamwork},
  modificationdate = {2022-06-27T00:41:37},
}

@Article{Chandrasekaran2016Adhoc,
  author           = {Muthukumaran Chandrasekaran and Prashant Doshi and Yifeng Zeng and Yingke Chen},
  journal          = {Autonomous Agents and Multi-Agent Systems},
  title            = {Can bounded and self-interested agents be teammates? Application to planning in ad hoc teams},
  year             = {2016},
  month            = nov,
  number           = {4},
  pages            = {821--860},
  volume           = {31},
  abstract         = {Planning for ad hoc teamwork is challenging because it involves agents collaborating without any prior coordination or communication. The focus is on principled methods for a single agent to cooperate with others. This motivates investigating the ad hoc teamwork problem in the context of self-interested decision-making frameworks. Agents engaged in individual decision making in multiagent settings face the task of having to reason about other agents’ actions, which may in turn involve reasoning about others. An established approximation that operationalizes this approach is to bound the infinite nesting from below by introducing level 0 models. For the purposes of this study, individual, self-interested decision making in multiagent settings is modeled using interactive dynamic influence diagrams ( I-DID ). These are graphical models with the benefit that they naturally offer a factored representation of the problem, allowing agents to ascribe dynamic models to others and reason about them. We demonstrate that an implication of bounded, finitely-nested reasoning by a self-interested agent is that we may not obtain optimal team solutions in cooperative settings, if it is part of a team. We address this limitation by including models at level 0 whose solutions involve reinforcement learning. We show how the learning is integrated into planning in the context of I-DID s. This facilitates optimal teammate behavior, and we demonstrate its applicability to ad hoc teamwork on several problem domains and configurations.},
  creationdate     = {2022-06-27T09:40:39},
  doi              = {10.1007/s10458-016-9354-4},
  file             = {:Chandrasekaran_2016_Can Bounded and Self Interested Agents Be Teammates_ Application to Planning in Ad Hoc Teams.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  modificationdate = {2023-04-04T13:45:41},
  publisher        = {Springer Science and Business Media {LLC}},
}

@InProceedings{Sarmasi2021MetaAdhoc,
  author           = {Aron Sarmasi and Timothy Zhang and Chu-Hung Cheng and Huyen Pham and Xuanchen Zhou and Duong Nguyen and Soumil Shekdar and Joshua Joshua McCoy},
  booktitle        = {The 16th International Conference on the Foundations of Digital Games ({FDG}) 2021},
  title            = {Meta-Learning a Solution to the Hanabi Ad-Hoc Challenge},
  year             = {2021},
  address          = {New York, NY, USA},
  month            = aug,
  pages            = {1--7},
  publisher        = {{ACM}},
  abstract         = {In this work we demonstrate that the First Order Model Agnostic Meta Learning (FOMAML) algorithm trained on the Hanabi Open Agent Dataset (HOAD) results in a model that is able to outplay both a naive MLP baseline, as well as a randomly selected partner in the Hanabi Ad-Hoc Challenge, in both low-shot and zero-shot setups. We first show that HOAD is well suited for the meta-learning task because its agents are high quality and utilize diverse strategies, thereby confirming that MAML is generalizing, and not memorizing agent strategies. We then detail our application of FOMAML to the cooperative decision making problem Hanabi entails, and we also provide evidence supporting recent results that the task update of MAML gives little to no test time performance boost. The pretrained models and game data are made available online at https://github.com/aronsar/hoad.},
  creationdate     = {2022-06-27T14:25:12},
  doi              = {10.1145/3472538.3472546},
  file             = {:Sarmasi_2021_Meta Learning a Solution to the Hanabi Ad Hoc Challenge.pdf:PDF},
  modificationdate = {2023-04-04T16:05:48},
  ranking          = {rank5},
}

@Article{Wang2022MetaTraffic,
  author           = {Min Wang and Libing Wu and Man Li and Dan Wu and Xiaochuan Shi and Chao Ma},
  journal          = {Knowledge-based Systems},
  title            = {Meta-learning based spatial-temporal graph attention network for traffic signal control},
  year             = {2022},
  month            = aug,
  pages            = {109166},
  volume           = {250},
  creationdate     = {2022-06-29T09:10:06},
  doi              = {10.1016/j.knosys.2022.109166},
  groups           = {knosys},
  modificationdate = {2023-04-04T16:05:55},
  publisher        = {Elsevier {BV}},
}

@Article{Feng2022MetaDiagnosis,
  author           = {Yong Feng and Jinglong Chen and Jingsong Xie and Tianci Zhang and Haixin Lv and Tongyang Pan},
  journal          = {Knowledge-Based Systems},
  title            = {Meta-learning as a promising approach for few-shot cross-domain fault diagnosis: Algorithms, applications, and prospects},
  year             = {2022},
  month            = jan,
  pages            = {107646},
  volume           = {235},
  abstract         = {The advances of intelligent fault diagnosis in recent years show that deep learning has strong capability of automatic feature extraction and accurate identification for fault signals. Nevertheless, data scarcity and varying working conditions can degrade the performance of the model. More recently, a tool has been proposed to address the above challenges simultaneously. Meta-learning, also known as learning to learn, uses a small sample to quickly adapt to a new task. It has great application potential in few-shot and cross-domain fault diagnosis, and thus has become a promising tool. However, there is a lack of a survey to conclude existing work and look into the future. This paper comprehensively investigates deep meta-learning in fault diagnosis from three views: (i) what to use, (ii) how to use, and (iii) how to develop, i.e. algorithms, applications, and prospects. Algorithms are illustrated by optimization-, metric-, and model-based methods, the applications are concluded in few-shot cross-domain fault diagnosis, and open challenges, as well as prospects, are given to motivate the future work. Additionally, we demonstrate the performance of three approaches on two few-shot cross-domain tasks. Typical meta-learning methods are implemented and available at https://github.com/fyancy/MetaFD.},
  creationdate     = {2022-06-29T09:18:07},
  doi              = {10.1016/j.knosys.2021.107646},
  groups           = {knosys},
  modificationdate = {2023-04-04T16:06:00},
  publisher        = {Elsevier {BV}},
}

@Article{Liu2022MetaStock,
  author           = {Tengteng Liu and Xiang Ma and Shuo Li and Xuemei Li and Caiming Zhang},
  journal          = {Knowledge-Based Systems},
  title            = {A stock price prediction method based on meta-learning and variational mode decomposition},
  year             = {2022},
  month            = jun,
  pages            = {109324},
  abstract         = {Stock price prediction is an important and challenging research topic, which has wide application prospects. Correct forecasting results can provide valuable guidance to investors and thus reduce the investment risk. To improve the prediction accuracy and obtain better prediction results, a new stock price prediction model called VML is proposed in this paper. First, the VML model slices the stock price series to obtain multiple window series, then uses variational mode decomposition (VMD) to decompose the window series to obtain multiple subseries. Unlike existing decomposition-based methods, VML decomposes the window series to solve the data leakage problem. Next, model-agnostic meta-learning (MAML) algorithm and long short-term memory (LSTM) network are applied to predict the subseries. A method of dividing the decomposed subseries into multiple tasks is proposed for the purpose of utilizing the MAML algorithm to train the initial parameters of the LSTM with good generalization ability. The initial parameters enable LSTM to fine tune dynamically to fit the latest data distribution of stock price data, which mitigates the impact of concept drift on prediction accuracy. Finally, the VML model merges the prediction results of the subseries to obtain the final predicted stock price. Experimental results on stock datasets of the Chinese Stock Market and the American Stock Market demonstrate that the proposed method improves the accuracy of prediction.},
  creationdate     = {2022-06-29T09:19:32},
  doi              = {10.1016/j.knosys.2022.109324},
  groups           = {knosys},
  modificationdate = {2023-04-04T16:06:03},
  publisher        = {Elsevier {BV}},
}

@Article{Heuillet2021ExplainabilityRL,
  author           = {Alexandre Heuillet and Fabien Couthouis and Natalia Díaz-Rodríguez},
  journal          = {Knowledge-Based Systems},
  title            = {Explainability in deep reinforcement learning},
  year             = {2021},
  issn             = {0950-7051},
  pages            = {106685},
  volume           = {214},
  abstract         = {A large set of the explainable Artificial Intelligence (XAI) literature is emerging on feature relevance techniques to explain a deep neural network (DNN) output or explaining models that ingest image source data. However, assessing how XAI techniques can help understand models beyond classification tasks, e.g. for reinforcement learning (RL), has not been extensively studied. We review recent works in the direction to attain Explainable Reinforcement Learning (XRL), a relatively new subfield of Explainable Artificial Intelligence, intended to be used in general public applications, with diverse audiences, requiring ethical, responsible and trustable algorithms. In critical situations where it is essential to justify and explain the agent’s behaviour, better explainability and interpretability of RL models could help gain scientific insight on the inner workings of what is still considered a black box. We evaluate mainly studies directly linking explainability to RL, and split these into two categories according to the way the explanations are generated: transparent algorithms and post-hoc explainability. We also review the most prominent XAI works from the lenses of how they could potentially enlighten the further deployment of the latest advances in RL, in the demanding present and future of everyday problems.},
  creationdate     = {2022-06-29T20:02:12},
  doi              = {10.1016/j.knosys.2020.106685},
  file             = {:C\:/Users/hccz95/Downloads/1-s2.0-S0950705120308145-main.pdf:PDF},
  keywords         = {Reinforcement Learning, Explainable artificial intelligence, Machine Learning, Deep Learning, Responsible artificial intelligence, Representation learning},
  modificationdate = {2022-06-29T20:32:51},
  url              = {https://www.sciencedirect.com/science/article/pii/S0950705120308145},
}

@InProceedings{Raghu2020Rapid,
  author           = {Aniruddh Raghu and Maithra Raghu and Samy Bengio and Oriol Vinyals},
  booktitle        = {International Conference on Learning Representations},
  title            = {Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML},
  year             = {2020},
  creationdate     = {2022-06-29T20:33:46},
  file             = {:Raghu_2020_Rapid Learning or Feature Reuse_ Towards Understanding the Effectiveness of MAML.pdf:PDF;:Raghu_2020_Rapid Learning or Feature Reuse_ Towards Understanding the Effectiveness of MAML (slides).pdf:PDF;:Raghu_2020_Rapid Learning or Feature Reuse_ Towards Understanding the Effectiveness of MAML (poster).pdf:PDF;:C\:/Users/hccz95/Downloads/RapidLearningFeatureReuse.pdf:PDF},
  groups           = {Meta Learning},
  modificationdate = {2022-06-29T20:46:37},
  url              = {https://openreview.net/forum?id=rkgMkCEtPB},
}

@PhdThesis{Sarmasi2022Hanabi,
  author           = {Sarmasi, Aron},
  school           = {University of California, Davis},
  title            = {Meta-Learning Action Conventions in Ad-Hoc Hanabi},
  year             = {2022},
  abstract         = {The purpose of this thesis is to investigate how we can learn action conventions by observation in an ad-hoc context. We argue that the game Hanabi in particular is a promising application for studying this topic because it distills the problem to its core components, while presenting a small computational overhead. To facilitate research in this direction, we have compiled the Hanabi Open Agent Dataset (HOAD), consisting of neural replicas of the majority of contemporary Hanabi agents developed prior to this work. We first validate that HOAD is appropriate to use in meta-learning studies by demonstrating that HOAD agents use diverse, high quality strategies, and then we show that the popular meta-learning algorithm MAML can be used to train an ad-hoc learner that performs superior to random and naive baselines. Finally, we corroborate recent findings that MAML doesn't benefit from its inner learning loop after a sufficient number of training epochs.},
  creationdate     = {2022-06-29T21:14:53},
  file             = {:Sarmasi_2022_Meta Learning Action Conventions in Ad Hoc Hanabi.pdf:PDF},
  groups           = {Meta Learning, Ad Hoc Teamwork},
  modificationdate = {2022-10-18T22:51:24},
  url              = {https://www.proquest.com/openview/bc75689210506c9d64fc28ba48f13e36/1?pq-origsite=gscholar&cbl=18750&diss=y},
}

@InProceedings{Siu2021Evaluation,
  author           = {Siu, Ho Chit and Pe\~{n}a, Jaime and Chen, Edenna and Zhou, Yutai and Lopez, Victor and Palko, Kyle and Chang, Kimberlee and Allen, Ross},
  booktitle        = {Advances in Neural Information Processing Systems},
  title            = {Evaluation of Human-AI Teams for Learned and Rule-Based Agents in Hanabi},
  year             = {2021},
  editor           = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages            = {16183--16195},
  publisher        = {Curran Associates, Inc.},
  volume           = {34},
  abstract         = {Deep reinforcement learning has generated superhuman AI in competitive games such as Go and StarCraft. Can similar learning techniques create a superior AI teammate for human-machine collaborative games? Will humans prefer AI teammates that improve objective team performance or those that improve subjective metrics of trust? In this study, we perform a single-blind evaluation of teams of humans and AI agents in the cooperative card game Hanabi, with both rule-based and learning-based agents. In addition to the game score, used as an objective metric of the human-AI team performance, we also quantify subjective measures of the human's perceived performance, teamwork, interpretability, trust, and overall preference of AI teammate. We find that humans have a clear preference toward a rule-based AI teammate (SmartBot) over a state-of-the-art learning-based AI teammate (Other-Play) across nearly all subjective metrics, and generally view the learning-based agent negatively, despite no statistical difference in the game score. This result has implications for future AI design and reinforcement learning benchmarking, highlighting the need to incorporate subjective metrics of human-AI teaming rather than a singular focus on objective task performance.},
  creationdate     = {2022-06-29T21:17:52},
  file             = {:Siu_2021_Evaluation of Human AI Teams for Learned and Rule Based Agents in Hanabi.pdf:PDF},
  groups           = {Teamwork},
  modificationdate = {2022-06-29T21:18:55},
}

@Misc{Zand2022Adaptation,
  author           = {Jaleh Zand and Jack Parker-Holder and Stephen J. Roberts},
  month            = mar,
  title            = {On-the-fly Strategy Adaptation for Ad-Hoc Agent Coordination},
  year             = {2022},
  abstract         = {Training agents in cooperative settings offers the promise of AI agents able to interact effectively with humans (and other agents) in the real world. Multi-agent reinforcement learning (MARL) has the potential to achieve this goal, demonstrating success in a series of challenging problems. However, whilst these advances are significant, the vast majority of focus has been on the self-play paradigm. This often results in a coordination problem, caused by agents learning to make use of arbitrary conventions when playing with themselves. This means that even the strongest self-play agents may have very low cross-play with other agents, including other initializations of the same algorithm. In this paper we propose to solve this problem by adapting agent strategies on the fly, using a posterior belief over the other agents' strategy. Concretely, we consider the problem of selecting a strategy from a finite set of previously trained agents, to play with an unknown partner. We propose an extension of the classic statistical technique, Gibbs sampling, to update beliefs about other agents and obtain close to optimal ad-hoc performance. Despite its simplicity, our method is able to achieve strong cross-play with unseen partners in the challenging card game of Hanabi, achieving successful ad-hoc coordination without knowledge of the partner's strategy a priori.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-06-29T21:23:14},
  eprint           = {2203.08015},
  file             = {:Zand_2022_On the Fly Strategy Adaptation for Ad Hoc Agent Coordination.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  keywords         = {cs.LG, cs.AI, cs.GT, stat.ML},
  modificationdate = {2022-10-26T10:52:34},
  primaryclass     = {cs.LG},
}

@Misc{Lockhart2020BridgeBidding,
  author           = {Edward Lockhart and Neil Burch and Nolan Bard and Sebastian Borgeaud and Tom Eccles and Lucas Smaira and Ray Smith},
  month            = nov,
  title            = {Human-Agent Cooperation in Bridge Bidding},
  year             = {2020},
  abstract         = {We introduce a human-compatible reinforcement-learning approach to a cooperative game, making use of a third-party hand-coded human-compatible bot to generate initial training data and to perform initial evaluation. Our learning approach consists of imitation learning, search, and policy iteration. Our trained agents achieve a new state-of-the-art for bridge bidding in three settings: an agent playing in partnership with a copy of itself; an agent partnering a pre-existing bot; and an agent partnering a human player.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-06-29T21:24:16},
  eprint           = {2011.14124},
  file             = {:http\://arxiv.org/pdf/2011.14124v1:PDF},
  groups           = {Human-Agent-Robot},
  keywords         = {cs.AI},
  modificationdate = {2022-06-29T21:24:53},
  primaryclass     = {cs.AI},
}

@Misc{Kantack2021HumanDecision,
  author           = {Nicholas Kantack},
  month            = nov,
  title            = {Reinforcement Learning on Human Decision Models for Uniquely Collaborative AI Teammates},
  year             = {2021},
  abstract         = {In 2021 the Johns Hopkins University Applied Physics Laboratory held an internal challenge to develop artificially intelligent (AI) agents that could excel at the collaborative card game Hanabi. Agents were evaluated on their ability to play with human players whom the agents had never previously encountered. This study details the development of the agent that won the challenge by achieving a human-play average score of 16.5, outperforming the current state-of-the-art for human-bot Hanabi scores. The winning agent's development consisted of observing and accurately modeling the author's decision making in Hanabi, then training with a behavioral clone of the author. Notably, the agent discovered a human-complementary play style by first mimicking human decision making, then exploring variations to the human-like strategy that led to higher simulated human-bot scores. This work examines in detail the design and implementation of this human compatible Hanabi teammate, as well as the existence and implications of human-complementary strategies and how they may be explored for more successful applications of AI in human machine teams.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-06-29T21:25:47},
  eprint           = {2111.09800},
  file             = {:http\://arxiv.org/pdf/2111.09800v1:PDF},
  groups           = {Human-Agent-Robot},
  keywords         = {cs.AI, cs.LG, I.2.6},
  modificationdate = {2022-06-29T21:26:14},
  primaryclass     = {cs.AI},
}

@InProceedings{Zeng2020AdaptiveRouting,
  author           = {Zeng, Siliang and Xu, Xingfei and Chen, Yi},
  booktitle        = {2020 IEEE 16th International Conference on Control \& Automation (ICCA)},
  title            = {Multi-Agent Reinforcement Learning for Adaptive Routing: A Hybrid Method using Eligibility Traces},
  year             = {2020},
  pages            = {1332-1339},
  creationdate     = {2022-06-29T22:16:05},
  doi              = {10.1109/ICCA51439.2020.9264518},
  file             = {:Zeng_2020_Multi Agent Reinforcement Learning for Adaptive Routing_ a Hybrid Method Using Eligibility Traces.pdf:PDF},
  groups           = {MARL},
  modificationdate = {2022-06-29T22:17:59},
}

@Article{Shou2022RoutingGame,
  author           = {Zhenyu Shou and Xu Chen and Yongjie Fu and Xuan Di},
  journal          = {Transportation Research Part C: Emerging Technologies},
  title            = {Multi-agent reinforcement learning for Markov routing games: A new modeling paradigm for dynamic traffic assignment},
  year             = {2022},
  month            = apr,
  pages            = {103560},
  volume           = {137},
  creationdate     = {2022-06-29T22:19:37},
  doi              = {10.1016/j.trc.2022.103560},
  file             = {:Shou_2022_Multi Agent Reinforcement Learning for Markov Routing Games_ a New Modeling Paradigm for Dynamic Traffic Assignment.pdf:PDF},
  groups           = {MARL},
  modificationdate = {2023-04-04T16:06:06},
  publisher        = {Elsevier {BV}},
}

@InProceedings{Cui2021Klevel,
  author           = {Cui, Brandon and Hu, Hengyuan and Pineda, Luis and Foerster, Jakob},
  booktitle        = {Advances in Neural Information Processing Systems},
  title            = {K-level Reasoning for Zero-Shot Coordination in Hanabi},
  year             = {2021},
  editor           = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages            = {8215--8228},
  publisher        = {Curran Associates, Inc.},
  volume           = {34},
  abstract         = {The standard problem setting in cooperative multi-agent settings is \emph{self-play} (SP), where the goal is to train a \emph{team} of agents that works well together.     However, optimal SP policies commonly contain arbitrary conventions  (``handshakes'') and are not compatible with other, independently trained agents or humans.     This latter desiderata was recently formalized by \cite{Hu2020-OtherPlay} as the \emph{zero-shot coordination} (ZSC) setting and partially addressed with their \emph{Other-Play} (OP) algorithm, which showed improved ZSC and human-AI performance in the card game Hanabi.     OP assumes access to the symmetries of the environment and prevents agents from breaking these in a mutually \emph{incompatible} way during training. However, as the authors point out, discovering symmetries for a given environment is a computationally hard problem.    Instead, we show that through a simple adaption of k-level reasoning (KLR) \cite{Costa-Gomes2006-K-level}, synchronously training all levels, we can obtain competitive ZSC and ad-hoc teamplay performance in Hanabi, including when paired with a human-like proxy bot. We also introduce a new method, synchronous-k-level reasoning with a best response (SyKLRBR), which further improves performance on our synchronous KLR by co-training a best response.},
  creationdate     = {2022-06-29T22:20:42},
  file             = {:Cui_2021_K Level Reasoning for Zero Shot Coordination in Hanabi.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  modificationdate = {2022-06-29T22:21:15},
}

@MastersThesis{Palmersten2020Hanabi,
  author           = {Joseph Palmersten},
  title            = {Approaching Hanabi with Q-Learning and Evolutionary Algorithm},
  year             = {2020},
  abstract         = {Hanabi is a cooperative card game with hidden information that requires cooperation and communication between the players. For a machine learning agent to be successful at the Hanabi, it will have to learn how to communicate and infer information from the communication of other players. To approach the problem of Hanabi the machine learning methods of Q-learning and Evolutionary algorithm are proposed as potential solutions. The agents that were created using the method are shown to not achieve human levels of communication.},
  creationdate     = {2022-06-29T22:23:15},
  file             = {:Palmersten_2020_Approaching Hanabi with Q Learning and Evolutionary Algorithm.pdf:PDF},
  groups           = {Teamwork},
  keywords         = {Artificial Intelligence, Evolutionary Algorithm, Gameplay, Machine Learning, Neural Networks, Q-Learning},
  modificationdate = {2022-06-29T22:27:40},
}

@MastersThesis{Soisson2021HanabiRainbow,
  author           = {Vanessa Soisson},
  school           = {Technische Universität Berlin},
  title            = {Applying the Rainbow Techniques on the Game of Hanabi},
  year             = {2021},
  abstract         = {Humans operate in a cooperative, social environment. They learn through interaction, by sharing information, experience and learned knowledge, and they apply Theory of Mind. That is the capability of a person to reason about the thoughts, beliefs and feelings of other people to predict their behavior and intentions. The game Hanabi is a cooperative environment on a small scale with predefned rules of behavior and communication. As such it represents an interesting playground for multi-agent Reinforcement Learning. Among other types, the Rainbow agent has been evaluated with regards to self-play and ad-hoc performance. Rainbow is a variant of the universally applicable Deep-Q-Learning algorithm. This thesis deepens the analysis of the Rainbow architecture for Hanabi and estimates the contribution of its individual components to the self-play performance. Further, it shows that the Rainbow agent is able to learn a variety of game strategies. Thus, the Rainbow algorithm can be used to generate a pool of agents necessary to continue the study on ad-hoc game play.},
  creationdate     = {2022-06-29T22:26:49},
  file             = {:Soisson_2021_Applying the Rainbow Techniques on the Game of Hanabi.pdf:PDF},
  groups           = {Teamwork},
  modificationdate = {2022-06-29T22:30:13},
}

@InProceedings{Sarmasi2021HanabiDataset,
  author           = {Sarmasi, Aron and Zhang, Timothy and Cheng, Chu-Hung and Pham, Huyen and Zhou, Xuanchen and Nguyen, Duong and Shekdar, Soumil and McCoy, Joshua},
  booktitle        = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  title            = {HOAD: The Hanabi Open Agent Dataset},
  year             = {2021},
  address          = {Richland, SC},
  pages            = {1646--1648},
  publisher        = {International Foundation for Autonomous Agents and Multiagent Systems},
  series           = {AAMAS '21},
  abstract         = {In this work we present the Hanabi Open Agent Dataset (HOAD)- meant to address the current lack of Hanabi datasets, HOAD is an easily extensible, open-sourced, and comprehensive collection of existing Hanabi playing agents, all ported to the Hanabi Learning Environment (HLE). We give a description and analysis of each agent's strategy, and we also show cross-play performance between all the agents, demonstrating both their high quality and diversity of strategy. These properties make HOAD especially well suited to studies involving meta-learning and transfer learning. Finally, we describe in detail an easy way to add new agents to HOAD regardless of the origin codebase of the agent and make our code and dataset publicly available at https://github.com/aronsar/hoad.},
  creationdate     = {2022-06-29T22:38:09},
  file             = {:Sarmasi_2021_HOAD_ the Hanabi Open Agent Dataset.pdf:PDF},
  groups           = {Teamwork},
  isbn             = {9781450383073},
  keywords         = {Hanabi, dataset},
  location         = {Virtual Event, United Kingdom},
  modificationdate = {2022-06-29T22:41:55},
  numpages         = {3},
}

@InProceedings{Gu2022OnlineAdhoc,
  author           = {Pengjie Gu and Mengchen Zhao and Jianye Hao and Bo An},
  booktitle        = {International Conference on Learning Representations},
  title            = {Online Ad Hoc Teamwork under Partial Observability},
  year             = {2022},
  abstract         = {Autonomous agents often need to work together as a team to accomplish complex cooperative tasks. Due to privacy and other realistic constraints, agents might need to collaborate with previously unknown teammates on the fly. This problem is known as ad hoc teamwork, which remains a core research challenge. Prior works usually rely heavily on strong assumptions like full observability, fixed and predefined teammates' types. This paper relaxes these assumptions with a novel reinforcement learning framework called ODITS, which allows the autonomous agent to adapt to arbitrary teammates in an online fashion. Instead of limiting teammates into a finite set of predefined types, ODITS automatically learns latent variables of teammates' behaviors to infer how to cooperate with new teammates effectively. To overcome partial observability, we introduce an information-based regularizer to derive proxy representations of the learned variables from local observations. Extensive experimental results show that ODITS significantly outperforms various baselines in widely used ad hoc teamwork tasks.},
  creationdate     = {2022-06-29T22:45:08},
  file             = {:Gu_2022_Online Ad Hoc Teamwork under Partial Observability.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  keywords         = {coordination, reinforcement learning},
  modificationdate = {2022-06-29T22:50:03},
  url              = {https://openreview.net/forum?id=18Ys0-PzyPI},
}

@Article{Yang2010Adhoc,
  author           = {杨志蓉 and 谢章澍 and 宝贡敏},
  journal          = {福州大学学报},
  title            = {团队快速信任 、互动行为对团队创造力的作用机理研究},
  year             = {2010},
  number           = {6},
  abstract         = {通过临时团队进行创造性合作以完成某项特定任务或解决某个特殊问题,在现代组织中越来越普遍。临时团队要尽快运作起来,就必须建立团队快速信伍。团队快速信任通过争辩行为、帮助行为和自发行为的中介传导促进团队创造力的实现和提升。},
  creationdate     = {2022-06-29T22:50:36},
  file             = {:杨志蓉_2010_团队快速信任 、互动行为对团队创造力的作用机理研究.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  modificationdate = {2022-06-29T22:52:49},
}

@Article{Chen2022Adhoc,
  author           = {Hao Chen and YANG, Likun and YIN, Qiyue and HUANG, Kaiqi},
  journal          = {Journal of University of Chinese Academy of Sciences},
  title            = {Local Observation Reconstruction for Ad-Hoc Cooperation},
  year             = {2022},
  abstract         = {In recent years, multi-agent reinforcement learning has received a lot of attention from researchers. In the study of multi-agent reinforcement learning, the question of how to perform ad-hoc cooperation, i.e., how to adapt to a changing variety and number of teammates, is a key problem. Existing methods either have strong prior knowledge assumptions or use hard-coded protocols for cooperation, which lack generality and can not be generalized to more general ad-hoc cooperation scenarios. To address this problem, this paper proposes a local observation reconstruction algorithm for ad-hoc cooperation, which uses attention mechanisms and sampling networks to reconstruct local observations, enabling the algorithm to recognize and make full use of high-dimensional state representations in different situations and achieve zero-shot generalization in ad-hoc cooperation scenarios. In this paper, the performance of the algorithm is compared and analyzed with representative algorithms on the StarCraft micromanagement environment and ad-hoc cooperation scenarios to verify the effectiveness of the algorithm.
近年来,多智能体强化学习得到了研究人员们的广泛关注。在多智能体强化学习的研究中,如何进行Ad-Hoc协作,也就是说如何适应种类和数量变化的队友,是一个关键问题。现有方法或者有很强的先验知识假设,或者使用硬编码的规则来进行合作,缺乏通用性,无法泛化到更一般的Ad-Hoc协作场景。为解决该问题,本文提出了一种面向Ad-Hoc协作的局部观测重建算法,利用注意力机制和采样网络对局部观测进行重建,使得算法认识到并充分利用不同局面中的高维状态表征,实现了在Ad-Hoc协作场景下的零样本泛化。本文在星际争霸微操环境和Ad-Hoc协作场景上与代表性算法的性能进行对比与分析,验证了算法的有效性。},
  creationdate     = {2022-07-07T19:29:49},
  doi              = {10.7523/j.ucas.2022.028},
  file             = {:Chen_2022_面向Ad-Hoc协作的局部观测重建方法.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  keywords         = {Multi-Agent, Deep Reinforcement Learning, Credit Assignment, Ad-Hoc Cooperation},
  modificationdate = {2022-07-07T19:32:25},
}

@Misc{Neves2022Learning,
  author           = {Alexandre Neves and Alberto Sardinha},
  month            = may,
  title            = {Learning to Cooperate with Completely Unknown Teammates},
  year             = {2022},
  abstract         = {A key goal of ad hoc teamwork is to develop a learning agent that cooperates with unknown teams, without resorting to any pre-coordination protocol. Despite a vast number of ad hoc teamwork algorithms in the literature, most of them cannot address the problem of learning to cooperate with a completely unknown team, unless it learns from scratch. This article presents a novel approach that uses transfer learning alongside the state-of-the-art PLASTIC-Policy to adapt to completely unknown teammates quickly. We test our solution within the Half Field Offense simulator with five different teammates. The teammates were designed independently by developers from different countries and at different times. Our empirical evaluation shows that it is advantageous for an ad hoc agent to leverage its past knowledge when adapting to a new team instead of learning how to cooperate with it from scratch.},
  archiveprefix    = {arXiv},
  comment          = {PLASTIC-Policy + Transfer Learning
貌似创新性很弱，而且结果提升很少，但是可以参考其论述},
  creationdate     = {2022-07-07T19:35:06},
  eprint           = {2205.03289},
  file             = {:Neves_2022_Learning to Cooperate with Completely Unknown Teammates.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  keywords         = {cs.MA, I.2.6},
  modificationdate = {2022-10-29T09:22:19},
  primaryclass     = {cs.MA},
}

@Misc{Ribeiro2021HOTSPOT,
  author           = {Jo{\~{a}}o G. Ribeiro and Luis Müller Henriques and S{\'{e}}rgio Colcher and Julio Cesar Duarte and Francisco S. Melo and Ruy Luiz Milidi{\'{u}} and Alberto Sardinha},
  month            = nov,
  title            = {{HOTSPOT}: An Ad Hoc Teamwork Platform for Mixed Human-Robot Teams},
  year             = {2021},
  abstract         = {Ad hoc teamwork is a research topic in multi-agent systems whereby an agent (the "ad hoc agent") must successfully collaborate with a set of unknown agents (the "teammates") without any prior coordination or communication protocol. However, research in ad hoc teamwork is predominantly focused on agent-only teams, but not in agent-human teams, which we believe is an exciting research avenue and has enormous application potential in human-robot teams. This paper will tap into this potential by proposing HOTSPOT, the first framework for ad hoc teamwork in human-robot teams. Our framework comprises two main modules, addressing the two key challenges in the interaction between a robot acting as the ad hoc agent and human teammates. First, a decision-theoretic module that is responsible for all task-related decision making (task identification, teammate identification, and planning). Second, a communication module that uses natural language processing in order to parse all communication between the robot and the human. To evaluate our framework, we use a task where a mobile robot and a human cooperatively collect objects in an open space, illustrating the main features of our framework in a real-world task.},
  comment          = {第一个human-agent ad hoc teamwork框架},
  creationdate     = {2022-07-07T20:01:18},
  doi              = {10.36227/techrxiv.17026013.v1},
  file             = {:Ribeiro_2021_HOTSPOT_ an Ad Hoc Teamwork Platform for Mixed Human Robot Teams.pdf:PDF},
  groups           = {Human-Agent Ad Hoc},
  keywords         = {Ad Hoc Teamwork; Multi-Agent Systems; Human-Robot Interaction; Natural Language Processing},
  modificationdate = {2022-09-03T10:14:38},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Misc{Chen2022RACA,
  author           = {Hao Chen and Guangkai Yang and Junge Zhang and Qiyue Yin and Kaiqi Huang},
  month            = jun,
  title            = {RACA: Relation-Aware Credit Assignment for Ad-Hoc Cooperation in Multi-Agent Deep Reinforcement Learning},
  year             = {2022},
  abstract         = {In recent years, reinforcement learning has faced several challenges in the multi-agent domain, such as the credit assignment issue. Value function factorization emerges as a promising way to handle the credit assignment issue under the centralized training with decentralized execution (CTDE) paradigm. However, existing value function factorization methods cannot deal with ad-hoc cooperation, that is, adapting to new configurations of teammates at test time. Specifically, these methods do not explicitly utilize the relationship between agents and cannot adapt to different sizes of inputs. To address these limitations, we propose a novel method, called Relation-Aware Credit Assignment (RACA), which achieves zero-shot generalization in ad-hoc cooperation scenarios. RACA takes advantage of a graph-based relation encoder to encode the topological structure between agents. Furthermore, RACA utilizes an attention-based observation abstraction mechanism that can generalize to an arbitrary number of teammates with a fixed number of parameters. Experiments demonstrate that our method outperforms baseline methods on the StarCraftII micromanagement benchmark and ad-hoc cooperation scenarios.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-07-08T09:10:17},
  eprint           = {2206.01207},
  file             = {:Chen_2022_RACA_ Relation Aware Credit Assignment for Ad Hoc Cooperation in Multi Agent Deep Reinforcement Learning.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  keywords         = {cs.LG, cs.AI},
  modificationdate = {2022-07-08T09:15:32},
  primaryclass     = {cs.LG},
}

@Misc{Janaina2022Smart,
  author           = {Moreira-Kanaley Janaína},
  title            = {Smart Team Play: Utility of Population-Based Training for Cooperative AI in Overcooked},
  year             = {2022},
  abstract         = {In an ad-hoc teamwork environment, artificial intelligence agents have the potential to take on supportive roles and complete tasks in collaboration with human players. The following paper investigates the use of employing population-based training (PBT) for reinforcement learning agents in the multi-player game Overcooked. In addition to this, the research examines whether the incorporation of highly mutated agents, which serve to introduce noise into the initial population, could enhance the final performance of PBT. As the method used to answer the previous inquiries, the learning curve of a selected PBT agent is first evaluated and its final performance with a human proxy then examined within different layouts of the game. Following this method, it was concluded that PBT, and other self-play agents, have the tendency to drastically underperform against the human proxy and agents that are trained based on human data. Furthermore, while incorporating the mutated agents increased sample efficiency in layouts with low risk of collisions, it had negligible effect on the final performance of PBT with the human proxy.},
  creationdate     = {2022-07-08T09:30:18},
  file             = {:Janaína_2022_Smart Team Play_ Utility of Population Based Training for Cooperative AI in Overcooked.pdf:PDF},
  groups           = {Human-Agent Ad Hoc},
  keywords         = {Interactive Intelligence, Artifical Intelligence, Collaborative AI, Teamwork, Games, Cooperation},
  modificationdate = {2022-09-02T11:27:03},
  url              = {http://resolver.tudelft.nl/uuid:396f42bd-b50e-47c5-8f41-2b2e6f9b7ab0},
}

@Misc{Nathan2022Fictional,
  author           = {Ordonez Cardenas Nathan},
  title            = {Fictional Co-Play for Human-Agent Collaboration: Evaluating state-of-the-art reinforcement learning technique for adaptability to human collaborators},
  year             = {2022},
  abstract         = {A longstanding problem in the area of reinforcement learning is human-agent col- laboration. As past research indicates that RL agents undergo a distributional shift when they start collaborating with human beings, the goal is to create agents that can adapt. We build upon research using the two-player Overcooked environment to repro- duce a simplified version of the Fictitious Co-Play algorithm in order to confirm past found improvements at a smaller scale of training and using Self-Play and Population- based trained algorithms as the baselines for comparison. We find that the agent on average slightly outperforms both baseline algorithms when evaluated using a human proxy. We also find high cross-seed variance in performance, indicating the potential for further hyperparameter tuning.},
  creationdate     = {2022-07-08T09:32:48},
  file             = {:Nathan_2022_Fictional Co Play for Human Agent Collaboration_ Evaluating State of the Art Reinforcement Learning Technique for Adaptability to Human Collaborators.pdf:PDF},
  groups           = {Human-Agent Ad Hoc},
  keywords         = {Reinforcement Learning, Ad-hoc teamwork, human-ai collaboration, Human-Agent Teamwork, Overcooked AI},
  modificationdate = {2022-09-03T01:47:38},
  url              = {https://repository.tudelft.nl/islandora/object/uuid:ca39cc40-049a-42ce-ba6f-003e5c358351?collection=education},
}

@Misc{Ma2022Action,
  author           = {Mingwei Ma and Jizhou Liu and Samuel Sokota and Max Kleiman-Weiner and Jakob Foerster},
  month            = jan,
  title            = {Learning to Coordinate with Humans using Action Features},
  year             = {2022},
  abstract         = {An unaddressed challenge in human-AI coordination is to enable AI agents to exploit the semantic relationships between the features of actions and the features of observations. Humans take advantage of these relationships in highly intuitive ways. For instance, in the absence of a shared language, we might point to the object we desire or hold up our fingers to indicate how many objects we want. To address this challenge, we investigate the effect of network architecture on the propensity of learning algorithms to exploit these semantic relationships. Across a procedurally generated coordination task, we find that attention-based architectures that jointly process a featurized representation of observations and actions have a better inductive bias for zero-shot coordination. Through fine-grained evaluation and scenario analysis, we show that the resulting policies are human-interpretable. Moreover, such agents coordinate with people without training on any human data.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-07-08T09:34:52},
  eprint           = {2201.12658},
  file             = {:http\://arxiv.org/pdf/2201.12658v1:PDF},
  groups           = {Human-Agent-Robot},
  keywords         = {cs.LG, cs.AI, cs.MA},
  modificationdate = {2023-03-02T11:02:20},
  primaryclass     = {cs.LG},
}

@Misc{Lucas2022Anyplay,
  author           = {Keane Lucas and Ross E. Allen},
  month            = jan,
  title            = {Any-Play: An Intrinsic Augmentation for Zero-Shot Coordination},
  year             = {2022},
  abstract         = {Cooperative artificial intelligence with human or superhuman proficiency in collaborative tasks stands at the frontier of machine learning research. Prior work has tended to evaluate cooperative AI performance under the restrictive paradigms of self-play (teams composed of agents trained together) and cross-play (teams of agents trained independently but using the same algorithm). Recent work has indicated that AI optimized for these narrow settings may make for undesirable collaborators in the real-world. We formalize an alternative criteria for evaluating cooperative AI, referred to as inter-algorithm cross-play, where agents are evaluated on teaming performance with all other agents within an experiment pool with no assumption of algorithmic similarities between agents. We show that existing state-of-the-art cooperative AI algorithms, such as Other-Play and Off-Belief Learning, under-perform in this paradigm. We propose the Any-Play learning augmentation -- a multi-agent extension of diversity-based intrinsic rewards for zero-shot coordination (ZSC) -- for generalizing self-play-based algorithms to the inter-algorithm cross-play setting. We apply the Any-Play learning augmentation to the Simplified Action Decoder (SAD) and demonstrate state-of-the-art performance in the collaborative card game Hanabi.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-07-08T09:36:09},
  eprint           = {2201.12436},
  file             = {:Lucas_2022_Any Play_ an Intrinsic Augmentation for Zero Shot Coordination.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  keywords         = {cs.AI, cs.LG, cs.MA, I.2.11},
  modificationdate = {2022-12-08T16:55:47},
  primaryclass     = {cs.AI},
}

@Misc{Zhao2021Entropy,
  author           = {Rui Zhao and Jinming Song and Yufeng Yuan and Hu Haifeng and Yang Gao and Yi Wu and Zhongqian Sun and Yang Wei},
  month            = dec,
  title            = {Maximum Entropy Population-Based Training for Zero-Shot Human-AI Coordination},
  year             = {2021},
  abstract         = {We study the problem of training a Reinforcement Learning (RL) agent that is collaborative with humans without using any human data. Although such agents can be obtained through self-play training, they can suffer significantly from distributional shift when paired with unencountered partners, such as humans. To mitigate this distributional shift, we propose Maximum Entropy Population-based training (MEP). In MEP, agents in the population are trained with our derived Population Entropy bonus to promote both pairwise diversity between agents and individual diversity of agents themselves, and a common best agent is trained by paring with agents in this diversified population via prioritized sampling. The prioritization is dynamically adjusted based on the training progress. We demonstrate the effectiveness of our method MEP, with comparison to Self-Play PPO (SP), Population-Based Training (PBT), Trajectory Diversity (TrajeDi), and Fictitious Co-Play (FCP) in the Overcooked game environment, with partners being human proxy models and real humans. A supplementary video showing experimental results is available at https://youtu.be/Xh-FKD0AAKE.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-07-08T09:37:00},
  eprint           = {2112.11701},
  file             = {:http\://arxiv.org/pdf/2112.11701v3:PDF},
  groups           = {Human-Agent Ad Hoc},
  keywords         = {cs.AI},
  modificationdate = {2022-07-08T09:37:09},
  primaryclass     = {cs.AI},
}

@Misc{Treutlein2021Survey,
  author           = {Johannes Treutlein and Michael Dennis and Caspar Oesterheld and Jakob Foerster},
  month            = jun,
  title            = {A New Formalism, Method and Open Issues for Zero-Shot Coordination},
  year             = {2021},
  abstract         = {In many coordination problems, independently reasoning humans are able to discover mutually compatible policies. In contrast, independently trained self-play policies are often mutually incompatible. Zero-shot coordination (ZSC) has recently been proposed as a new frontier in multi-agent reinforcement learning to address this fundamental issue. Prior work approaches the ZSC problem by assuming players can agree on a shared learning algorithm but not on labels for actions and observations, and proposes other-play as an optimal solution. However, until now, this "label-free" problem has only been informally defined. We formalize this setting as the label-free coordination (LFC) problem by defining the label-free coordination game. We show that other-play is not an optimal solution to the LFC problem as it fails to consistently break ties between incompatible maximizers of the other-play objective. We introduce an extension of the algorithm, other-play with tie-breaking, and prove that it is optimal in the LFC problem and an equilibrium in the LFC game. Since arbitrary tie-breaking is precisely what the ZSC setting aims to prevent, we conclude that the LFC problem does not reflect the aims of ZSC. To address this, we introduce an alternative informal operationalization of ZSC as a starting point for future work.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-07-08T09:37:26},
  eprint           = {2106.06613},
  file             = {:http\://arxiv.org/pdf/2106.06613v2:PDF},
  groups           = {Ad Hoc Teamwork},
  keywords         = {cs.AI, cs.LG},
  modificationdate = {2022-07-08T09:37:45},
  primaryclass     = {cs.AI},
}

@Misc{Tejwani2021Social,
  author           = {Ravi Tejwani and Yen-Ling Kuo and Tianmin Shu and Bennett Stankovits and Dan Gutfreund and Joshua B. Tenenbaum and Boris Katz and Andrei Barbu},
  month            = oct,
  title            = {Incorporating Rich Social Interactions Into MDPs},
  year             = {2021},
  abstract         = {Much of what we do as humans is engage socially with other agents, a skill that robots must also eventually possess. We demonstrate that a rich theory of social interactions originating from microsociology and economics can be formalized by extending a nested MDP where agents reason about arbitrary functions of each other's hidden rewards. This extended Social MDP allows us to encode the five basic interactions that underlie microsociology: cooperation, conflict, coercion, competition, and exchange. The result is a robotic agent capable of executing social interactions zero-shot in new environments; like humans it can engage socially in novel ways even without a single example of that social interaction. Moreover, the judgments of these Social MDPs align closely with those of humans when considering which social interaction is taking place in an environment. This method both sheds light on the nature of social interactions, by providing concrete mathematical definitions, and brings rich social interactions into a mathematical framework that has proven to be natural for robotics, MDPs.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-07-08T09:39:14},
  eprint           = {2110.10298},
  file             = {:http\://arxiv.org/pdf/2110.10298v3:PDF},
  keywords         = {cs.RO},
  modificationdate = {2022-07-08T09:39:28},
  primaryclass     = {cs.RO},
}

@Misc{Balduzzi2019MAL,
  author           = {D Balduzzi},
  title            = {Tutorial: Multi-Agent Learning},
  year             = {2019},
  creationdate     = {2022-07-08T10:13:52},
  file             = {:Balduzzi_2019_Tutorial_ Multi Agent Learning.pdf:PDF},
  modificationdate = {2022-07-08T11:24:29},
  ranking          = {rank5},
}

@Misc{Qin2022MATransfer,
  author           = {Rongjun Qin and Feng Chen and Tonghan Wang and Lei Yuan and Xiaoran Wu and Zongzhang Zhang and Chongjie Zhang and Yang Yu},
  month            = mar,
  title            = {Multi-Agent Policy Transfer via Task Relationship Modeling},
  year             = {2022},
  abstract         = {Team adaptation to new cooperative tasks is a hallmark of human intelligence, which has yet to be fully realized in learning agents. Previous work on multi-agent transfer learning accommodate teams of different sizes, heavily relying on the generalization ability of neural networks for adapting to unseen tasks. We believe that the relationship among tasks provides the key information for policy adaptation. In this paper, we try to discover and exploit common structures among tasks for more efficient transfer, and propose to learn effect-based task representations as a common space of tasks, using an alternatively fixed training scheme. We demonstrate that the task representation can capture the relationship among tasks, and can generalize to unseen tasks. As a result, the proposed method can help transfer learned cooperation knowledge to new tasks after training on a few source tasks. We also find that fine-tuning the transferred policies help solve tasks that are hard to learn from scratch.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-07-14T00:40:03},
  eprint           = {2203.04482},
  file             = {:Qin_2022_Multi Agent Policy Transfer Via Task Relationship Modeling.pdf:PDF},
  groups           = {MARL},
  keywords         = {cs.AI, cs.LG},
  modificationdate = {2022-07-14T00:43:38},
  primaryclass     = {cs.AI},
  ranking          = {rank5},
}

@Article{Silva2021TransferMAS,
  author           = {Silva, Felipe Leno da and Costa, Anna Helena Reali},
  journal          = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  title            = {Transfer Learning for Multiagent Reinforcement Learning Systems},
  year             = {2021},
  number           = {3},
  pages            = {1--129},
  volume           = {15},
  abstract         = {Learning to solve sequential decision-making tasks is difficult. Humans take years exploring the environment essentially in a random way until they are able to reason, solve difficult tasks, and collaborate with other humans towards a common goal. Artificial Intelligent agents are like humans in this aspect. Reinforcement Learning (RL) is a well-known technique to train autonomous agents through interactions with the environment. Unfortunately, the learning process has a high sample complexity to infer an effective actuation policy, especially when multiple agents are simultaneously actuating in the environment.
However, previous knowledge can be leveraged to accelerate learning and enable solving harder tasks. In the same way humans build skills and reuse them by relating different tasks, RL agents might reuse knowledge from previously solved tasks and from the exchange of knowledge with other agents in the environment. In fact, virtually all of the most challenging tasks currently solved by RL rely on embedded knowledge reuse techniques, such as Imitation Learning, Learning from Demonstration, and Curriculum Learning.
This book surveys the literature on knowledge reuse in multiagent RL. The authors define a unifying taxonomy of state-of-the-art solutions for reusing knowledge, providing a comprehensive discussion of recent progress in the area. In this book, readers will find a comprehensive discussion of the many ways in which knowledge can be reused in multiagent sequential decision-making tasks, as well as in which scenarios each of the approaches is more efficient. The authors also provide their view of the current low-hanging fruit developments of the area, as well as the still-open big questions that could result in breakthrough developments. Finally, the book provides resources to researchers who intend to join this area or leverage those techniques, including a list of conferences, journals, and implementation tools.
This book will be useful for a wide audience; and will hopefully promote new dialogues across communities and novel developments in the area.},
  comment          = {参考 Multi-Agent Policy Transfer via Task Relationship Modeling},
  creationdate     = {2022-07-14T00:50:59},
  file             = {:Silva_2021_Transfer Learning for Multiagent Reinforcement Learning Systems.pdf:PDF},
  groups           = {MARL},
  modificationdate = {2022-07-14T00:52:44},
  publisher        = {Morgan \& Claypool Publishers},
  ranking          = {rank5},
}

@Misc{Ha2018World,
  author           = {David Ha and Jürgen Schmidhuber},
  month            = mar,
  title            = {World Models},
  year             = {2018},
  abstract         = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/},
  archiveprefix    = {arXiv},
  creationdate     = {2022-07-14T00:53:52},
  doi              = {10.5281/zenodo.1207631},
  eprint           = {1803.10122},
  file             = {:Ha_2018_World Models.pdf:PDF},
  keywords         = {cs.LG, stat.ML},
  modificationdate = {2022-10-29T09:27:42},
  primaryclass     = {cs.LG},
}

@Article{Moerland2022Unifying,
  author           = {Thomas M. Moerland and Joost Broekens and Aske Plaat and Catholijn M. Jonker},
  journal          = {Frontiers in Artificial Intelligence},
  title            = {A Unifying Framework for Reinforcement Learning and Planning},
  year             = {2022},
  month            = jul,
  volume           = {5},
  abstract         = {Sequential decision making, commonly formalized as optimization of a Markov Decision Process, is a key challenge in artificial intelligence. Two successful approaches to MDP optimization are reinforcement learning and planning, which both largely have their own research communities. However, if both research fields solve the same problem, then we might be able to disentangle the common factors in their solution approaches. Therefore, this paper presents a unifying algorithmic framework for reinforcement learning and planning (FRAP), which identifies underlying dimensions on which MDP planning and learning algorithms have to decide. At the end of the paper, we compare a variety of well-known planning, model-free and model-based RL algorithms along these dimensions. Altogether, the framework may help provide deeper insight in the algorithmic design space of planning and reinforcement learning.},
  creationdate     = {2022-07-14T22:52:25},
  doi              = {10.3389/frai.2022.908353},
  file             = {:Moerland_2022_A Unifying Framework for Reinforcement Learning and Planning.pdf:PDF},
  modificationdate = {2022-07-14T23:27:34},
  priority         = {prio1},
  publisher        = {Frontiers Media {SA}},
  ranking          = {rank5},
}

@InProceedings{Fox2016Taming,
  author           = {Fox, Roy and Pakman, Ari and Tishby, Naftali},
  booktitle        = {Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence},
  title            = {Taming the Noise in Reinforcement Learning via Soft Updates},
  year             = {2016},
  address          = {Arlington, Virginia, USA},
  pages            = {202--211},
  publisher        = {AUAI Press},
  series           = {UAI'16},
  abstract         = {Model-free reinforcement learning algorithms, such as Q-learning, perform poorly in the early stages of learning in noisy environments, because much effort is spent unlearning biased estimates of the state-action value function. The bias results from selecting, among several noisy estimates, the apparent optimum, which may actually be suboptimal. We propose G-learning, a new off-policy learning algorithm that regularizes the value estimates by penalizing deterministic policies in the beginning of the learning process. We show that this method reduces the bias of the value-function estimation, leading to faster convergence to the optimal value and the optimal policy. Moreover, G-learning enables the natural incorporation of prior domain knowledge, when available. The stochastic nature of G-learning also makes it avoid some exploration costs, a property usually attributed only to on-policy algorithms. We illustrate these ideas in several examples, where G-learning results in significant improvements of the convergence rate and the cost of the learning process.},
  comment          = {Max Entropy RL 最大熵强化学习},
  creationdate     = {2022-07-16T16:50:34},
  doi              = {10.5555/3020948.3020970},
  file             = {:Fox_2016_Taming the Noise in Reinforcement Learning Via Soft Updates.pdf:PDF},
  isbn             = {9780996643115},
  location         = {Jersey City, New Jersey, USA},
  modificationdate = {2022-07-16T19:57:38},
  numpages         = {10},
}

@Misc{Haarnoja2018SACv2,
  author           = {Tuomas Haarnoja and Aurick Zhou and Kristian Hartikainen and George Tucker and Sehoon Ha and Jie Tan and Vikash Kumar and Henry Zhu and Abhishek Gupta and Pieter Abbeel and Sergey Levine},
  month            = dec,
  title            = {Soft Actor-Critic Algorithms and Applications},
  year             = {2018},
  abstract         = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-07-17T17:06:20},
  eprint           = {1812.05905},
  file             = {:Haarnoja_2018_Soft Actor Critic Algorithms and Applications.pdf:PDF},
  keywords         = {cs.LG, cs.AI, cs.RO, stat.ML},
  modificationdate = {2022-07-17T23:41:30},
  primaryclass     = {cs.LG},
}

@Misc{Hausknecht2015DRQN,
  author           = {Matthew Hausknecht and Peter Stone},
  month            = jul,
  title            = {Deep Recurrent Q-Learning for Partially Observable MDPs},
  year             = {2015},
  abstract         = {Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting \textit{Deep Recurrent Q-Network} (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-07-19T16:40:17},
  eprint           = {1507.06527},
  file             = {:Hausknecht_2015_Deep Recurrent Q Learning for Partially Observable MDPs (DRQN).pdf:PDF},
  keywords         = {cs.LG},
  modificationdate = {2022-07-19T16:41:07},
  primaryclass     = {cs.LG},
}

@InProceedings{Jain2020GeneralizationAction,
  author           = {Jain, Ayush and Szot, Andrew and Lim, Joseph},
  booktitle        = {Proceedings of the 37th International Conference on Machine Learning},
  title            = {Generalization to New Actions in Reinforcement Learning},
  year             = {2020},
  editor           = {III, Hal Daumé and Singh, Aarti},
  month            = jul,
  pages            = {4661--4672},
  publisher        = {PMLR},
  series           = {Proceedings of Machine Learning Research},
  volume           = {119},
  abstract         = {A fundamental trait of intelligence is the ability to achieve goals in the face of novel circumstances, such as making decisions from new action choices. However, standard reinforcement learning assumes a fixed set of actions and requires expensive retraining when given a new action set. To make learning agents more adaptable, we introduce the problem of zero-shot generalization to new actions. We propose a two-stage framework where the agent first infers action representations from action information acquired separately from the task. A policy flexible to varying action sets is then trained with generalization objectives. We benchmark generalization on sequential tasks, such as selecting from an unseen tool-set to solve physical reasoning puzzles and stacking towers with novel 3D shapes. Videos and code are available at https://sites.google.com/view/action-generalization.},
  code             = {https://github.com/clvrai/new-actions-rl},
  creationdate     = {2022-07-24},
  file             = {:Jain_2020_Generalization to New Actions in Reinforcement Learning.pdf:PDF},
  modificationdate = {2022-07-24T10:38:09},
  url              = {https://proceedings.mlr.press/v119/jain20b.html},
}

@InProceedings{Oh2017Zero,
  author           = {Junhyuk Oh and Satinder Singh and Honglak Lee and Pushmeet Kohli},
  booktitle        = {Proceedings of the 34th International Conference on Machine Learning},
  title            = {Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning},
  year             = {2017},
  editor           = {Precup, Doina and Teh, Yee Whye},
  month            = aug,
  pages            = {2661--2670},
  publisher        = {PMLR},
  series           = {Proceedings of Machine Learning Research},
  volume           = {70},
  abstract         = {As a step towards developing zero-shot task generalization capabilities in reinforcement learning (RL), we introduce a new RL problem where the agent should learn to execute sequences of instructions after learning useful skills that solve subtasks. In this problem, we consider two types of generalizations: to previously unseen instructions and to longer sequences of instructions. For generalization over unseen instructions, we propose a new objective which encourages learning correspondences between similar subtasks by making analogies. For generalization over sequential instructions, we present a hierarchical architecture where a meta controller learns to use the acquired skills for executing the instructions. To deal with delayed reward, we propose a new neural architecture in the meta controller that learns when to update the subtask, which makes learning more efficient. Experimental results on a stochastic 3D domain show that the proposed ideas are crucial for generalization to longer instructions as well as unseen instructions.},
  creationdate     = {2022-07-24T10:49:36},
  file             = {:Oh_2017_Zero Shot Task Generalization with Multi Task Deep Reinforcement Learning.pdf:PDF},
  modificationdate = {2022-07-24T11:18:35},
  url              = {https://proceedings.mlr.press/v70/oh17a.html},
}

@Misc{Samvelyan2019SMAC,
  author           = {Mikayel Samvelyan and Tabish Rashid and Christian Schroeder de Witt and Gregory Farquhar and Nantas Nardelli and Tim G. J. Rudner and Chia-Man Hung and Philip H. S. Torr and Jakob Foerster and Shimon Whiteson},
  month            = feb,
  title            = {The StarCraft Multi-Agent Challenge},
  year             = {2019},
  abstract         = {In the last few years, deep multi-agent reinforcement learning (RL) has become a highly active area of research. A particularly challenging class of problems in this area is partially observable, cooperative, multi-agent learning, in which teams of agents must learn to coordinate their behaviour while conditioning only on their private observations. This is an attractive research area since such problems are relevant to a large number of real-world systems and are also more amenable to evaluation than general-sum problems. Standardised environments such as the ALE and MuJoCo have allowed single-agent RL to move beyond toy domains, such as grid worlds. However, there is no comparable benchmark for cooperative multi-agent RL. As a result, most papers in this field use one-off toy problems, making it difficult to measure real progress. In this paper, we propose the StarCraft Multi-Agent Challenge (SMAC) as a benchmark problem to fill this gap. SMAC is based on the popular real-time strategy game StarCraft II and focuses on micromanagement challenges where each unit is controlled by an independent agent that must act based on local observations. We offer a diverse set of challenge maps and recommendations for best practices in benchmarking and evaluations. We also open-source a deep multi-agent RL learning framework including state-of-the-art algorithms. We believe that SMAC can provide a standard benchmark environment for years to come. Videos of our best agents for several SMAC scenarios are available at: https://youtu.be/VZ7zmQ_obZ0.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-08-20T09:21:32},
  eprint           = {1902.04043},
  file             = {:Samvelyan_2019_The StarCraft Multi Agent Challenge.pdf:PDF},
  keywords         = {cs.LG, cs.MA, stat.ML},
  modificationdate = {2022-08-20T09:22:13},
  primaryclass     = {cs.LG},
}

@Misc{Gupta2020MASAC,
  author           = {Shubham Gupta and Ambedkar Dukkipati},
  title            = {Probabilistic View of Multi-agent Reinforcement Learning: A Unified Approach},
  year             = {2020},
  abstract         = {Formulating the reinforcement learning (RL) problem in the framework of probabilistic inference not only offers a new perspective about RL, but also yields practical algorithms that are more robust and easier to train. While this connection between RL and probabilistic inference has been extensively studied in the single-agent setting, it has not yet been fully understood in the multi-agent setup. In this paper, we pose the problem of multi-agent reinforcement learning as the problem of performing inference in a particular graphical model. We model the environment, as seen by each of the agents, using separate but related Markov decision processes. We derive a practical off-policy maximum-entropy actor-critic algorithm that we call Multi-agent Soft Actor-Critic (MA-SAC) for performing approximate inference in the proposed model using variational inference. MA-SAC can be employed in both cooperative and competitive settings. Through experiments, we demonstrate that MA-SAC outperforms a strong baseline on several multi-agent scenarios. While MA-SAC is one resultant multi-agent RL algorithm that can be derived from the proposed probabilistic framework, our work provides a unified view of maximum-entropy algorithms in the multi-agent setting.},
  creationdate     = {2022-09-15T08:53:11},
  file             = {:Gupta_2020_Probabilistic View of Multi Agent Reinforcement Learning_ a Unified Approach (MASAC).pdf:PDF},
  groups           = {MARL},
  keywords         = {multi-agent reinforcement learning, maximum entropy reinforcement learning},
  modificationdate = {2022-09-15T08:59:57},
  url              = {https://openreview.net/forum?id=S1ef6JBtPr},
}

@Book{Konar2020MACoordination,
  author           = {Konar, Amit and Sadhu, Arup Kumar},
  publisher        = {John Wiley \& Sons},
  title            = {Multi-Agent Coordination: A Reinforcement Learning Approach},
  year             = {2020},
  abstract         = {Discover the latest developments in multi-robot coordination techniques with this insightful and original resource

Multi-Agent Coordination: A Reinforcement Learning Approach delivers a comprehensive, insightful, and unique treatment of the development of multi-robot coordination algorithms with minimal computational burden and reduced storage requirements when compared to traditional algorithms. The accomplished academics, engineers, and authors provide readers with both a high-level introduction to, and overview of, multi-robot coordination, and in-depth analyses of learning-based planning algorithms.

You'll learn about how to accelerate the exploration of the team-goal and alternative approaches to speeding up the convergence of TMAQL by identifying the preferred joint action for the team. The authors also propose novel approaches to consensus Q-learning that address the equilibrium selection problem and a new way of evaluating the threshold value for uniting empires without imposing any significant computation overhead. Finally, the book concludes with an examination of the likely direction of future research in this rapidly developing field.

Readers will discover cutting-edge techniques for multi-agent coordination, including:

An introduction to multi-agent coordination by reinforcement learning and evolutionary algorithms, including topics like the Nash equilibrium and correlated equilibrium
Improving convergence speed of multi-agent Q-learning for cooperative task planning
Consensus Q-learning for multi-agent cooperative planning
The efficient computing of correlated equilibrium for cooperative q-learning based multi-agent planning
A modified imperialist competitive algorithm for multi-agent stick-carrying applications
Perfect for academics, engineers, and professionals who regularly work with multi-agent learning algorithms, Multi-Agent Coordination: A Reinforcement Learning Approach also belongs on the bookshelves of anyone with an advanced interest in machine learning and artificial intelligence as it applies to the field of cooperative or competitive robotics.},
  creationdate     = {2022-09-20T18:16:46},
  doi              = {10.1002/9781119699057},
  file             = {:Konar_2020_Multi Agent Coordination_ a Reinforcement Learning Approach.pdf:PDF},
  modificationdate = {2022-09-20T18:19:05},
  ranking          = {rank5},
}

@InCollection{Otterlo2012RL,
  author           = {Otterlo, Martijn van and Wiering, Marco},
  booktitle        = {Reinforcement learning},
  publisher        = {Springer},
  title            = {Reinforcement learning and markov decision processes},
  year             = {2012},
  pages            = {3--42},
  creationdate     = {2022-09-20T18:29:32},
  file             = {:Otterlo_2012_Reinforcement Learning and Markov Decision Processes.pdf:PDF},
  modificationdate = {2022-09-20T18:30:18},
}

@PhdThesis{Zintgraf2022Fast,
  author           = {Zintgraf, Luisa},
  school           = {University of Oxford},
  title            = {Fast adaptation via meta reinforcement learning},
  year             = {2022},
  abstract         = {Reinforcement Learning (RL) is a way to train artificial agents to autonomously interact with the world. In practice however, RL still has limitations that prohibit the deployment of RL agents in many real world settings. This is because RL takes long, typically requires human oversight, and produces specialised agents that can behave unexpected in unfamiliar situations. This thesis is motivated by the goal of making RL agents more flexible, robust, and safe to deploy in the real world. We develop agents capable of Fast Adaptation, i.e., agents that can learn new tasks efficiently.

To this end, we use Meta Reinforcement Learning (Meta-RL), where we teach agents not only to act autonomously, but to learn autonomously. We propose four novel Meta-RL methods based on the intuition that adapting fast can be divided into "task inference" (understanding the task) and "task solving" (solving the task). We hypothesise that this split can simplify optimisation and thus improve performance, and is more amenable to downstream tasks. To implement this, we propose a context-based approach, where the agent conditions on a context that represents its current knowledge about the task. The agent can then use this to decide whether to learn more about the task, or try and solve it.

In Chapter 5, we use a deterministic context and establish that this can indeed improve performance and adequately captures the task. In the subsequent chapters, we then introduce Bayesian reasoning over the context, to enable decision-making under task uncertainty. By combining Meta-RL, context-based learning, and approximate variational inference, we develop methods to compute approximately Bayes-optimal agents for single-agent settings (Chapter 6) and multi-agent settings (Chapter 7). Finally, Chapter 8 addresses the challenge of meta-learning with sparse rewards, which is an important setting for many real-world applications. We observe that existing Meta-RL methods can fail entirely if rewards are sparse, and propose a way to overcome this by encouraging the agent to explore during meta-training. We conclude the thesis with a reflection on the work presented in the context of current developments, and a discussion of open questions.

In summary, the contributions in this thesis significantly advance the field of Fast Adaptation via Meta-RL. The agents develop in this thesis can adapt faster than any previous methods across a variety of tasks, and we can compute approximately Bayes-optimal policies for much more complex task distributions than previously possible. We hope that this helps drive forward Meta-RL research and, in the long term, using RL to address important real world challenges.},
  comment          = {Related item: VariBAD: A very good method for Bayes-Adaptive deep RL via meta-learning
Related item: VariBAD: Variational Bayes-Adaptive Deep RL via meta-learning
Related item: Exploration in approximate hyper-state space for meta-reinforcement learning
Related item: Fast context adaptation via meta-learning
Related item: Deep interactive Bayesian reinforcement learning via meta-learning},
  creationdate     = {2022-09-20T18:30:53},
  file             = {:Zintgraf_2022_Fast Adaptation Via Meta Reinforcement Learning.pdf:PDF},
  modificationdate = {2022-09-20T18:32:53},
  ranking          = {rank5},
  url              = {https://ora.ox.ac.uk/objects/uuid:f6cced51-a71d-4c5b-bf29-0681ded284d8},
}

@InProceedings{Rashid2020WQMIX,
  author           = {Rashid, Tabish and Farquhar, Gregory and Peng, Bei and Whiteson, Shimon},
  booktitle        = {Advances in Neural Information Processing Systems},
  title            = {Weighted QMIX: Expanding Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning},
  year             = {2020},
  editor           = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages            = {10199--10210},
  publisher        = {Curran Associates, Inc.},
  volume           = {33},
  creationdate     = {2022-09-30T20:54:04},
  file             = {:Rashid_2020_Weighted QMIX_ Expanding Monotonic Value Function Factorisation for Deep Multi Agent Reinforcement Learning (WQMIX).pdf:PDF},
  groups           = {MARL},
  modificationdate = {2022-09-30T20:56:13},
  url              = {https://proceedings.neurips.cc/paper/2020/file/73a427badebe0e32caa2e1fc7530b7f3-Paper.pdf},
}

@InProceedings{Littman2001FFQ,
  author           = {Littman, Michael L.},
  booktitle        = {Proceedings of the Eighteenth International Conference on Machine Learning},
  title            = {Friend-or-Foe Q-Learning in General-Sum Games},
  year             = {2001},
  address          = {San Francisco, CA, USA},
  pages            = {322--328},
  publisher        = {Morgan Kaufmann Publishers Inc.},
  series           = {ICML '01},
  abstract         = {This paper describes an approach to reinforcement learning in multiagent general-sum games in which a learner is told to treat each other agent as either a \friend" or \foe". This Q-learning-style algorithm provides strong convergence guarantees compared to an existing Nash-equilibrium-based learning rule.},
  comment          = {跟方淇讨论MARL的均衡性时，她提到了这篇文章},
  creationdate     = {2022-10-10T15:37:34},
  file             = {:Littman_2001_Friend or Foe Q Learning in General Sum Games.pdf:PDF},
  groups           = {Equilibrium},
  isbn             = {1558607781},
  modificationdate = {2022-10-10T15:40:19},
  numpages         = {7},
}

@Article{Hu2003NashQ,
  author           = {Hu, Junling and Wellman, Michael P.},
  journal          = {J. Mach. Learn. Res.},
  title            = {Nash Q-Learning for General-Sum Stochastic Games},
  year             = {2003},
  issn             = {1532-4435},
  month            = dec,
  pages            = {1039--1069},
  volume           = {4},
  abstract         = {We extend Q-learning to a noncooperative multiagent context, using the framework of general-sum stochastic games. A learning agent maintains Q-functions over joint actions, and performs updates based on assuming Nash equilibrium behavior over the current Q-values. This learning protocol provably converges given certain restrictions on the stage games (defined by Q-values) that arise during learning. Experiments with a pair of two-player grid games suggest that such restrictions on the game structure are not necessarily required. Stage games encountered during learning in both grid environments violate the conditions. However, learning consistently converges in the first grid game, which has a unique equilibrium Q-function, but sometimes fails to converge in the second, which has three different equilibrium Q-functions. In a comparison of offline learning performance in both games, we find agents are more likely to reach a joint optimal path with Nash Q-learning than with a single-agent Q-learning method. When at least one agent adopts Nash Q-learning, the performance of both agents is better than using single-agent Q-learning. We have also implemented an online version of Nash Q-learning that balances exploration with exploitation, yielding improved performance.},
  creationdate     = {2022-10-10T15:40:00},
  file             = {:Hu_2003_Nash Q Learning for General Sum Stochastic Games.pdf:PDF},
  groups           = {Equilibrium},
  issue_date       = {12/1/2003},
  modificationdate = {2022-10-10T15:41:37},
  numpages         = {31},
  publisher        = {JMLR.org},
}

@Article{Bistaffa2022Efficient,
  author           = {Bistaffa, Filippo and Chalkiadakis, Georgios and Farinelli, Alessandro},
  journal          = {IEEE Transactions on Cybernetics},
  title            = {Efficient Coalition Structure Generation via Approximately Equivalent Induced Subgraph Games},
  year             = {2022},
  number           = {6},
  pages            = {5548--5558},
  volume           = {52},
  abstract         = {We show that any characteristic function game (CFG) G can be always turned into an approximately equivalent game represented using the induced subgraph game (ISG) representation. Such a transformation incurs obvious benefits in terms of tractability of computing solution concepts for G . Our transformation approach, namely, AE-ISG, is based on the solution of a norm approximation problem. We then propose a novel coalition structure generation (CSG) approach for ISGs that is based on graph clustering, which outperforms existing CSG approaches for ISGs by using off-the-shelf optimization solvers. Finally, we provide theoretical guarantees on the value of the optimal CSG solution of G with respect to the optimal CSG solution of the approximately equivalent ISG. As a consequence, our approach allows one to compute approximate CSG solutions with quality guarantees for any CFG. Results on a real-world application domain show that our approach outperforms a domain-specific CSG algorithm, both in terms of quality of the solutions and theoretical quality guarantees.},
  creationdate     = {2022-10-10T15:43:18},
  doi              = {10.1109/TCYB.2020.3040622},
  file             = {:Bistaffa_2022_Efficient Coalition Structure Generation Via Approximately Equivalent Induced Subgraph Games.pdf:PDF},
  groups           = {Coalition Structure},
  modificationdate = {2022-10-10T15:56:13},
}

@InCollection{Abuhaimed2022Effective,
  author           = {Sami Abuhaimed and Sandip Sen},
  booktitle        = {{HHAI}2022: Augmenting Human Intellect},
  publisher        = {{IOS} Press},
  title            = {Effective Task Allocation in Ad Hoc Human-Agent Teams},
  year             = {2022},
  month            = sep,
  pages            = {171--183},
  abstract         = {With accelerated progress in autonomous agent capabilities, mixed human and agent teams will become increasingly commonplace in both our personal and professional spheres. Hence, further examination of factors affecting collaboration efficacy in these types of teams are needed to inform the design and use of effective human-agent teams. Ad hoc human-agent teams, where team members interact without prior experience with teammates and only for a limited number of interactions, will be commonplace in dynamic environments with short opportunity windows for collaboration between diverse groups. We study ad-hoc team scenarios pairing a human with an agent where both need to assess and adapt to the capabilities of the partner to maximize team performance. In this work, we investigate the relative efficacy of two human-agent collaboration protocols that differ in the team member responsible for allocating tasks to the team. We designed, implemented, and experimented with an environment in which human-agent teams repeatedly collaborate to complete heterogeneous task sets.},
  creationdate     = {2022-10-10T16:07:10},
  doi              = {10.3233/faia220197},
  file             = {:Abuhaimed_2022_Effective Task Allocation in Ad Hoc Human Agent Teams.pdf:PDF},
  groups           = {Ad Hoc Task Allocation, Task Assignment},
  modificationdate = {2023-04-04T16:06:10},
  url              = {https://ebooks.iospress.nl/volumearticle/60857},
}

@InProceedings{Zhang2022MAPF,
  author           = {Zhang, Han and Chen, Jingkai and Li, Jiaoyang and Williams, Brian C. and Koenig, Sven},
  booktitle        = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  title            = {Multi-Agent Path Finding for Precedence-Constrained Goal Sequences},
  year             = {2022},
  address          = {Richland, SC},
  pages            = {1464--1472},
  publisher        = {International Foundation for Autonomous Agents and Multiagent Systems},
  series           = {AAMAS '22},
  abstract         = {With the rising demand for deploying robot teams in autonomous warehouses and factories, the Multi-Agent Path Finding (MAPF) problem has drawn more and more attention. The classical MAPF problem and most of its variants focus on navigating agent teams to goal locations while avoiding collisions. However, they do not take into account any precedence constraints that agents should respect when reaching their goal locations. Planning with precedence constraints is important for real-world multi-agent systems. For example, a mobile robot can only pick up a package at a station after it has been delivered by another robot. In this paper, we study the Multi-Agent Path Finding with Precedence Constraints (MAPF-PC) problem, in which agents need to visit sequences of goal locations while satisfying precedence constraints between the goal locations. We propose two algorithms for solving this problem systematically: Conflict-Based Search with Precedence Constraints (CBS-PC) is complete and optimal, and Priority-Based Search with Precedence Constraints (PBS-PC) is incomplete but more efficient in finding near-optimal solutions in practice. Our experimental results show that CBS-PC scales to dozens of agents and hundreds of goal locations and precedence constraints, and PBS-PC scales to hundreds of agents, around one thousand goal locations, and hundreds of precedence constraints.},
  comment          = {具有时序约束的多智能体路径规划MAPF，按顺序访问目标序列},
  creationdate     = {2022-10-18T21:39:33},
  doi              = {10.5555/3535850.3536013},
  file             = {:Zhang_2022_Multi Agent Path Finding for Precedence Constrained Goal Sequences.pdf:PDF},
  isbn             = {9781450392136},
  keywords         = {multi-agent path finding, precedence constraints},
  location         = {Virtual Event, New Zealand},
  modificationdate = {2022-10-26T16:39:38},
  numpages         = {9},
}

@InProceedings{Yang2022Adaptive,
  author           = {Yang, Jiachen and Wang, Ethan and Trivedi, Rakshit and Zhao, Tuo and Zha, Hongyuan},
  booktitle        = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  title            = {Adaptive Incentive Design with Multi-Agent Meta-Gradient Reinforcement Learning},
  year             = {2022},
  address          = {Richland, SC},
  pages            = {1436–1445},
  publisher        = {International Foundation for Autonomous Agents and Multiagent Systems},
  series           = {AAMAS '22},
  abstract         = {Critical sectors of human society are progressing toward the adoption of powerful artificial intelligence (AI) agents, which are trained individually on behalf of self-interested principals but deployed in a shared environment. Short of direct centralized regulation of AI, which is as difficult an issue as regulation of human actions, one must design institutional mechanisms that indirectly guide agents' behaviors to safeguard and improve social welfare in the shared environment. Our paper focuses on one important class of such mechanisms: the problem of adaptive incentive design, whereby a central planner intervenes on the payoffs of an agent population via incentives in order to optimize a system objective. To tackle this problem in high-dimensional environments whose dynamics may be unknown or too complex to model, we propose a model-free meta-gradient method to learn an adaptive incentive function in the context of multi-agent reinforcement learning. Via the principle of online cross-validation, the incentive designer explicitly accounts for its impact on agents' learning and, through them, the impact on future social welfare. Experiments on didactic benchmark problems show that the proposed method can induce selfish agents to learn near-optimal cooperative behavior and significantly outperform learning-oblivious baselines. When applied to a complex simulated economy, the proposed method finds tax policies that achieve better trade-off between economic productivity and equality than baselines, a result that we interpret via a detailed behavioral analysis.},
  creationdate     = {2022-10-18T22:11:32},
  doi              = {10.5555/3535850.3536010},
  file             = {:Yang_2022_Adaptive Incentive Design with Multi Agent Meta Gradient Reinforcement Learning.pdf:PDF},
  groups           = {Multi_Agent_Meta_Learning},
  isbn             = {9781450392136},
  keywords         = {incentive design, multi-agent reinforcement learning},
  location         = {Virtual Event, New Zealand},
  modificationdate = {2022-10-18T22:23:04},
  numpages         = {10},
}

@InProceedings{Tasbas2021AirCombat,
  author           = {Ahmet Semih Tasbas and Sevket Utku Aydinli},
  booktitle        = {2021 International Conference on Engineering and Emerging Technologies (ICEET)},
  title            = {2-D Air Combat Maneuver Decision Using Reinforcement Learning},
  year             = {2021},
  month            = oct,
  publisher        = {{IEEE}},
  abstract         = {Applications of reinforcement learning (RL) have become common in many decision-making problems. One of these applications is the air combat maneuver decision problem. It is still an open problem, which actions the aircraft will take in the air combat area and which trajectories it will follow. For the solution to this problem, suitable action sets can be sought by using RL that can put the aircraft in an advantageous position. In this study, a 2D air combat environment was created that supports multi-agent training. RL algorithms such as Q-learning and Double Deep Q learning (DDQN) are implemented to create agent policy. These algorithms determine what actions the aircraft will take regarding speed and direction. While the previous studies searched for a solution to this problem using different algorithms, this study focuses on how the enemy agent's policy affects the exploration of our agent. In the training phase, agents are trained against random policy and pre-trained agents. The result of the study showed that when the enemy agent's policy is better, the policy of the agent we train is also better. In addition, the RL algorithms mentioned above are compared and sample trajectories under different initial conditions are shown.},
  creationdate     = {2022-10-19T11:24:37},
  doi              = {10.1109/iceet53442.2021.9659753},
  file             = {:Tasbas_2021_2 D Air Combat Maneuver Decision Using Reinforcement Learning.pdf:PDF},
  modificationdate = {2023-04-04T16:06:13},
}

 
@Article{Xu2022Research,
  author           = {Xu, Dan and Chen, Gang},
  journal          = {Aerospace Systems},
  title            = {The research on intelligent cooperative combat of {UAV} cluster with multi-agent reinforcement learning},
  year             = {2022},
  issn             = {2523-3955},
  month            = mar,
  number           = {1},
  pages            = {107--121},
  volume           = {5},
  abstract         = {With the rapid development of computer hardware and intelligent technology, the intelligent combat of unmanned aerial vehicle (UAV) cluster will become the main battle mode in the future battlefield. The UAV cluster as a multi-agent system (MAS), the traditional single-agent reinforcement learning (SARL) algorithm is no longer applicable. To truly achieve autonomous and cooperative combat of the UAV cluster, the multi-agent reinforcement learning (MARL) algorithm has become a research hotspot. Considering that the current UAV cluster combat is still in the program control stage, the fully autonomous and intelligent cooperative combat has not been realized. To realize the autonomous planning of the UAV cluster according to the changing environment and cooperate with each other to complete the combat goal, we propose a new MARL framework which adopts the policy of centralized training with decentralized execution, and uses actor-critic network to select the execution action and make the corresponding evaluation. By improving the structure of the learning network and refining the reward mechanism, the new algorithm can further optimize the training results and greatly improve the operation security. Compared with the original multi-agent deep deterministic policy gradient (MADDPG) algorithm, the ability of cluster cooperative operation gets effectively enhanced.},
  creationdate     = {2022-10-19T22:18:47},
  doi              = {10.1007/s42401-021-00105-x},
  file             = {:Xu_2022_The Research on Intelligent Cooperative Combat of UAV Cluster with Multi Agent Reinforcement Learning.pdf:PDF},
  keywords         = {Multi-agent system, Autonomous learning, Cooperative combat, Multi-agent reinforcement learning, Improved multi-agent deep deterministic policy gradient},
  language         = {en},
  modificationdate = {2022-10-19T22:19:27},
  url              = {https://doi.org/10.1007/s42401-021-00105-x},
  urldate          = {2022-10-19},
}

 
@InProceedings{Hafizoglu2018Effects,
  author           = {Hafizoğlu, Feyza Merve and Sen, Sandip},
  booktitle        = {Proceedings of the 17th {International} {Conference} on {Autonomous} {Agents} and {MultiAgent} {Systems}},
  title            = {The {Effects} of {Past} {Experience} on {Trust} in {Repeated} {Human}-{Agent} {Teamwork}},
  year             = {2018},
  address          = {Richland, SC},
  pages            = {514--522},
  publisher        = {International Foundation for Autonomous Agents and Multiagent Systems},
  series           = {{AAMAS} '18},
  abstract         = {For human-agent virtual ad hoc teams to be effective, humans must be able to trust their agent counterparts. To earn the human's trust, agents need to quickly develop an understanding of the expectation of human team members and adapt accordingly. This study empirically investigates the impact of past experience on human trust in and behavior towards agent teammates. To do so, we developed a repeated team coordination game, the Game of Trust (GoT), in which two players repeatedly cooperate to complete team tasks without prior assignment of subtasks. The effects of past experience on human trust are evaluated by performing an extensive set of controlled experiments with participants recruited from Amazon Mechanical Turk, a crowdsourcing marketplace. We collect both teamwork performance data as well as surveys to gauge participants' trust in their agent teammates. The results show that positive (negative) past experience increases (decreases) human trust in agent teammates and past experience can affect three antecedents of trust: emotional state, game expertise, and expectation. These findings provide clear and significant evidence of the influence of key factors on human trust in virtual agent teammates and enhance our understanding of the changes in human trust in peer-level agent teammates with respect to past experience.},
  comment          = {human-agent virtual ad hoc team, 人类如何信任其虚拟队友？agent必须快速了解human的期望，并进行适应。
本文讨论过去的经验会如何影响human对agent的信任和采取的行动？
结论：积极的经历会增加信任；取决于情绪状态，专业程度，期望
众包，Mechanical Turk
信任游戏：repeated team coordination game，没有事先的任务分配，需要共同完成一些子任务},
  creationdate     = {2022-10-20T15:11:39},
  file             = {:Hafizoğlu_2018_The Effects of Past Experience on Trust in Repeated Human Agent Teamwork.pdf:PDF},
  groups           = {Ad Hoc Teamwork, Human-Agent-Robot},
  keywords         = {past experience, human-agent teamwork, virtual environment, trust},
  modificationdate = {2022-12-03T09:43:26},
  urldate          = {2022-10-20},
}

@InProceedings{Li2022MAC,
  author           = {Yibo Li and Xuemei Sui and Senyue Zhang and Haibin Zhu},
  booktitle        = {2022 {IEEE} 25th International Conference on Computer Supported Cooperative Work in Design ({CSCWD})},
  title            = {Multi-Agent Collaboration Planning in Mentorship Mode},
  year             = {2022},
  month            = may,
  pages            = {77--82},
  publisher        = {{IEEE}},
  abstract         = {In order to solve the problem of the task collaboration of multi-agent, we propose a task collaboration planning based on mentorship mode (MM-TCP). In this paper, the Group Role Assignment (GRA) framework is used to formalize the multi-agent task collaboration problem. The Growth curve is adopted to describe the change trend of the qualification values of the prior and posterior agents in a mentorship mode. The IBM ILOG CPLEX optimization package (CPLEX) is used to find the solution of the model. Experiments show that the proposed method is feasible and effective. With the utilization of a mentorship mode, the performance of the whole team is improved. In addition, the effect of the Growth curve model on team performance is also discussed in this paper.},
  creationdate     = {2022-10-20T15:59:44},
  doi              = {10.1109/cscwd54268.2022.9776287},
  file             = {:Li_2022_Multi Agent Collaboration Planning in Mentorship Mode.pdf:PDF},
  modificationdate = {2023-04-04T16:06:15},
}

@Article{Drake2018MAPF,
  author           = {Daniel Drake and Scott Koziol and Eugene Chabot},
  journal          = {{IEEE} Access},
  title            = {Mobile Robot Path Planning With a Moving Goal},
  year             = {2018},
  pages            = {12800--12814},
  volume           = {6},
  abstract         = {Path planners, in which a hunter is required to chase after a moving target, are an important problem for robotic systems such as unmanned aerial vehicles and unmanned underwater vehicles. This paper describes an incremental moving target path planning algorithm which leverages previous planning data to update the path in the case where the target moves. The algorithm in this paper addresses the need for a quick path-planner that can be used in an environment where the target is moving. The algorithm does this by sacrificing optimality in order to reduce the computational complexity of the problem. Examples show that the algorithm reduces the complexity of re-planning by approximately 12 times while only increasing path length taken by 1.5%. Analytical estimates of the best and worst case complexity of the algorithm were developed, and these estimates are validated with experimental data.},
  comment          = {类似D*？},
  creationdate     = {2022-10-20T23:03:40},
  doi              = {10.1109/access.2018.2797070},
  modificationdate = {2022-10-20T23:04:05},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Kim2022MetaTraffic,
  author           = {Gyeongjun Kim and Jiwon Kang and Keemin Sohn},
  journal          = {Computer-Aided Civil and Infrastructure Engineering},
  title            = {A meta{\textendash}reinforcement learning algorithm for traffic signal control to automatically switch different reward functions according to the saturation level of traffic flows},
  year             = {2022},
  month            = sep,
  abstract         = {Reinforcement learning (RL) algorithms have been widely applied in solving traffic signal control problems. Traffic environments, however, are intrinsically nonstationary, which creates a convergence problem that RL algorithms struggle to overcome. Basically, as a target problem for an RL algorithm, the Markov decision process (MDP) can be solved only when both the transition and reward functions do not vary. Unfortunately, the environment for traffic signal control is not stationary since the goal of traffic signal control varies according to congestion levels. For unsaturated traffic conditions, the objective of traffic signal control should be to minimize vehicle delay. On the other hand, the objective must be to maximize the throughput when traffic flow is saturated. A multiregime analysis is possible for varying conditions, but classifying the traffic regime creates another complex task. The present study provides a meta-RL algorithm that embeds a latent vector to recognize the different contexts of an environment in order to automatically classify traffic regimes and apply a customized reward for each context. In simulation experiments, the proposed meta-RL algorithm succeeded in differentiating rewards according to the saturation level of traffic conditions.},
  creationdate     = {2022-10-21T09:18:30},
  doi              = {10.1111/mice.12924},
  groups           = {Meta Learning},
  modificationdate = {2023-04-04T16:06:18},
  publisher        = {Wiley},
  ranking          = {rank5},
}

@Misc{Dodampegama2022Knowledge,
  author           = {Hasra Dodampegama and Mohan Sridharan},
  month            = aug,
  title            = {Knowledge-based and Data-driven Reasoning and Learning for Ad Hoc Teamwork},
  year             = {2022},
  abstract         = {We present an architecture for ad hoc teamwork, which refers to collaboration in a team of agents without prior coordination. State of the art methods for this problem often include a data-driven component that uses a long history of prior observations to model the behaviour of other agents (or agent types) and to determine the ad hoc agent's behaviour. In many practical domains, it is challenging to find large training datasets, and necessary to understand and incrementally extend the existing models to account for changes in team composition or domain attributes. Our architecture combines the principles of knowledge-based and data-driven reasoning and learning. Specifically, we enable an ad hoc agent to perform non-monotonic logical reasoning with prior commonsense domain knowledge and incrementally-updated simple predictive models of other agents' behaviour. We use the benchmark simulated multi-agent collaboration domain Fort Attack to demonstrate that our architecture supports adaptation to unforeseen changes, incremental learning and revision of models of other agents' behaviour from limited samples, transparency in the ad hoc agent's decision making, and better performance than a data-driven baseline.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-10-22T22:30:21},
  eprint           = {2208.11556},
  file             = {:Dodampegama_2022_Knowledge Based and Data Driven Reasoning and Learning for Ad Hoc Teamwork.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  keywords         = {cs.AI, cs.MA},
  modificationdate = {2022-10-22T22:31:11},
  primaryclass     = {cs.AI},
  ranking          = {rank5},
}

@InProceedings{Cook2021Adhoc,
  author           = {Joshua Cook and Kagan Tumer},
  booktitle        = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
  title            = {Ad hoc teaming through evolution},
  year             = {2021},
  month            = jul,
  publisher        = {{ACM}},
  abstract         = {Cooperative Co-evolutionary Algorithms effectively train policies in multiagent systems with a single, statically defined team. However, many real-world problems, such as search and rescue, require agents to operate in multiple teams. When the structure of the team changes, these policies show reduced performance as they were trained to cooperate with only one team. In this work, we solve the cooperation problem by training agents to fill the needs of an arbitrary team, thereby gaining the ability to support a large variety of teams. We introduce Ad hoc Teaming Through Evolution (ATTE) which evolves a limited number of policy types using fitness aggregation across multiple teams. ATTE leverages agent types to reduce the dimensionality of the interaction search space, while fitness aggregation across teams selects for more adaptive policies. In a simulated multi-robot exploration task, ATTE is able to learn policies that are effective in a variety of teaming schemes, improving the performance of CCEA by a factor of up to five times.},
  creationdate     = {2022-10-22T22:33:56},
  doi              = {10.1145/3449726.3459560},
  file             = {:Cook_2021_Ad Hoc Teaming through Evolution.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  modificationdate = {2023-04-04T16:06:22},
}

@InProceedings{Yourdshahi2018Towards,
  author           = {Elnaz Shafipour Yourdshahi and Thomas Pinder and Gauri Dhawan and Leandro Soriano Marcolino and Plamen Angelov},
  booktitle        = {2018 {IEEE} International Conference on Agents ({ICA})},
  title            = {Towards Large Scale Ad-hoc Teamwork},
  year             = {2018},
  month            = jul,
  publisher        = {{IEEE}},
  creationdate     = {2022-10-26T10:31:02},
  doi              = {10.1109/agents.2018.8460136},
  file             = {:Yourdshahi_2018_Towards Large Scale Ad Hoc Teamwork.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  modificationdate = {2023-04-04T16:06:25},
}

@Article{Kolling2013HumanSwarm,
  author           = {Andreas Kolling and Katia Sycara and Steve Nunnally and Michael Lewis},
  journal          = {Journal of Human-Robot Interaction},
  title            = {Human Swarm Interaction: An Experimental Study of Two Types of Interaction with Foraging Swarms},
  year             = {2013},
  month            = jun,
  number           = {2},
  volume           = {2},
  abstract         = {In this paper we present the first study of human-swarm interaction comparing two fundamental types of interaction, coined intermittent and environmental. These types are exemplified by two control methods, selection and beacon control, made available to a human operator to control a foraging swarm of robots. Selection and beacon control differ with respect to their temporal and spatial influence on the swarm and enable an operator to generate different strategies from the basic behaviors of the swarm. Selection control requires an active selection of groups of robots while beacon control exerts an influence on nearby robots within a set range. Both control methods are implemented in a testbed in which operators solve an information foraging problem by utilizing a set of swarm behaviors. The robotic swarm has only local communication and sensing capabilities. The number of robots in the swarm range from 50 to 200. Operator performance for each control method is compared in a series of missions in different environments with no obstacles up to cluttered and structured obstacles. In addition, performance is compared to simple and advanced autonomous swarms. Thirty-two participants were recruited for participation in the study. Autonomous swarm algorithms were tested in repeated simulations. Our results showed that selection control scales better to larger swarms and generally outperforms beacon control. Operators utilized different swarm behaviors with different frequency across control methods, suggesting an adaptation to different strategies induced by choice of control method. Simple autonomous swarms outperformed human operators in open environments, but operators adapted better to complex environments with obstacles. Human controlled swarms fell short of task-specific benchmarks under all conditions. Our results reinforce the importance of understanding and choosing appropriate types of human-swarm interaction when designing swarm systems, in addition to choosing appropriate swarm behaviors.},
  creationdate     = {2022-10-26T16:07:20},
  doi              = {10.5898/jhri.2.2.kolling},
  file             = {:Kolling_2013_Human Swarm Interaction_ an Experimental Study of Two Types of Interaction with Foraging Swarms.pdf:PDF},
  groups           = {Human-Swarm},
  modificationdate = {2023-04-04T16:06:28},
  publisher        = {Journal of Human-Robot Interaction},
}

@Article{Kolling2016Survey,
  author           = {Andreas Kolling and Phillip Walker and Nilanjan Chakraborty and Katia Sycara and Michael Lewis},
  journal          = {IEEE Trans. Human-Mach. Syst.},
  title            = {Human Interaction With Robot Swarms: A Survey},
  year             = {2016},
  month            = feb,
  number           = {1},
  pages            = {9--26},
  volume           = {46},
  abstract         = {Recent advances in technology are delivering robots of reduced size and cost. A natural outgrowth of these advances are systems comprised of large numbers of robots that collaborate autonomously in diverse applications. Research on effective autonomous control of such systems, commonly called swarms, has increased dramatically in recent years and received attention from many domains, such as bioinspired robotics and control theory. These kinds of distributed systems present novel challenges for the effective integration of human supervisors, operators, and teammates that are only beginning to be addressed. This paper is the first survey of human-swarm interaction (HSI) and identifies the core concepts needed to design a human-swarm system. We first present the basics of swarm robotics. Then, we introduce HSI from the perspective of a human operator by discussing the cognitive complexity of solving tasks with swarm systems. Next, we introduce the interface between swarm and operator and identify challenges and solutions relating to human-swarm communication, state estimation and visualization, and human control of swarms. For the latter, we develop a taxonomy of control methods that enable operators to control swarms effectively. Finally, we synthesize the results to highlight remaining challenges, unanswered questions, and open problems for HSI, as well as how to address them in future works.},
  creationdate     = {2022-10-26T16:15:54},
  doi              = {10.1109/thms.2015.2480801},
  file             = {:Kolling_2016_Human Interaction with Robot Swarms_ a Survey.pdf:PDF},
  groups           = {Human-Swarm},
  modificationdate = {2023-04-04T20:28:52},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
  ranking          = {rank4},
}

@Article{Cody2021Human,
  author           = {Jason R. Cody and Karina A. Roundtree and Julie A. Adams},
  journal          = {{ACM} Transactions on Human-Robot Interaction},
  title            = {Human-Collective Collaborative Target Selection},
  year             = {2021},
  month            = jun,
  number           = {2},
  pages            = {1--29},
  volume           = {10},
  abstract         = {Robotic collectives are composed of hundreds or thousands of distributed robots using local sensing and communication that encompass characteristics of biological spatial swarms, colonies, or a combination of both. Interactions between the individual entities can result in emergent collective behaviors. Human operators in future disaster response or military engagement scenarios are likely to deploy semi-autonomous collectives to gather information and execute tasks within a wide area, while reducing the exposure of personnel to danger. This article presents and evaluates two action selection models in an experiment consisting of a single human operator supervising four simulated collectives. The action selection models have two parts: (1) a best-of-n decision-making model that attempts to choose the highest-quality target from a set of n targets and (2) a quorum sensing task sequencing model that enables autonomous target site occupation. An original biologically inspired insect colony decision model is compared to a bias-reducing model that attempts to reduce environmental bias, which can negatively influence collective best-of-n decisions when poorer-quality targets are easier to evaluate than higher-quality targets. The collective decision-making models are compared in both supervised and unsupervised trials. The bias-reducing model without human supervision is slower than the original model but is 57% more accurate for decisions where evaluating the optimal target is more difficult. Human-collective teams using the bias-reducing model require less operator influence and achieve 25% higher accuracy with difficult decisions compared to the teams using the original model.},
  comment          = {在灾难或军事领域，human可能会部署半自主的群体在广阔空间中执行任务，减少人员暴露在危险中。
涉及了human operator的指令如何影响swarm
本文评估了两种动作选择模型，动作选择分为两步：从n个目标中选择最优的；},
  creationdate     = {2022-10-26T16:16:59},
  doi              = {10.1145/3442679},
  file             = {:Cody_2021_Human Collective Collaborative Target Selection.pdf:PDF},
  groups           = {Human-Swarm},
  modificationdate = {2023-04-04T16:06:32},
  publisher        = {Association for Computing Machinery ({ACM})},
  url              = {https://dl.acm.org/doi/10.1145/3442679},
}

@Misc{Hussein2018Towards,
  author           = {Aya Hussein and Leo Ghignone and Tung Nguyen and Nima Salimi and Hung Nguyen and Min Wang and Hussein A. Abbass},
  month            = mar,
  title            = {Towards Bi-Directional Communication in Human-Swarm Teaming: A Survey},
  year             = {2018},
  abstract         = {Swarm systems consist of large numbers of robots that collaborate autonomously. With an appropriate level of human control, swarm systems could be applied in a variety of contexts ranging from search-and-rescue situations to Cyber defence. The two decision making cycles of swarms and humans operate on two different time-scales, where the former is normally orders of magnitude faster than the latter. Closing the loop at the intersection of these two cycles will create fast and adaptive human-swarm teaming networks. This paper brings desperate pieces of the ground work in this research area together to review this multidisciplinary literature. We conclude with a framework to synthesize the findings and summarize the multi-modal indicators needed for closed-loop human-swarm adaptive systems.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-10-26T16:19:00},
  eprint           = {1803.03093},
  file             = {:Hussein_2018_Towards Bi Directional Communication in Human Swarm Teaming_ a Survey.pdf:PDF},
  groups           = {Human-Swarm},
  keywords         = {cs.HC},
  modificationdate = {2022-10-26T16:19:28},
  primaryclass     = {cs.HC},
}

@Article{Hasbach2021Design,
  author           = {Jonas D Hasbach and Maren Bennewitz},
  journal          = {Adaptive Behavior},
  title            = {The design of self-organizing human{\textendash}swarm intelligence},
  year             = {2021},
  month            = jul,
  number           = {4},
  pages            = {361--386},
  volume           = {30},
  abstract         = {Human–swarm interaction is a frontier in the realms of swarm robotics and human-factors engineering. However, no holistic theory has been explicitly formulated that can inform how humans and robot swarms should interact through an interface while considering real-world demands, the relative capabilities of the components, as well as the desired jointsystem behaviours. In this article, we apply a holistic perspective that we refer to as joint human–swarm loops, that is, a cybernetic system made of human, swarm and interface. We argue that a solution for human–swarm interaction should make the joint human–swarm loop an intelligent system that balances between centralized and decentralized control. The swarm-amplified human is suggested as a possible design that combines perspectives from swarm robotics, humanfactors engineering and theoretical neuroscience to produce such a joint human–swarm loop. Essentially, it states that the robot swarm should be integrated into the human’s low-level nervous system function. This requires modelling both the robot swarm and the biological nervous system as self-organizing systems. We discuss multiple design implications that follow from the swarm-amplified human, including a computational experiment that shows how the robot swarm itself can be a self-organizing interface based on minimal computational logic.},
  comment          = {四区SCI
a cybernetic system made of human, swarm， interface},
  creationdate     = {2022-10-27T10:30:05},
  doi              = {10.1177/10597123211017550},
  file             = {:Hasbach_2021_The Design of Self Organizing Human_swarm Intelligence.pdf:PDF},
  groups           = {Human-Swarm},
  modificationdate = {2023-04-04T16:06:39},
  publisher        = {{SAGE} Publications},
  ranking          = {rank4},
}

 
@Article{Wang2021Cooperative,
  author           = {Wang, Ning and Li, Zhe and Liang, Xiaolong and Li, Ying and Zhao, Feihu},
  journal          = {Mathematical Problems in Engineering},
  title            = {Cooperative Target Search of UAV Swarm with Communication Distance Constraint},
  year             = {2021},
  issn             = {1024-123X},
  month            = sep,
  pages            = {e3794329},
  volume           = {2021},
  abstract         = {This paper proposes a cooperative search algorithm to enable swarms of unmanned aerial vehicles (UAVs) to capture moving targets. It is based on prior information and target probability constrained by inter-UAV distance for safety and communication. First, a rasterized environmental cognitive map is created to characterize the task area. Second, based on Bayesian theory, the posterior probability of a target’s existence is updated using UAV detection information. Third, the predicted probability distribution of the dynamic time-sensitive target is obtained by calculating the target transition probability. Fourth, a customized information interaction mechanism switches the interaction strategy and content according to the communication distance to produce cooperative decision-making in the UAV swarm. Finally, rolling-time domain optimization generates interactive information, so interactive behavior and autonomous decision-making among the swarm members are realized. Simulation results showed that the proposed algorithm can effectively complete a cooperative moving-target search when constrained by communication distance yet still cooperate effectively in unexpected situations such as a fire.},
  comment          = {值得参考},
  creationdate     = {2022-10-27T10:55:59},
  doi              = {10.1155/2021/3794329},
  file             = {:Wang_2021_Cooperative Target Search of UAV Swarm with Communication Distance Constraint.pdf:PDF},
  groups           = {Swarm Intelligence},
  language         = {en},
  modificationdate = {2023-01-07T22:01:04},
  publisher        = {Hindawi},
  ranking          = {rank5},
  url              = {https://www.hindawi.com/journals/mpe/2021/3794329/},
  urldate          = {2022-10-27},
}

 
@Article{Zhou2022Intelligent,
  author           = {Zhou, Longyu and Leng, Supeng and Liu, Qiang and Wang, Qing},
  journal          = {IEEE Internet of Things Journal},
  title            = {Intelligent {UAV} {Swarm} {Cooperation} for {Multiple} {Targets} {Tracking}},
  year             = {2022},
  issn             = {2327-4662},
  number           = {1},
  pages            = {743--754},
  volume           = {9},
  abstract         = {With the advantages of easy deployment and flexible usage, unmanned aerial vehicle (UAV) has advanced the multitarget tracking (MTT) applications. The UAV-MTT system has great potentials to execute dull, dangerous, and critical missions for frontier defense and security. A key challenge in UAV-MTT is how to coordinate multiple UAVs to track diverse invading targets accurately and consecutively. In this article, we propose a UAV swarm-based cooperative tracking architecture to systematically improve the UAV tracking performance. We design an intelligent UAV swarm-based cooperative algorithm for consecutive target tracking and physical collision avoidance. Moreover, we design an efficient cooperative algorithm to predict the trajectory of invading targets accurately. Our simulation results demonstrate that the swarm behaviors stay stable in realistic scenarios with perturbing obstacles. Compared with state-of-the-art solutions, such as the matched deep \$Q\$ -network, our algorithms can increase tracking accuracy by 60\%, reduce tracking delay by 23\%, and achieve physical collision-avoidance during the tracking process.},
  comment          = {IoT},
  creationdate     = {2022-10-27T21:32:51},
  doi              = {10.1109/JIOT.2021.3085673},
  file             = {:Zhou_2022_Intelligent UAV Swarm Cooperation for Multiple Targets Tracking.pdf:PDF},
  groups           = {Swarm Intelligence},
  keywords         = {Target tracking, Sensors, Task analysis, Trajectory, Unmanned aerial vehicles, Computational modeling, Prediction algorithms, Mobile target tracking, prediction, scheduling, unmanned-aerial-vehicle (UAV) swarm intelligence (SI)},
  modificationdate = {2022-11-11T09:08:46},
}

 
@Misc{Qin2022Drones,
  author           = {Qin, Chuhao and Candan, Fethi and Mihaylova, Lyudmila S. and Pournaras, Evangelos},
  month            = aug,
  note             = {arXiv:2208.05914 [cs] type: article},
  title            = {3, 2, 1, {Drones} {Go}! {A} {Testbed} to {Take} off {UAV} {Swarm} {Intelligence} for {Distributed} {Sensing}},
  year             = {2022},
  abstract         = {This paper introduces a testbed to study distributed sensing problems of Unmanned Aerial Vehicles (UAVs) exhibiting swarm intelligence. Several Smart City applications, such as transport and disaster response, require efficient collection of sensor data by a swarm of intelligent and cooperative UAVs. This often proves to be too complex and costly to study systematically and rigorously without compromising scale, realism and external validity. With the proposed testbed, this paper sets a stepping stone to emulate, within small laboratory spaces, large sensing areas of interest originated from empirical data and simulation models. Over this sensing map, a swarm of low-cost drones can fly allowing the study of a large spectrum of problems such as energy consumption, charging control, navigation and collision avoidance. The applicability of a decentralized multi-agent collective learning algorithm (EPOS) for UAV swarm intelligence along with the assessment of power consumption measurements provide a proof-of-concept and validate the accuracy of the proposed testbed.},
  annote           = {Comment: 12 pages, 9 figures, Accepted for UKCI (2022)},
  creationdate     = {2022-10-27T21:33:33},
  doi              = {10.48550/arXiv.2208.05914},
  file             = {:Qin_2022_3, 2, 1, Drones Go! a Testbed to Take off UAV Swarm Intelligence for Distributed Sensing.pdf:PDF},
  groups           = {Swarm Intelligence},
  keywords         = {Computer Science - Robotics, Computer Science - Distributed, Parallel, and Cluster Computing},
  modificationdate = {2022-10-27T21:40:09},
  school           = {arXiv},
  url              = {http://arxiv.org/abs/2208.05914},
  urldate          = {2022-10-27},
}

 
@InProceedings{Xu2020Coverage,
  author           = {Xu, Xiaotian and Diaz-Mercado, Yancy},
  booktitle        = {2020 {American} {Control} {Conference} ({ACC})},
  title            = {Multi-{Agent} {Control} {Using} {Coverage} {Over} {Time}-{Varying} {Domains}},
  year             = {2020},
  month            = jul,
  note             = {ISSN: 2378-5861},
  pages            = {2030--2035},
  abstract         = {Multi-agent coverage control is used as a mechanism to influence the behavior of a group of robots by introducing time-varying domains. The coverage optimization problem is modified to adopt time-varying domains, and the proposed control law possesses an exponential convergence characteristic. Complex multi-agent control is simplified by specifying the desired distribution and behavior of the robot team as a whole. In the proposed approach, design of the inputs to the multi-agent system, i.e., time-varying density and time-varying domain, are agnostic to the size of the system. Analytic expressions of surface and line integrals present in the control law are obtained under uniform density. The scalability of the proposed control strategy is analyzed and verified via numerical simulation. Experiments on real robots are used to test the proposed control law.},
  comment          = {好像不是很相关，而且比较有门槛；偏控制论的方法},
  creationdate     = {2022-10-29T09:30:54},
  doi              = {10.23919/ACC45564.2020.9147326},
  file             = {:Xu_2020_Multi Agent Control Using Coverage Over Time Varying Domains.pdf:PDF},
  groups           = {Swarm Intelligence},
  issn             = {2378-5861},
  keywords         = {Robot kinematics, Convergence, Multi-agent systems, Control systems, Integral equations, Density functional theory},
  modificationdate = {2022-11-11T08:59:44},
}

 
@Article{SaiRayala2020PSO,
  author           = {Sai Rayala, Santosh and Ashok Kumar, N.},
  journal          = {Materials Today: Proceedings},
  title            = {Particle {Swarm} {Optimization} for robot target tracking application},
  year             = {2020},
  issn             = {2214-7853},
  month            = jan,
  pages            = {3600--3603},
  volume           = {33},
  abstract         = {Tracking is a process of finding continuous path of a selected target. Tracking is important in various civil and defense applications. For tracking applications, various adaptive filters are being used such as Kalman Filter and it's variants. In this paper, a popular meta-heuristic algorithm, namely, Particle Swarm Optimization (PSO) algorithm is applied to track a target which is a robot and it is transmitting noisy data due to installation of cheap quality sensors onboard. These noisy observations are filtered by PSO algorithm in order to estimate true path of the robot. The simulated results are presented by using MATLAB software. From the results, it is observed that the PSO algorithm is very effective algorithm in estimating the target location continuously with almost zero estimation error. Thus, Particle Swarm Optimization algorithm is a suitable algorithm for robot tracking applications and it can also be used for nonlinear data processing in autonomous robot design applications.},
  comment          = {基于噪声路径数据(廉价传感器)，用PSO来拟合真实路径
全文只介绍了PSO的算法流程(废话)，没有说明PSO是如何用来做估计的},
  creationdate     = {2022-11-01T21:40:30},
  doi              = {10.1016/j.matpr.2020.05.660},
  file             = {:Sai Rayala_2020_Particle Swarm Optimization for Robot Target Tracking Application.pdf:PDF},
  groups           = {Swarm Intelligence},
  keywords         = {Estimation, Meta-heuristic, Particle swarm optimization, Tracking, Zero constant error},
  language         = {en},
  modificationdate = {2022-11-22T14:55:27},
  series           = {International {Conference} on {Nanotechnology}: {Ideas}, {Innovation} and {Industries}},
  url              = {https://www.sciencedirect.com/science/article/pii/S2214785320342711},
  urldate          = {2022-11-01},
}

@InProceedings{Giacomossi2021TargetSearch,
  author           = {Giacomossi, Luiz and Souza, Flavio and Cortes, Raphael Gomes and Mirko Montecinos Cortez, Huascar and Ferreira, Caue and Marcondes, Cesar A. C. and Loubach, Denis S. and Sbruzzi, Elton F. and Verri, Filipe A. N. and Marques, Johnny C. and Pereira, Lourenço A. and Maximo, Marcos R. O. A. and Curtis, Vitor V.},
  booktitle        = {2021 Latin American Robotics Symposium (LARS), 2021 Brazilian Symposium on Robotics (SBR), and 2021 Workshop on Robotics in Education (WRE)},
  title            = {Autonomous and Collective Intelligence for UAV Swarm in Target Search Scenario},
  year             = {2021},
  note             = {ISSN: 2643-685X},
  pages            = {72--77},
  abstract         = {Unmanned Aerial Vehicle (UAV) swarm, also named drone swarm, has been the study object of many types of research due to its potential to improve applications such as monitoring, surveillance, and search missions. With several drones flying simultaneously, the challenge is to increase their level of automation and intelligence while avoiding collision, reducing communication level with these entities, and improving strategical organization to accomplish a specific task. In this sense, we propose a solution to coordinate a UAV swarm using bivariate potential fields with autonomous and distributed intelligence among drones for a cooperative target search application. Results have shown an improvement in the swarm effectiveness by reducing the number of UAVs blocked at local minima by using distributed decision-making methods, proving to be an effective approach to solve this frequent problem in potential fields.},
  code             = {https://github.com/luizgiacomossi/Search_Drone_Swarms},
  comment          = {python开源代码， tree environment search
环境设置：38x16 grids (10x10m/grid)
每架无人机可以识别20m以内的目标},
  creationdate     = {2022-11-02T11:08:51},
  doi              = {10.1109/LARS/SBR/WRE54079.2021.9605450},
  file             = {:Giacomossi_2021_Autonomous and Collective Intelligence for UAV Swarm in Target Search Scenario.pdf:PDF},
  groups           = {Swarm Intelligence},
  issn             = {2643-685X},
  keywords         = {Automation, Robot kinematics, Surveillance, Conferences, Education, Organizations, Collective intelligence, UAV Swarm, Target Search, potential fields, Swarm intelligence},
  modificationdate = {2023-01-07T21:59:35},
}

 
@InProceedings{Li2022FOCAL,
  author           = {Li, Lanqing and Yang, Rui and Luo, Dijun},
  booktitle        = {International Conference on Learning Representations},
  title            = {{FOCAL}: {Efficient} {Fully}-{Offline} {Meta}-{Reinforcement} {Learning} via {Distance} {Metric} {Learning} and {Behavior} {Regularization}},
  year             = {2022},
  month            = feb,
  abstract         = {We study the offline meta-reinforcement learning (OMRL) problem, a paradigm which enables reinforcement learning (RL) algorithms to quickly adapt to unseen tasks without any interactions with the environments, making RL truly practical in many real-world applications. This problem is still not fully understood, for which two major challenges need to be addressed. First, offline RL usually suffers from bootstrapping errors of out-of-distribution state-actions which leads to divergence of value functions. Second, meta-RL requires efficient and robust task inference learned jointly with control policy. In this work, we enforce behavior regularization on learned policy as a general approach to offline RL, combined with a deterministic context encoder for efficient task inference. We propose a novel negative-power distance metric on bounded context embedding space, whose gradients propagation is detached from the Bellman backup. We provide analysis and insight showing that some simple design choices can yield substantial improvements over recent approaches involving meta-RL and distance metric learning. To the best of our knowledge, our method is the first model-free and end-to-end OMRL algorithm, which is computationally efficient and demonstrated to outperform prior algorithms on several meta-RL benchmarks.},
  creationdate     = {2022-11-02T14:37:42},
  file             = {:Li_2022_FOCAL_ Efficient Fully Offline Meta Reinforcement Learning Via Distance Metric Learning and Behavior Regularization.pdf:PDF},
  groups           = {Task Similarity},
  language         = {en},
  modificationdate = {2022-11-04T20:53:13},
  shorttitle       = {{FOCAL}},
  url              = {https://openreview.net/forum?id=8cpHIfgY4Dj},
  urldate          = {2022-11-02},
}

@PhdThesis{SchroederdeWitt2021Coordination,
  author           = {Schroeder de Witt, CA},
  school           = {University of Oxford},
  title            = {Coordination and communication in deep multi-agent reinforcement learning},
  year             = {2021},
  abstract         = {A growing number of real-world control problems require teams of software agents to solve a joint task through cooperation. Such tasks naturally arise whenever human workers are replaced by machines, such as robot arms in manufacturing or autonomous cars in transportation. At the same time, new technologies have given rise to novel cooperative control problems that are beyond human reach, such as in package routing.
Be it for physical constraints such as partial observability, robustness requirements, or to manage large joint action spaces, cooperative agents are often required to function in a fully decentralised fashion. This means that each agent merely has access to its own local sensory input during task execution, and does not have explicit communication channels to other agents. Deep multi-agent reinforcement learning (DMARL) is a natural framework for learning control policies in such settings. When trained in simulation or in a laboratory, learning algorithms often have access to additional information that will not be available at execution. Such centralised training with decentralised execution (CTDE) poses a number of technical challenges to DMARL algorithms that try to exploit the centralised setting in order to facilitate the training of decentralised policies. These difficulties arise primarily from the apparent incongruency between joint policy learning, which can learn arbitrary policies but is not naively decentralisable and scales poorly with the number of agents, and independent learning, which is readily decentralisable and scalable but provably less expressive and prone to environment non-stationarity due to the presence other of learning agents.
The first part of this thesis develops algorithms that use the technique of value decomposition in order to exploit the centralised training of decentralised policies. In Monotonic Value Factorisation for Deep Multi-Agent Reinforcement Learning, we introduce the novel Q-learning algorithm QMIX. QMIX uses a centralised monotonic mixing network in order to model joint team action-value functions that are nevertheless decomposable into decentralised agent policies over discrete action spaces. To evaluate the performance of QMIX, we develop a novel benchmark suite, the StarCraft Multi-Agent Challenge (SMAC), which features a variety of discrete-action cooperative control tasks in StarCraft II unit micromanagement. Unlike pre-existing toy environments, SMAC scenarios feature diverse dynamics owing to a large number of different unit types and sophisticated in-built enemy heuristics. Many robotic control tasks feature continuous action spaces. To extend value decomposition to those settings, in FACMAC: Factored Multi-Agent Centralised Policy Gradients, we focus on actor-critic approaches to multi-agent learning in CTDE settings. The resulting learning algorithm, FACMAC, achieves state-of-the-art performance on SMAC and opens the door toward using nonmonotonic critic factorisations. Just as for QMIX, we introduce a novel benchmark suite for cooperative continuous control tasks, Multi-Agent Mujoco (MAMujoco). MAMujoco decomposes robots from the popular Mujoco benchmark suite into multiple agents with configurable partial observability constraints.
The second part of this thesis explores the value of common knowledge as a resource for both coordinating and communicating through actions. Common knowledge between groups of agents arises in a large class of tasks of practical interest, for example, if agents can recognize each other in overlapping fields of view. In Multi-Agent Common Knowledge Reinforcement Learning, we introduce a novel actor-critic method, MACKRL, which constructs a hierarchy of controllers over common knowledge across agent groups of varying sizes. This hierarchy gives rise to a decentralized policy structure that effectuates a joint-independent hybrid policy which executes decentralized joint policies or falls back to independent policies depending on whether the common knowledge between agent groups is sufficiently informative for action coordination. In this way, MACKRL enjoys the coordinative advantage of joint policy training while being fully decentralised.
The third part of thesis investigates how to learn efficient implicit communication protocols for collaborative tasks. In Communicating via Markov Decision Processes, we explore how a sender agent can execute a task optimally while at the same time communicating information to a receiver agent solely through its actions. In this novel implicit referential game, both sender and receiver agents commonly know both the sender policy, as well as the sender’s trajectory. By splitting the sender task into a single-agent maximum entropy reinforcement learning task and a separate message encoding step based on minimum-entropy coupling, we show that our method GME allows establishing communication channels of significantly higher bandwidth than those trained end-to-end.
In summary, this thesis presents a number of significant contributions deep multi-agent reinforcement for cooperative control within the framework of centralised training with decentralised execution and two associated novel benchmark suites. Within this setting, we make contributions to value decomposition, the use of common knowledge in multi-agent learning, and how to learn implicit communication protocols efficiently.},
  creationdate     = {2022-11-02T20:14:54},
  file             = {:Schroeder de Witt_2021_Coordination and Communication in Deep Multi Agent Reinforcement Learning.pdf:PDF},
  groups           = {MARL},
  keywords         = {communication, coordination, deep multi-agent reinforcement learning},
  modificationdate = {2023-01-07T21:59:38},
  publisher        = {University of Oxford},
}

 
@Article{Garcia2022Taxonomy,
  author           = {García, Javier and Visús, Álvaro and Fernández, Fernando},
  journal          = {Machine Learning},
  title            = {A taxonomy for similarity metrics between {Markov} decision processes},
  year             = {2022},
  issn             = {1573-0565},
  month            = oct,
  abstract         = {Although the notion of task similarity is potentially interesting in a wide range of areas such as curriculum learning or automated planning, it has mostly been tied to transfer learning. Transfer is based on the idea of reusing the knowledge acquired in the learning of a set of source tasks to a new learning process in a target task, assuming that the target and source tasks are close enough. In recent years, transfer learning has succeeded in making reinforcement learning (RL) algorithms more efficient (e.g., by reducing the number of samples needed to achieve (near-)optimal performance). Transfer in RL is based on the core concept of similarity: whenever the tasks are similar, the transferred knowledge can be reused to solve the target task and significantly improve the learning performance. Therefore, the selection of good metrics to measure these similarities is a critical aspect when building transfer RL algorithms, especially when this knowledge is transferred from simulation to the real world. In the literature, there are many metrics to measure the similarity between MDPs, hence, many definitions of similarity or its complement distance have been considered. In this paper, we propose a categorization of these metrics and analyze the definitions of similarity proposed so far, taking into account such categorization. We also follow this taxonomy to survey the existing literature, as well as suggesting future directions for the construction of new metrics.},
  comment          = {MDP相似性度量的综述},
  creationdate     = {2022-11-03T21:57:04},
  doi              = {10.1007/s10994-022-06242-4},
  file             = {:García_2022_A Taxonomy for Similarity Metrics between Markov Decision Processes.pdf:PDF},
  groups           = {Task Similarity},
  keywords         = {Markov decision processes, Similarity metrics, Transfer learning},
  language         = {en},
  modificationdate = {2022-11-06T14:24:47},
  urldate          = {2022-11-03},
}

 
@Misc{Visus2021Taxonomy,
  author           = {Visús, Álvaro and García, Javier and Fernández, Fernando},
  month            = mar,
  note             = {arXiv:2103.04706 [cs] type: article},
  title            = {A {Taxonomy} of {Similarity} {Metrics} for {Markov} {Decision} {Processes}},
  year             = {2021},
  abstract         = {Although the notion of task similarity is potentially interesting in a wide range of areas such as curriculum learning or automated planning, it has mostly been tied to transfer learning. Transfer is based on the idea of reusing the knowledge acquired in the learning of a set of source tasks to a new learning process in a target task, assuming that the target and source tasks are close enough. In recent years, transfer learning has succeeded in making Reinforcement Learning (RL) algorithms more efficient (e.g., by reducing the number of samples needed to achieve the (near-)optimal performance). Transfer in RL is based on the core concept of similarity: whenever the tasks are similar, the transferred knowledge can be reused to solve the target task and significantly improve the learning performance. Therefore, the selection of good metrics to measure these similarities is a critical aspect when building transfer RL algorithms, especially when this knowledge is transferred from simulation to the real world. In the literature, there are many metrics to measure the similarity between MDPs, hence, many definitions of similarity or its complement distance have been considered. In this paper, we propose a categorization of these metrics and analyze the definitions of similarity proposed so far, taking into account such categorization. We also follow this taxonomy to survey the existing literature, as well as suggesting future directions for the construction of new metrics.},
  annote           = {Comment: 9 pages, submitted to IJCAI},
  creationdate     = {2022-11-03T21:58:11},
  doi              = {10.48550/arXiv.2103.04706},
  file             = {:Visús_2021_A Taxonomy of Similarity Metrics for Markov Decision Processes.pdf:PDF},
  groups           = {Task Similarity},
  keywords         = {Computer Science - Machine Learning},
  modificationdate = {2022-11-06T14:24:47},
  school           = {arXiv},
  urldate          = {2022-11-03},
}

 
@InProceedings{Patel2019Mixed,
  author           = {Patel, Jayam and Xu, Yicong and Pinciroli, Carlo},
  booktitle        = {2019 International Conference on Robotics and Automation (ICRA)},
  title            = {Mixed-{Granularity} {Human}-{Swarm} {Interaction}},
  year             = {2019},
  note             = {ISSN: 2577-087X},
  pages            = {1059--1065},
  abstract         = {We present an augmented reality human-swarm interface that combines two modalities of interaction: environment-oriented and robot-oriented. The environment-oriented modality allows the user to modify the environment (either virtual or physical) to indicate a goal to attain for the robot swarm. The robot-oriented modality makes it possible to select individual robots to reassign them to other tasks to increase performance or remedy failures. Previous research has concluded that environment-oriented interaction might prove more difficult to grasp for untrained users. In this paper, we report a user study which indicates that, at least in collective transport, environment-oriented interaction is more effective than purely robot-oriented interaction, and that the two combined achieve remarkable efficacy.},
  comment          = {ICRA B会
人在app上操作虚拟物体/robot，来控制实际的物体/robot，来完成运输任务。分析了两种交互模式(面向robot，面向环境)，在真实机器人上进行实验，得出结论：允许两种交互模式可以减少交互次数},
  creationdate     = {2022-11-03T23:13:05},
  doi              = {10.1109/ICRA.2019.8793261},
  file             = {:Patel_2019_Mixed Granularity Human Swarm Interaction.pdf:PDF},
  groups           = {Human-Swarm},
  issn             = {2577-087X},
  keywords         = {Task analysis, Robot kinematics, Augmented reality, Cognitive science, Navigation, Engines},
  modificationdate = {2023-01-07T22:05:27},
}

@Article{Roundtree2019Visualization,
  author           = {Karina A. Roundtree and Jason R. Cody and Jennifer Leaf and H. Onan Demirel and Julie A. Adams},
  journal          = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
  title            = {Visualization Design for Human-Collective Teams},
  year             = {2019},
  month            = nov,
  number           = {1},
  pages            = {417--421},
  volume           = {63},
  abstract         = {Robotic collectives (i.e., colonies and swarms) are applicable to a wide range of applications, including environmental monitoring, search and rescue, as well as infrastructure monitoring. The presented evaluation focuses on how two visualization designs impact human-collective team performance during a best-of-n sequential decision making task with colonies of 200 agents. Traditional visualizations present all the individual robots that encompass the entirety of the collective, which may cause the human operator to suffer from information overload which hinders understanding the collective’s current state, the reasoning behind actions, and associated predictive future outcomes. Interface designs that abstract the individual collective member details and present the collective’s state are needed to alleviate high workload and mitigate human error. The evaluation determined that an abstract visualization of the collective’s state produced better overall performance than the visualization that showed all the individual agents.},
  comment          = {着重可视化设计，为了降低信息负载，如何将大规模群体的信息进行抽象并可视化},
  creationdate     = {2022-11-04T09:24:48},
  doi              = {10.1177/1071181319631028},
  file             = {:Roundtree_2019_Visualization Design for Human Collective Teams.pdf:PDF},
  groups           = {Human-Swarm},
  modificationdate = {2023-04-04T16:06:42},
  publisher        = {{SAGE} Publications},
}

@Article{Roundtree2018Analysis,
  author           = {Karina A. Roundtree and Matthew D. Manning and Julie A. Adams},
  journal          = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
  title            = {Analysis of Human-Swarm Visualizations},
  year             = {2018},
  month            = sep,
  number           = {1},
  pages            = {287--291},
  volume           = {62},
  abstract         = {Interest in robotic swarms has increased exponentially. Prior research determined that humans perceive biological swarm motions as a single entity, rather than perceiving the individuals. An open question is how the swarm’s visual representation and the associated task impact human performance when identifying current swarm tasks. The majority of the existing swarm visualizations present each robot individually. Swarms typically incorporate large numbers of individuals, where the individuals exhibit simple behaviors, but the swarm appears to exhibit more intelligent behavior. As the swarm size increases, it becomes increasingly difficult for the human operator to understand the swarm’s current state, the emergent behaviors, and predict future outcomes. Alternative swarm visualizations are one means of mitigating high operator workload and risk of human error. Five visualizations were evaluated for two tasks, go to and avoid, in the presence or absence of obstacles. The results indicate that visualizations incorporating representations of individual agents resulted in higher accuracy when identifying tasks.},
  creationdate     = {2022-11-04T09:31:51},
  doi              = {10.1177/1541931218621066},
  file             = {:Roundtree_2018_Analysis of Human Swarm Visualizations.pdf:PDF},
  groups           = {Human-Swarm},
  modificationdate = {2023-04-04T16:06:44},
  publisher        = {{SAGE} Publications},
}

@Article{Baxter2022Premise,
  author           = {Daniel P. Baxter and Adam J. Hepworth and Keith F. Joiner and Hussein Abbass},
  journal          = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
  title            = {On the Premise of a Swarm Guidance Ontology for Human-Swarm Teaming},
  year             = {2022},
  month            = sep,
  number           = {1},
  pages            = {2249--2253},
  volume           = {66},
  abstract         = {Effective Human-Swarm Teaming (HST) relies on bi-directional information flow between the human and the swarm. Systems with human control or oversight rely on information flow from the swarm to the humans to inform decisions, while information that flows back from humans is only that necessary for actuation, which remains primarily physical. To unlock the full potential of HSTs, the augmentation must extend into the overall logic of teaming, including both the human’s and machine’s cognitive domains, whereby an AI-equipped robot teammate is capable of complex cognitive functions. The effectiveness of HST will need a sufficient level of transparency in the interaction space formed by the bi-directional information flow between the human and the swarm. This transparency must continuously and constructively interpret the information exchanged between the human and the swarm to afford both cognitive agents with the capacity to form shared understanding and situation awareness, and thus, facilitating effective teaming through trust. An ontology is one formal representational construct that enables bi-directional interpretation, thus, transparency. In this paper, we conceptualise and present a meta-ontology for transparent HST interactions.},
  comment          = {偏重理论总结},
  creationdate     = {2022-11-04T09:43:04},
  doi              = {10.1177/1071181322661541},
  file             = {:Baxter_2022_On the Premise of a Swarm Guidance Ontology for Human Swarm Teaming.pdf:PDF},
  groups           = {Human-Swarm},
  modificationdate = {2023-04-04T16:06:46},
  publisher        = {{SAGE} Publications},
}

@InProceedings{Rockbach2022Towards,
  author           = {Jonas D. Rockbach and Luka-Franziska Bluhm and Maren Bennewitz},
  booktitle        = {The 2022 Conference on Artificial Life},
  title            = {Towards Hierarchical Hybrid Architectures for Human-Swarm Interaction},
  year             = {2022},
  publisher        = {{MIT} Press},
  comment          = {讨论了人类认知如何与分布式智能群体结合
how a JHSL can be formalized as nested interaction matrices
人的态势感知能力强，但是反应时间慢；swarm相反},
  creationdate     = {2022-11-04T09:44:58},
  doi              = {10.1162/isal_a_00532},
  file             = {:Rockbach_2022_Towards Hierarchical Hybrid Architectures for Human Swarm Interaction.pdf:PDF},
  groups           = {Human-Swarm},
  modificationdate = {2022-11-23T22:11:29},
  url              = {https://direct.mit.edu/isal/proceedings/isal/34/74/112299},
}

@Article{Soorati2021Designing,
  author           = {Mohammad Divband Soorati and Jediah Clark and Javad Ghofrani and Danesh Tarapore and Sarvapali D. Ramchurn},
  journal          = {Drones},
  title            = {Designing a User-Centered Interaction Interface for Human{\textendash}Swarm Teaming},
  year             = {2021},
  month            = nov,
  number           = {4},
  pages            = {131},
  volume           = {5},
  abstract         = {A key challenge in human–swarm interaction is to design a usable interface that allows the human operators to monitor and control a scalable swarm. In our study, we restrict the interactions to only one-to-one communications in local neighborhoods between UAV-UAV and operator-UAV. This type of proximal interactions will decrease the cognitive complexity of the human–swarm interaction to O(1). In this paper, a user study with 100 participants provides evidence that visualizing a swarm as a heat map is more effective in addressing usability and acceptance in human–swarm interaction. We designed an interactive interface based on the users’ preference and proposed a controlling mechanism that allows a human operator to control a large swarm of UAVs. We evaluated the proposed interaction interface with a complementary user study. Our testbed and results establish a benchmark to study human–swarm interaction where a scalable swarm can be managed by a single operator.},
  comment          = {可以参考这篇文章

MDPI
单个operator如何控制多个uav机群
无人机用于应急场景下的快速态势感知，但往往依赖于多个operator来操作vehicle
先通过100个人的实验，来确定热力图更适合交互，然后用热力图构建了一个界面；地图是600x600分辨率的，但是agent只有一个低分辨率图40x40
human不需要与所有uav通信，就能控制机群

结论到底是什么？
相比直接与个体进行交互，使用maps进行交互，可以减少交互次数；},
  creationdate     = {2022-11-04T10:49:09},
  doi              = {10.3390/drones5040131},
  file             = {:Soorati_2021_Designing a User Centered Interaction Interface for Human_Swarm Teaming.pdf:PDF},
  groups           = {Human-Swarm},
  modificationdate = {2023-04-04T16:06:48},
  publisher        = {{MDPI} {AG}},
}

 
@InProceedings{Ashcraft2019Moderating,
  author           = {Ashcraft, C. Chace and Goodrich, Michael A. and Crandall, Jacob W.},
  booktitle        = {2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)},
  title            = {Moderating Operator Influence in Human-Swarm Systems},
  year             = {2019},
  note             = {ISSN: 2577-1655},
  pages            = {4275--4282},
  abstract         = {In human-swarm systems, human input to a robot swarm can both inhibit desirable swarm behaviors and allow the operator to properly guide the swarm to achieve mission goals. Indeed, the way that control is shared between the human operator and the inherent collective robot behaviors determines in large part the success of the human-swarm system. In this paper, we seek to understand how to design human-swarm systems that effectively moderate human influence over a robot swarm. To do this, we implement a simulated swarm system based on honeybees, and study how interacting with this swarm using various methods of moderating human influence impacts the success of the resulting human-swarm system. Our results demonstrate that moderating human influence is essential to achieving effective human-swarm systems, and highlight the need for future work in determining how to better moderate human influence in human-swarm systems.},
  creationdate     = {2022-11-04T10:50:10},
  doi              = {10.1109/SMC.2019.8914592},
  file             = {:Ashcraft_2019_Moderating Operator Influence in Human Swarm Systems.pdf:PDF},
  groups           = {Human-Swarm},
  issn             = {2577-1655},
  keywords         = {Biological system modeling, Task analysis, Robot sensing systems, Computational modeling, Adaptation models, Control systems},
  modificationdate = {2023-01-07T22:05:04},
  ranking          = {rank5},
}

@Article{Vasarhelyi2019Optimized,
  author           = {G{\'{a}}bor V{\'{a}}s{\'{a}}rhelyi and Csaba Vir{\'{a}}gh and Gerg{\H{o}} Somorjai and Tam{\'{a}}s Nepusz and Agoston E. Eiben and Tam{\'{a}}s Vicsek},
  journal          = {Science Robotics},
  title            = {Optimized flocking of autonomous drones in confined environments},
  year             = {2018},
  month            = jul,
  number           = {20},
  volume           = {3},
  abstract         = {We address a fundamental issue of collective motion of aerial robots: how to ensure that large flocks of autonomous drones seamlessly navigate in confined spaces. The numerous existing flocking models are rarely tested on actual hardware because they typically neglect some crucial aspects of multirobot systems. Constrained motion and communication capabilities, delays, perturbations, or the presence of barriers should be modeled and treated explicitly because they have large effects on collective behavior during the cooperation of real agents. Handling these issues properly results in additional model complexity and a natural increase in the number of tunable parameters, which calls for appropriate optimization methods to be coupled tightly to model development. In this paper, we propose such a flocking model for real drones incorporating an evolutionary optimization framework with carefully chosen order parameters and fitness functions. We numerically demonstrated that the induced swarm behavior remained stable under realistic conditions for large flock sizes and notably for large velocities. We showed that coherent and realistic collective motion patterns persisted even around perturbing obstacles. Furthermore, we validated our model on real hardware, carrying out field experiments with a self-organized swarm of 30 drones. This is the largest of such aerial outdoor systems without central control reported to date exhibiting flocking with collective collision and object avoidance. The results confirmed the adequacy of our approach. Successfully controlling dozens of quadcopters will enable substantially more efficient task management in various contexts involving drones.},
  comment          = {中科院一区sci 考虑了硬件约束，大集群狭小空间避障导航},
  copyright        = {Creative Commons Zero v1.0 Universal},
  creationdate     = {2022-11-04T20:00:07},
  doi              = {10.1126/scirobotics.aat3536},
  file             = {:Vásárhelyi_2018_Optimized Flocking of Autonomous Drones in Confined Environments.pdf:PDF},
  groups           = {Swarm Intelligence},
  keywords         = {swarm, self-organized, evolutionary optimization, flocking, Collective motion, collective object avoidance, drone, agent based},
  language         = {en},
  modificationdate = {2023-04-04T16:06:54},
  publisher        = {American Association for the Advancement of Science ({AAAS})},
}

@Article{Ji2021QLearning,
  author           = {Ji, Jian-Jiao and Guo, Yi-Nan and Gao, Xiao-Zhi and Gong, Dun-Wei and Wang, Ya-Peng},
  journal          = {IEEE Transactions on Cybernetics},
  title            = {Q-Learning-Based Hyperheuristic Evolutionary Algorithm for Dynamic Task Allocation of Crowdsensing},
  year             = {2021},
  pages            = {1--14},
  comment          = {Q-learning如何用于启发式算法？},
  creationdate     = {2022-09-27T16:20:14},
  doi              = {10.1109/TCYB.2021.3112675},
  file             = {:Ji_2021_Q Learning Based Hyperheuristic Evolutionary Algorithm for Dynamic Task Allocation of Crowdsensing.pdf:PDF},
  groups           = {RL-based},
  modificationdate = {2022-11-05T10:46:48},
  ranking          = {rank5},
}

 
@Article{Li2021TA,
  author           = {Li, Mincan and Wang, Zidong and Li, Kenli and Liao, Xiangke and Hone, Kate and Liu, Xiaohui},
  journal          = {IEEE Transactions on Evolutionary Computation},
  title            = {Task {Allocation} on {Layered} {Multiagent} {Systems}: {When} {Evolutionary} {Many}-{Objective} {Optimization} {Meets} {Deep} {Q}-{Learning}},
  year             = {2021},
  issn             = {1941-0026},
  number           = {5},
  pages            = {842--855},
  volume           = {25},
  abstract         = {This article is concerned with the multitask multiagent allocation problem via many-objective optimization for multiagent systems (MASs). First, a novel layered MAS model is constructed to address the multitask multiagent allocation problem that includes both the original task simplification and the many-objective allocation. In the first layer of the model, the deep Q-learning method is introduced to simplify the prioritization of the original task set. In the second layer of the model, the modified shift-based density estimation (MSDE) method is put forward to improve the conventional strength Pareto evolutionary algorithm 2 (SPEA2) in order to achieve many-objective optimization on task assignments. Then, an MSDE-SPEA2-based method is proposed to tackle the many-objective optimization problem with objectives including task allocation, makespan, agent satisfaction, resource utilization, task completion, and task waiting time. As compared with the existing allocation methods, the developed method in this article exhibits an outstanding feature that the task assignment and the task scheduling are carried out simultaneously. Finally, extensive experiments are conducted to: 1) verify the validity of the proposed model and the effectiveness of two main algorithms and 2) illustrate the optimal solution for task allocation and efficient strategy for task scheduling under different scenarios.},
  creationdate     = {2022-09-27T16:22:43},
  doi              = {10.1109/TEVC.2021.3049131},
  file             = {:Li_2021_Task Allocation on Layered Multiagent Systems_ When Evolutionary Many Objective Optimization Meets Deep Q Learning.pdf:PDF},
  groups           = {RL-based},
  keywords         = {Task analysis, Resource management, Optimization, Multi-agent systems, Heuristic algorithms, Estimation, Deep learning, Deep Q-learning (DQL), evolutionary computation, many-objective optimization, multiagent systems (MAS), task allocation},
  modificationdate = {2022-11-05T10:46:48},
  shorttitle       = {Task {Allocation} on {Layered} {Multiagent} {Systems}},
}

 
@InProceedings{Ye2021TA,
  author           = {Ye, Guanyu and Zhao, Yan and Chen, Xuanhao and Zheng, Kai},
  booktitle        = {Proceedings of the 30th {ACM} {International} {Conference} on {Information} \& {Knowledge} {Management}},
  title            = {Task {Allocation} with {Geographic} {Partition} in {Spatial} {Crowdsourcing}},
  year             = {2021},
  address          = {New York, NY, USA},
  pages            = {2404--2413},
  publisher        = {Association for Computing Machinery},
  series           = {{CIKM} '21},
  abstract         = {Recent years have witnessed a revolution in Spatial Crowdsourcing (SC), in which people with mobile connectivity can perform spatio-temporal tasks that involve travel to specified locations. In this paper, we identify and study in depth a new multi-center-based task allocation problem in the context of SC, where multiple allocation centers exist. In particular, we aim to maximize the total number of the allocated tasks while minimizing the average allocated task number difference. To solve the problem, we propose a two-phase framework, called Task Allocation with Geographic Partition, consisting of a geographic partition phase and a task allocation phase. The first phase is to divide the whole study area based on the allocation centers by using both a basic Voronoi diagram-based algorithm and an adaptive weighted Voronoi diagram-based algorithm. In the allocation phase, we utilize a Reinforcement Learning method to achieve the task allocation, where a graph neural network with the attention mechanism is used to learn the embeddings of allocation centers, delivery points and workers. Extensive experiments give insight into the effectiveness and efficiency of the proposed solutions.},
  creationdate     = {2022-09-27T16:24:15},
  doi              = {10.1145/3459637.3482300},
  file             = {:Ye_2021_Task Allocation with Geographic Partition in Spatial Crowdsourcing.pdf:PDF},
  groups           = {Spatial},
  isbn             = {9781450384469},
  keywords         = {reinforcement learning, spatial crowdsourcing, task allocation, geographic partition},
  modificationdate = {2022-11-05T10:46:39},
  url              = {https://doi.org/10.1145/3459637.3482300},
  urldate          = {2022-09-27},
}

@Misc{Ferreira2021Distributed,
  author           = {Barbara Arbanas Ferreira and Tamara Petrović and Matko Orsag and J. Ramiro Martínez-de-Dios and Stjepan Bogdan},
  month            = sep,
  title            = {Distributed Allocation and Scheduling of Tasks with Cross-Schedule Dependencies for Heterogeneous Multi-Robot Teams},
  year             = {2021},
  abstract         = {To enable safe and efficient use of multi-robot systems in everyday life, a robust and fast method for coordinating their actions must be developed. In this paper, we present a distributed task allocation and scheduling algorithm for missions where the tasks of different robots are tightly coupled with temporal and precedence constraints. The approach is based on representing the problem as a variant of the vehicle routing problem, and the solution is found using a distributed metaheuristic algorithm based on evolutionary computation (CBM-pop). Such an approach allows a fast and near-optimal allocation and can therefore be used for online replanning in case of task changes. Simulation results show that the approach has better computational speed and scalability without loss of optimality compared to the state-of-the-art distributed methods. An application of the planning procedure to a practical use case of a greenhouse maintained by a multi-robot system is given.},
  archiveprefix    = {arXiv},
  code             = {https://github.com/barbara0811/cbm_pop_mdvrp_optimization},
  creationdate     = {2022-09-27T16:27:12},
  eprint           = {2109.03089},
  file             = {:Ferreira_2021_Distributed Allocation and Scheduling of Tasks with Cross Schedule Dependencies for Heterogeneous Multi Robot Teams.pdf:PDF},
  groups           = {Precedence},
  keywords         = {cs.RO, cs.AI},
  modificationdate = {2022-11-05T10:46:56},
  primaryclass     = {cs.RO},
}

@Misc{Agrawal2022DC-MRTA,
  author           = {Aakriti Agrawal and Senthil Hariharan and Amrit Singh Bedi and Dinesh Manocha},
  month            = sep,
  title            = {DC-MRTA: Decentralized Multi-Robot Task Allocation and Navigation in Complex Environments},
  year             = {2022},
  abstract         = {We present a novel reinforcement learning (RL) based task allocation and decentralized navigation algorithm for mobile robots in warehouse environments. Our approach is designed for scenarios in which multiple robots are used to perform various pick up and delivery tasks. We consider the problem of joint decentralized task allocation and navigation and present a two level approach to solve it. At the higher level, we solve the task allocation by formulating it in terms of Markov Decision Processes and choosing the appropriate rewards to minimize the Total Travel Delay (TTD). At the lower level, we use a decentralized navigation scheme based on ORCA that enables each robot to perform these tasks in an independent manner, and avoid collisions with other robots and dynamic obstacles. We combine these lower and upper levels by defining rewards for the higher level as the feedback from the lower level navigation algorithm. We perform extensive evaluation in complex warehouse layouts with large number of agents and highlight the benefits over state-of-the-art algorithms based on myopic pickup distance minimization and regret-based task selection. We observe improvement up to 14% in terms of task completion time and up-to 40% improvement in terms of computing collision-free trajectories for the robots.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-09-27T16:28:28},
  eprint           = {2209.02865},
  file             = {:Agrawal_2022_DC MRTA_ Decentralized Multi Robot Task Allocation and Navigation in Complex Environments.pdf:PDF},
  groups           = {RL-based},
  journal          = {IROS-2022},
  keywords         = {cs.RO, cs.LG, cs.MA},
  modificationdate = {2022-11-05T10:46:48},
  primaryclass     = {cs.RO},
}

@Article{Lee2022TA,
  author           = {Meng-Lun Lee and Sara Behdad and Xiao Liang and Minghui Zheng},
  journal          = {Robotics and Computer-Integrated Manufacturing},
  title            = {Task allocation and planning for product disassembly with human{\textendash}robot collaboration},
  year             = {2022},
  month            = aug,
  pages            = {102306},
  volume           = {76},
  comment          = {偏实际应用，实验场景是废弃硬盘拆卸},
  creationdate     = {2022-09-27T16:30:12},
  doi              = {10.1016/j.rcim.2021.102306},
  file             = {:Lee_2022_Task Allocation and Planning for Product Disassembly with Human_robot Collaboration.pdf:PDF},
  groups           = {Task Assignment},
  modificationdate = {2023-04-04T16:06:55},
  publisher        = {Elsevier {BV}},
  qualityassured   = {qualityAssured},
  ranking          = {rank1},
}

@InProceedings{Wu2022TA,
  author           = {Haochen Wu and Amin Ghadami and Alparslan Emrah Bayrak and Jonathon M. Smereka and Bogdan I. Epureanu},
  booktitle        = {2022 International Conference on Robotics and Automation ({ICRA})},
  title            = {Task Allocation with Load Management in Multi-Agent Teams},
  year             = {2022},
  month            = may,
  publisher        = {{IEEE}},
  creationdate     = {2022-09-27T16:30:40},
  doi              = {10.1109/icra46639.2022.9811374},
  file             = {:Wu_2022_Task Allocation with Load Management in Multi Agent Teams.pdf:PDF},
  groups           = {RL-based},
  modificationdate = {2023-04-04T16:06:57},
}

@Article{Geng2021Particle,
  author           = {Na Geng and Zhiting Chen and Quang A. Nguyen and Dunwei Gong},
  journal          = {Complex \& Intelligent Systems},
  title            = {Particle swarm optimization algorithm for the optimization of rescue task allocation with uncertain time constraints},
  year             = {2021},
  month            = jan,
  number           = {2},
  pages            = {873--890},
  volume           = {7},
  creationdate     = {2022-09-27T16:32:00},
  doi              = {10.1007/s40747-020-00252-2},
  file             = {:Geng_2021_Particle Swarm Optimization Algorithm for the Optimization of Rescue Task Allocation with Uncertain Time Constraints.pdf:PDF},
  groups           = {Task Assignment},
  modificationdate = {2023-04-04T16:06:59},
  publisher        = {Springer Science and Business Media {LLC}},
}

@Article{Wang2022Cooperative,
  author           = {Shengli Wang and Youjiang Liu and Yongtao Qiu and Qi Zhang and Feixiang Huo and Yafan Huangfu and Chun Yang and Jie Zhou},
  journal          = {{IEEE} Access},
  title            = {Cooperative Task Allocation for Multi-Robot Systems Based on Multi-Objective Ant Colony System},
  year             = {2022},
  pages            = {56375--56387},
  volume           = {10},
  abstract         = {This paper proposes a novel multi-objective ant colony system (MOACS) approach to solve the cooperative task allocation problem of multi-robot systems. The task allocation problem is formulated as a multi-objective multiple traveling salesman problem (MTSP). The objectives are to minimize the total and maximum cost of the robotic vehicles so that the workload of each vehicle could be balanced. The time cost matrices of the salesmen are different and asymmetric due to the different flight speeds of vehicles and executing time of tasks. Based on the single-objective ant colony system (ACS), a novel solution construction method and a novel pheromone update rule are proposed. At each step in the solution construction phase, the ant with minimum cost has the biggest chance to add an unassigned task to balance the workload of each vehicle, while the ant with maximum cost also has a bigger chance than any other ants to add an unassigned task to find better Pareto front. The minimum value of the pheromone is limited in the pheromone update phase, which is helpful in avoiding fast convergence and local optima. Extensive simulation results suggest that the proposed MOACS has better performance and effectiveness than the existing non-dominated sorting genetic algorithm II (NSGA-II) and multi-objective particle swarm optimization (MOPSO). Hardware-in-the-loop experiments on multiple unmanned aerial vehicles (UAVs) also show that compared with NSGA-II and MOPSO, the maximum and total flight distance of the UAVs with the proposed MOACS are decreased by up to 28.46% and 26.34%, respectively, while the maximum and total time used to finish all tasks are decreased by up to 23.86% and 17.94%.},
  creationdate     = {2022-09-27T21:13:30},
  doi              = {10.1109/access.2022.3165198},
  file             = {:Wang_2022_Cooperative Task Allocation for Multi Robot Systems Based on Multi Objective Ant Colony System.pdf:PDF},
  groups           = {Task Assignment},
  modificationdate = {2022-11-05T10:08:59},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Misc{Amaris2017Generic,
  author           = {Marcos Amaris and Giorgio Lucarelli and Clément Mommessin and Denis Trystram},
  month            = nov,
  title            = {Generic algorithms for scheduling applications on heterogeneous multi-core platforms},
  year             = {2017},
  abstract         = {We study the problem of executing an application represented by a precedence task graph on a parallel machine composed of standard computing cores and accelerators. Contrary to most existing approaches, we distinguish the allocation and the scheduling phases and we mainly focus on the allocation part of the problem: choose the most appropriate type of computing unit for each task. We address both off-line and on-line settings and design generic scheduling approaches. In the first case, we establish strong lower bounds on the worst-case performance of a known approach based on Linear Programming for solving the allocation problem. Then, we refine the scheduling phase and we replace the greedy List Scheduling policy used in this approach by a better ordering of the tasks. Although this modification leads to the same approximability guarantees, it performs much better in practice. We also extend this algorithm to more types of computing units, achieving an approximation ratio which depends on the number of different types. In the on-line case, we assume that the tasks arrive in any, not known in advance, order which respects the precedence relations and the scheduler has to take irrevocable decisions about their allocation and execution. In this setting, we propose the first on-line scheduling algorithm which takes into account precedences. Our algorithm is based on adequate rules for selecting the type of processor where to allocate the tasks and it achieves a constant factor approximation guarantee if the ratio of the number of CPUs over the number of GPUs is bounded. Finally, all the previous algorithms for hybrid architectures have been experimented on a large number of simulations built on actual libraries. These simulations assess the good practical behavior of the algorithms with respect to the state-of-the-art solutions, whenever these exist, or baseline algorithms.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-09-27T21:14:15},
  eprint           = {1711.06433},
  file             = {:Amaris_2017_Generic Algorithms for Scheduling Applications on Heterogeneous Multi Core Platforms.pdf:PDF},
  groups           = {Precedence},
  keywords         = {cs.DC},
  modificationdate = {2022-11-05T10:46:56},
  primaryclass     = {cs.DC},
}

 
@Article{Wang2022TASwarm,
  author           = {Wang, Ziheng and Zhang, Jianlei},
  journal          = {Knowledge-Based Systems},
  title            = {A task allocation algorithm for a swarm of unmanned aerial vehicles based on bionic wolf pack method},
  year             = {2022},
  issn             = {0950-7051},
  month            = aug,
  pages            = {109072},
  volume           = {250},
  abstract         = {Unmanned Aerial Vehicle (UAV) swarms are attracting more and more research attention due to their low cost and high efficiency. Task allocation is a highly important process for a UAV swarm, currently suffering from several constraints such as large scale and real-time requirements, with a possible solution being quite challenging. Hence, this paper models the behavioral characteristics of a wolf pack in its natural environment and exploits it to solve a UAV swarm dynamic task allocation problem in complex scenes. Specifically, the proposed method considers the non-balanced task assignment problem and utilizes the task scheduling algorithm to make the assignment results meet the task performance constraints. Moreover, a path planning algorithm and a coverage search algorithm appropriate for UAV swarms in complex scenes are proposed. Simulation experiments involving different target numbers and random target positions demonstrate that the suggested method achieves high task completion and balances the UAVs’ load affording a suitable solution for complex scenes.},
  comment          = {KBS},
  creationdate     = {2022-09-27T21:15:48},
  doi              = {10.1016/j.knosys.2022.109072},
  groups           = {Task Assignment},
  keywords         = {Bionic wolf pack, UAV swarm, Task allocation, Assignment problem, Coverage search},
  language         = {en},
  modificationdate = {2022-11-05T10:08:59},
  url              = {https://www.sciencedirect.com/science/article/pii/S0950705122005287},
  urldate          = {2022-09-27},
}

 
@Article{Gao2022TA,
  author           = {Gao, Hui and Feng, Jianhao and Xiao, Yu and Zhang, Bo and Wang, Wendong},
  journal          = {IEEE Transactions on Mobile Computing},
  title            = {A {UAV}-assisted {Multi}-task {Allocation} {Method} for {Mobile} {Crowd} {Sensing}},
  year             = {2022},
  issn             = {1558-0660},
  pages            = {1--1},
  abstract         = {Mobile crowd sensing (MCS) has been widely used as a cost-efcient way to collect data for smart cities, which typically starts with participant recruitment and task allocation. Previous work mainly focused on selecting a proper subset of humans for contributing sensing data. However, there often exist situations where humans are not able to reach the target areas, such as traffic jams or accidents. One solution is to complement manual data collection with autonomous data collection using unmanned aerial vehicles (UAVs) equipped with various sensors. In this paper, we focus on the scenarios of UAV-assisted MCS and propose a highly efficient task allocation method, called UMA (UAV-assisted Multi-task Allocation method) to jointly optimize the sensing coverage and data quality. The method incentivizes and guides human participants to contribute high-quality sensing data. Meanwhile, the UAVs are employed to sense data from rarely sensed points of interest, and calibrate data contributed by human participants. The method leverages emerging deep reinforcement learning techniques for directing UAVs sensing and movement actions based on the human participants locations and tasks achievement. The results well justify the efficiency of UMA in terms of coverage completed ratio, calibrating ratio, task fairness and energy efficiency, compared with the state-of-the-art.},
  creationdate     = {2022-09-27T21:16:58},
  doi              = {10.1109/TMC.2022.3147871},
  file             = {:Gao_2022_A UAV Assisted Multi Task Allocation Method for Mobile Crowd Sensing.pdf:PDF},
  groups           = {Spatial},
  keywords         = {Mobile crowd sensing, UAV, multi-task allocation, reinforcement learning},
  modificationdate = {2022-11-05T10:46:39},
}

@InProceedings{Tan2019Computational,
  author           = {Xing Tan and Jimmy Xiangji Huang},
  booktitle        = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence},
  title            = {On Computational Complexity of Pickup-and-Delivery Problems with Precedence Constraints or Time Windows},
  year             = {2019},
  month            = aug,
  publisher        = {International Joint Conferences on Artificial Intelligence Organization},
  abstract         = {Pickup-and-Delivery (PD) problems consider routing vehicles to achieve a set of tasks related to ``Pickup'', and to ``Delivery''. Meanwhile these tasks might subject to Precedence Constraints (PDPC) or Time Windows (PDTW). PD is a variant to Vehicle Routing Problems (VRP), which have been extensively studied for decades. In the recent years, PD demonstrates its closer relevance to AI. With an awareness that few work has been dedicated so far in addressing where the tractability boundary line can be drawn for PD problems, we identify in this paper a set of highly restricted PD problems and prove their NP-completeness. Many problems from a multitude of applications and industry domains are general versions of PDPC. Thus this new result of NP-hardness, of PDPC, not only clarifies the computational complexity of these problems, but also sets up a firm base for the requirement on use of approximation or heuristics, as opposed to looking for exact but intractable algorithms for solving them. We move on to perform an empirical study to locate sources of intractability in PD problems. That is, we propose a local-search formalism and algorithm for solving PDPC problems in particular. Experimental results support strongly effectiveness and efficiency of the local-search. Using the local-search as a solver for randomly generated PDPC problem instances, we obtained interesting and potentially useful insights regarding computational hardness of PDPC and PD.},
  creationdate     = {2022-09-27T21:22:52},
  doi              = {10.24963/ijcai.2019/782},
  file             = {:Tan_2019_On Computational Complexity of Pickup and Delivery Problems with Precedence Constraints or Time Windows.pdf:PDF},
  groups           = {Precedence},
  modificationdate = {2023-04-04T16:07:00},
}

@Article{Alitappeh2021MRTAExploration,
  author           = {Reza Javanmard Alitappeh and Kossar Jeddisaravi},
  journal          = {Applied Intelligence},
  title            = {Multi-robot exploration in task allocation problem},
  year             = {2021},
  month            = jun,
  number           = {2},
  pages            = {2189--2211},
  volume           = {52},
  creationdate     = {2022-09-27T21:24:17},
  doi              = {10.1007/s10489-021-02483-3},
  file             = {:Alitappeh_2021_Multi Robot Exploration in Task Allocation Problem.pdf:PDF},
  groups           = {Spatial},
  modificationdate = {2023-04-04T16:07:02},
  publisher        = {Springer Science and Business Media {LLC}},
}

 
@Article{Liu2022TAGroundToAir,
  author           = {Liu, Jia-yi and Wang, Gang and Fu, Qiang and Yue, Shao-hua and Wang, Si-yuan},
  journal          = {Defence Technology},
  title            = {Task assignment in ground-to-air confrontation based on multiagent deep reinforcement learning},
  year             = {2022},
  issn             = {2214-9147},
  month            = apr,
  abstract         = {The scale of ground-to-air confrontation task assignments is large and needs to deal with many concurrent task assignments and random events. Aiming at the problems where existing task assignment methods are applied to ground-to-air confrontation, there is low efficiency in dealing with complex tasks, and there are interactive conflicts in multiagent systems. This study proposes a multiagent architecture based on a one-general agent with multiple narrow agents (OGMN) to reduce task assignment conflicts. Considering the slow speed of traditional dynamic task assignment algorithms, this paper proposes the proximal policy optimization for task assignment of general and narrow agents (PPO-TAGNA) algorithm. The algorithm based on the idea of the optimal assignment strategy algorithm and combined with the training framework of deep reinforcement learning (DRL) adds a multihead attention mechanism and a stage reward mechanism to the bilateral band clipping PPO algorithm to solve the problem of low training efficiency. Finally, simulation experiments are carried out in the digital battlefield. The multiagent architecture based on OGMN combined with the PPO-TAGNA algorithm can obtain higher rewards faster and has a higher win ratio. By analyzing agent behavior, the efficiency, superiority and rationality of resource utilization of this method are verified.},
  creationdate     = {2022-09-27T21:30:19},
  doi              = {10.1016/j.dt.2022.04.001},
  file             = {:Liu_2022_Task Assignment in Ground to Air Confrontation Based on Multiagent Deep Reinforcement Learning.pdf:PDF},
  groups           = {RL-based},
  keywords         = {Ground-to-air confrontation, Task assignment, General and narrow agents, Deep reinforcement learning, Proximal policy optimization (PPO)},
  language         = {en},
  modificationdate = {2022-11-05T10:46:48},
  url              = {https://www.sciencedirect.com/science/article/pii/S2214914722000678},
  urldate          = {2022-09-27},
}

@Article{Pupa2021HumanCentered,
  author           = {Andrea Pupa and Wietse Van Dijk and Cristian Secchi},
  journal          = {{IEEE} Robotics and Automation Letters},
  title            = {A Human-Centered Dynamic Scheduling Architecture for Collaborative Application},
  year             = {2021},
  month            = jul,
  number           = {3},
  pages            = {4736--4743},
  volume           = {6},
  creationdate     = {2022-09-27T21:31:39},
  doi              = {10.1109/lra.2021.3068888},
  file             = {:Pupa_2021_A Human Centered Dynamic Scheduling Architecture for Collaborative Application.pdf:PDF},
  groups           = {Precedence},
  modificationdate = {2023-04-04T16:07:03},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
  ranking          = {rank5},
}

 
@InProceedings{Xu2018Tabu,
  author           = {Xu, Kailiang and Fei, Rong and He, Dahai},
  booktitle        = {2018 13th IEEE Conference on Industrial Electronics and Applications (ICIEA)},
  title            = {A tabu-search algorithm for scheduling jobs with precedence constraints on parallel machines},
  year             = {2018},
  note             = {ISSN: 2158-2297},
  pages            = {2774--2781},
  abstract         = {A tabu-search algorithm for scheduling jobs with tree-formed precedence constraints on parallel machines is discussed in this paper. The structural characteristic of the problem is studied, which helps the searching procedure focus on “promising” neighboring schedules only, such that computing time can be effectively reduced. Experiment shows the algorithm is capable for solving medium or large-scaled problems within acceptable computing time.},
  creationdate     = {2022-09-27T21:33:10},
  doi              = {10.1109/ICIEA.2018.8398181},
  file             = {:Xu_2018_A Tabu Search Algorithm for Scheduling Jobs with Precedence Constraints on Parallel Machines.pdf:PDF},
  groups           = {Precedence},
  issn             = {2158-2297},
  keywords         = {Schedules, Parallel machines, Job shop scheduling, Heuristic algorithms, Processor scheduling, Electronic mail, parallel-machine, schedule, tabu-search, prece-dence constraints},
  modificationdate = {2023-01-07T21:57:53},
}

 
@InProceedings{Pupa2021Safety,
  author           = {Pupa, Andrea and Secchi, Cristian},
  booktitle        = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title            = {A Safety-Aware Architecture for Task Scheduling and Execution for Human-Robot Collaboration},
  year             = {2021},
  month            = sep,
  note             = {ISSN: 2153-0866},
  pages            = {1895--1902},
  abstract         = {In collaborative robotic applications, human and robot have to work together to accomplish a common job, composed by a set of tasks. In order to achieve an efficient human-robot collaboration (HRC), it is important to have an integration between a proper task scheduling strategy and a task execution strategy. The first must deal with the variability of the two agents, while the second must deal with the safety standards. In this paper, we propose an integrated architecture for task scheduling and execution in a collaborative cell. The tasks are dynamically scheduled handling the uncertainity in both the human and the robot behaviors. Subsequently, at the execution level, the task is accomplished computing trajectories comply with the safety regulations. The planning information are mutually integrated in real-time with the scheduling procedure in order improve the HRC.},
  creationdate     = {2022-09-27T21:35:30},
  doi              = {10.1109/IROS51168.2021.9636855},
  file             = {:Pupa_2021_A Safety Aware Architecture for Task Scheduling and Execution for Human Robot Collaboration.pdf:PDF},
  groups           = {Precedence},
  issn             = {2153-0866},
  keywords         = {Job shop scheduling, Processor scheduling, Service robots, Collaboration, Computer architecture, Dynamic scheduling, Safety},
  modificationdate = {2023-01-07T21:57:27},
}

@Article{Alirezazadeh2022Dynamic,
  author           = {Saeid Alirezazadeh and Luis A. Alexandre},
  journal          = {{IEEE} Robotics and Automation Letters},
  title            = {Dynamic Task Scheduling for Human-Robot Collaboration},
  year             = {2022},
  month            = oct,
  number           = {4},
  pages            = {8699--8704},
  volume           = {7},
  creationdate     = {2022-09-27T21:36:37},
  doi              = {10.1109/lra.2022.3188906},
  file             = {:Alirezazadeh_2022_Dynamic Task Scheduling for Human Robot Collaboration.pdf:PDF},
  groups           = {Precedence},
  modificationdate = {2023-04-04T16:07:04},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
  ranking          = {rank5},
}

@Misc{Hettiachchi2021SurveyTA,
  author           = {Danula Hettiachchi and Vassilis Kostakos and Jorge Goncalves},
  month            = nov,
  title            = {A Survey on Task Assignment in Crowdsourcing},
  year             = {2021},
  abstract         = {Quality improvement methods are essential to gathering high-quality crowdsourced data, both for research and industry applications. A popular and broadly applicable method is task assignment that dynamically adjusts crowd workflow parameters. In this survey, we review task assignment methods that address: heterogeneous task assignment, question assignment, and plurality problems in crowdsourcing. We discuss and contrast how these methods estimate worker performance, and highlight potential challenges in their implementation. Finally, we discuss future research directions for task assignment methods, and how crowdsourcing platforms and other stakeholders can benefit from them.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-09-28T11:17:24},
  doi              = {10.1145/3494522},
  eprint           = {2111.08501},
  file             = {:Hettiachchi_2021_A Survey on Task Assignment in Crowdsourcing.pdf:PDF},
  groups           = {Task Assignment},
  keywords         = {cs.HC},
  modificationdate = {2022-11-05T10:08:59},
  primaryclass     = {cs.HC},
}

 
@InProceedings{Bischoff2020MRTA,
  author           = {Bischoff, Esther and Meyer, Fabian and Inga, Jairo and Hohmann, Sören},
  booktitle        = {2020 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
  title            = {Multi-{Robot} {Task} {Allocation} and {Scheduling} {Considering} {Cooperative} {Tasks} and {Precedence} {Constraints}},
  year             = {2020},
  note             = {ISSN: 2577-1655},
  pages            = {3949--3956},
  abstract         = {In order to fully exploit the advantages inherent to cooperating heterogeneous multi-robot teams, sophisticated coordination algorithms are essential. Time-extended multi-robot task allocation approaches assign and schedule a set of tasks to a group of robots such that certain objectives are optimized and operational constraints are met. This is particularly challenging if cooperative tasks, i.e. tasks that require two or more robots to work directly together, are considered. In this paper, we present an easy-to-implement criterion to validate the feasibility, i.e. executability, of solutions to time-extended multi-robot task allocation problems with cross schedule dependencies arising from the consideration of cooperative tasks and precedence constraints. Using the introduced feasibility criterion, we propose a local improvement heuristic based on a neighborhood operator for the problem class under consideration. The initial solution is obtained by a greedy constructive heuristic. Both methods use a generalized cost structure and are therefore able to handle various objective function instances. We evaluate the proposed approach using test scenarios of different problem sizes, all comprising the complexity aspects of the regarded problem. The simulation results illustrate the improvement potential arising from the application of the local improvement heuristic.},
  creationdate     = {2022-09-28T16:59:31},
  doi              = {10.1109/SMC42975.2020.9283215},
  file             = {:Bischoff_2020_Multi Robot Task Allocation and Scheduling Considering Cooperative Tasks and Precedence Constraints.pdf:PDF},
  groups           = {Precedence},
  issn             = {2577-1655},
  keywords         = {Schedules, Robot kinematics, Simulation, Linear programming, Resource management, Task analysis, Cybernetics},
  modificationdate = {2022-11-05T10:46:56},
  ranking          = {rank3},
}

@Article{Zhang2013Considering,
  author           = {Yu Zhang and Lynne E. Parker},
  journal          = {Autonomous Agents and Multi-Agent Systems},
  title            = {Considering inter-task resource constraints in task allocation},
  year             = {2013},
  month            = mar,
  number           = {3},
  pages            = {389--419},
  volume           = {26},
  abstract         = {This paper focuses on task allocation with single-task robots, multi-robot tasks and instantaneous assignment, which has been shown to be strongly NP-hard. Although this problem has been studied extensively, few efficient approximation algorithms have been provided due to its inherent complexity. In this paper, we first provide discussions and analyses for two natural greedy heuristics for solving this problem. Then, a new greedy heuristic is introduced, which considers inter-task resource constraints to approximate the influence between different assignments in task allocation. Instead of only looking at the utility of the assignment, our approach computes the expected loss of utility (due to the assigned robots and task) as an offset and uses the offset utility for making the greedy choice. A formal analysis is provided for the new heuristic, which reveals that the solution quality is bounded by two different factors. A new algorithm is then provided to approximate the new heuristic for performance improvement. Finally, for more complicated applications, we extend this problem to include general task dependencies and provide a result on the hardness of approximating this new formulation. Comparison results with the two natural heuristics in simulation are provided for both formulations, which show that the new approach achieves improved performance.},
  creationdate     = {2022-09-28T19:50:50},
  doi              = {10.1007/s10458-012-9196-7},
  file             = {:Zhang_2013_Considering Inter Task Resource Constraints in Task Allocation.pdf:PDF},
  groups           = {Precedence},
  modificationdate = {2023-04-04T16:07:07},
  publisher        = {Springer Science and Business Media {LLC}},
}

 
@Article{Gerkey2004TaxonomyMRTA,
  author           = {Gerkey, Brian P. and Matarić, Maja J.},
  journal          = {The International Journal of Robotics Research},
  title            = {A {Formal} {Analysis} and {Taxonomy} of {Task} {Allocation} in {Multi}-{Robot} {Systems}},
  year             = {2004},
  issn             = {0278-3649},
  month            = sep,
  number           = {9},
  pages            = {939--954},
  volume           = {23},
  abstract         = {Despite more than a decade of experimental work in multi-robot systems, important theoretical aspects of multi-robot coordination mechanisms have, to date, been largely untreated. To address this issue, we focus on the problem of multi-robot task allocation (MRTA). Most work on MRTA has been ad hoc and empirical, with many coordination architectures having been proposed and validated in a proof-of-concept fashion, but infrequently analyzed. With the goal of bringing objective grounding to this important area of research, we present a formal study of MRTA problems. A domain-independent taxonomy of MRTA problems is given, and it is shown how many such problems can be viewed as instances of other, well-studied, optimization problems. We demonstrate how relevant theory from operations research and combinatorial optimization can be used for analysis and greater understanding of existing approaches to task allocation, and to show how the same theory can be used in the synthesis of new approaches.},
  creationdate     = {2022-09-29T09:02:09},
  doi              = {10.1177/0278364904045564},
  file             = {:Gerkey_2004_A Formal Analysis and Taxonomy of Task Allocation in Multi Robot Systems.pdf:PDF},
  groups           = {Task Assignment},
  language         = {en},
  modificationdate = {2022-11-05T10:08:59},
  publisher        = {SAGE Publications Ltd STM},
  url              = {https://doi.org/10.1177/0278364904045564},
  urldate          = {2022-09-29},
}

@Article{Yu2021Optimizing,
  author           = {Tian Yu and Jing Huang and Qing Chang},
  journal          = {Journal of Manufacturing Systems},
  title            = {Optimizing task scheduling in human-robot collaboration with deep multi-agent reinforcement learning},
  year             = {2021},
  month            = jul,
  pages            = {487--499},
  volume           = {60},
  abstract         = {Human-Robot Collaboration (HRC) presents an opportunity to improve the efficiency of manufacturing processes. However, the existing task planning approaches for HRC are still limited in many ways, e.g., co-robot encoding must rely on experts’ knowledge and the real-time task scheduling is applicable within small state-action spaces or simplified problem settings. In this paper, the HRC assembly working process is formatted into a novel chessboard setting, in which the selection of chess piece move is used to analogize to the decision making by both humans and robots in the HRC assembly working process. To optimize the completion time, a Markov game model is considered, which takes the task structure and the agent status as the state input and the overall completion time as the reward. Without experts’ knowledge, this game model is capable of seeking for correlated equilibrium policy among agents with convergency in making real-time decisions facing a dynamic environment. To improve the efficiency in finding an optimal policy of the task scheduling, a deep-Q-network (DQN) based multi-agent reinforcement learning (MARL) method is applied and compared with the Nash-Q learning, dynamic programming and the DQN-based single-agent reinforcement learning method. A height-adjustable desk assembly is used as a case study to demonstrate the effectiveness of the proposed algorithm with different number of tasks and agents.},
  creationdate     = {2022-10-19T09:38:18},
  doi              = {10.1016/j.jmsy.2021.07.015},
  file             = {:Yu_2021_Optimizing Task Scheduling in Human Robot Collaboration with Deep Multi Agent Reinforcement Learning.pdf:PDF},
  groups           = {RL-based},
  modificationdate = {2023-04-04T16:07:08},
  publisher        = {Elsevier {BV}},
}

@InProceedings{IzquierdoBadiola2022Improved,
  author           = {Silvia Izquierdo-Badiola and Gerard Canal and Carlos Rizzo and Guillem Alenya},
  booktitle        = {2022 International Conference on Robotics and Automation ({ICRA})},
  title            = {Improved Task Planning through Failure Anticipation in Human-Robot Collaboration},
  year             = {2022},
  month            = may,
  pages            = {7875--7880},
  publisher        = {{IEEE}},
  abstract         = {Human-Robot Collaboration (HRC) has become a major trend in robotics in recent years with the idea of combining the strengths from both humans and robots. In order to share the work to be done, many task planning approaches have been implemented. However, they don't fully satisfy the required adaptability in human-robot collaborative tasks, with most approaches not considering neither the state of the human partner nor the possibility of adapting the collaborative plan during execution or even anticipating failures. In this paper, we present a planning system for human-robot collaborative plans that takes into account the agents' states and deals with unforeseen human behaviour, by replanning in anticipation when the human state changes to prevent action failure. The human state is defined in terms of capacity, knowledge and motivation. The system has been implemented in a standardised environment using the Planning Domain Definition Language (PDDL) and the modular ROSPlan framework, and we have validated the approach in multiple simulation settings. Our results show that using the human model fosters an appropriate task allocation while allowing failure anticipation, replanning in time to prevent it.},
  creationdate     = {2022-10-19T09:49:18},
  doi              = {10.1109/icra46639.2022.9812236},
  file             = {:Izquierdo-Badiola_2022_Improved Task Planning through Failure Anticipation in Human Robot Collaboration.pdf:PDF},
  groups           = {Task Assignment},
  modificationdate = {2023-04-04T16:07:10},
  url              = {https://ieeexplore.ieee.org/document/9812236},
}

 
@Misc{Gong2022Towards,
  author           = {Gong, Xin and Wang, Tieniu and Cui, Yukang and Huang, Tingwen},
  month            = mar,
  note             = {arXiv:2203.04052 [cs] type: article},
  title            = {Towards {Safe} and {Efficient} {Swarm}-{Human} {Collaboration}: {A} {Hierarchical} {Multi}-{Agent} {Pickup} and {Delivery} framework},
  year             = {2022},
  abstract         = {The multi-Agent Pickup and Delivery (MAPD) problem is crucial in the realm of Intelligent Storage Systems (ISSs), where multiple robots are assigned with time-varying, heterogeneous, and potentially uncertain tasks. When it comes to Human-Swarm Hybrid System ((HS)\$\_2\$), robots and human workers will accomplish the MAPD tasks in collaboration. Herein, we propose a Human-Swarm Hybrid System Pickup and Delivery ((HS)\$\_2\$PD) framework, which is predominant in future ISSs. A two-layer decision framework based on the prediction horizon window is established in light of the unpredictability of human behavior and the dynamic changes of tasks. The first layer is a two-level programming problem to solve the problems of mode assignment and TA. The second layer is devoted to the exact path of each agent via solving mixed-integer programming (MIP) problems. An integrated algorithm for the (HS)\$\_2\$PD problem is summarized. The practicality and validity of the above algorithm are illustrated via a numerical simulation example towards (HS)\$\_2\$PD tasks.},
  creationdate     = {2022-10-19T09:58:42},
  doi              = {10.48550/arXiv.2203.04052},
  file             = {:Gong_2022_Towards Safe and Efficient Swarm Human Collaboration_ a Hierarchical Multi Agent Pickup and Delivery Framework.pdf:PDF},
  groups           = {Task Assignment},
  keywords         = {Computer Science - Multiagent Systems},
  modificationdate = {2022-11-05T10:08:59},
  school           = {arXiv},
  shorttitle       = {Towards {Safe} and {Efficient} {Swarm}-{Human} {Collaboration}},
  url              = {http://arxiv.org/abs/2203.04052},
  urldate          = {2022-10-19},
}

@InProceedings{Zhai2021WTA,
  author           = {Haoran Zhai and Weihong Wang and Qingze Li and Wei Zhang},
  booktitle        = {2021 33rd Chinese Control and Decision Conference (CCDC)},
  title            = {Weapon-Target Assignment Based on Improved {PSO} Algorithm},
  year             = {2021},
  month            = may,
  publisher        = {{IEEE}},
  abstract         = {In the research of modern combat command, the weapon-target assignment (WTA) problem contains many variables, which is a typical NP (Non-deterministic Polynomial) problem. The WTA optimal allocation model of multi-target cooperative attack is established by synthesizing various factors affecting operational effectiveness evaluation. Aiming at the shortcoming that the basic PSO algorithm is easy to fall into the local optimal solution, this paper proposes an improved PSO algorithm to improve the calculation speed and accuracy. The improved PSO algorithm uses the inertia weight of nonlinear decrement, improves the acceleration constant and introduces the adaptive mutation operation, which improves the convergence speed and accuracy of the algorithm, and avoids falling into the local optimal solution. Aiming at the WTA problem, the decimal coding strategy is adopted for the particle, and the position and velocity of the particle are discretized and limited by the boundary. Finally, the improved PSO algorithm is obtained. For large-scale WTA problems, the improved PSO algorithm is compared with the basic PSO algorithm. The simulation results show that the improved PSO algorithm speeds up the convergence speed of the algorithm, has better solution accuracy, and can effectively solve the WTA problem.},
  creationdate     = {2022-10-19T10:53:59},
  doi              = {10.1109/ccdc52312.2021.9601736},
  file             = {:Zhai_2021_Weapon Target Assignment Based on Improved PSO Algorithm.pdf:PDF},
  groups           = {Task Assignment},
  modificationdate = {2023-04-04T16:07:12},
}

 
@Article{Lu2021WTA,
  author           = {Lu, Yiping and Chen, Danny Z.},
  journal          = {Omega},
  title            = {A new exact algorithm for the {Weapon}-{Target} {Assignment} problem},
  year             = {2021},
  issn             = {0305-0483},
  month            = jan,
  volume           = {98},
  abstract         = {The Weapon-Target Assignment (WTA) problem is of military importance; it computes an optimal assignment of m weapons to n targets such that the expected total damage of the targets is maximized (or equivalently, the expected total survival possibility of the targets is minimized). The WTA problem is known to be NP-complete and was commonly formulated as nonlinear models. In previous studies, the largest WTA problem instances that could be solved exactly were of only moderate sizes (e.g., 80 weapons and 80 targets, with a long execution time of 16.2 h). In this paper, unlike the previous methods that tackled the WTA problem as nonlinear models, we formulate the problem as a linear model, and present a new exact algorithm that is much more efficient for solving the problem. More specifically, our new exact algorithm formulates the WTA problem as an integer linear programming model which has binary columns, and solves the model using column enumeration as well as branch and bound techniques. To drastically reduce the number of columns needed to be enumerated, we propose new methods called weapon number bounding and weapon domination. Extensive computational experiments are conducted, and the results show that our new exact algorithm can solve all the instances considered by previous studies but our solutions take much shorter execution time. In particular, the execution time for exactly solving the instance of 80 weapons and 80 targets is only 0.40 s. Furthermore, we can solve exactly much larger problem instances than previous methods, and the maximum problem size that we can solve exactly is up to 400 weapons and 400 targets, with an average execution time of 4.68 s. In addition, our method can be extended to be applicable to the Assignment of Assets to Tasks (AATT) problem.},
  creationdate     = {2022-10-19T11:27:14},
  doi              = {10.1016/j.omega.2019.102138},
  file             = {:Lu_2021_A New Exact Algorithm for the Weapon Target Assignment Problem.pdf:PDF},
  groups           = {Task Assignment},
  keywords         = {Weapon-Target Assignment, Integer linear programming, Column enumeration, Assignment of Assets to Tasks},
  language         = {en},
  modificationdate = {2022-11-05T10:08:59},
  url              = {https://www.sciencedirect.com/science/article/pii/S030504831930711X},
  urldate          = {2022-10-19},
}

@Article{Fu2021Air,
  author           = {Li Fu and Xi Long and Wenbin He},
  journal          = {Journal of Shanghai Jiaotong University (Science)},
  title            = {Air Combat Assignment Problem Based on Bayesian Optimization Algorithm},
  year             = {2021},
  month            = jan,
  number           = {6},
  pages            = {799--805},
  volume           = {27},
  creationdate     = {2022-10-19T15:36:58},
  doi              = {10.1007/s12204-021-2270-z},
  file             = {:Fu_2021_Air Combat Assignment Problem Based on Bayesian Optimization Algorithm.pdf:PDF},
  groups           = {Task Assignment},
  modificationdate = {2023-04-04T16:07:13},
  publisher        = {Springer Science and Business Media {LLC}},
}

@InProceedings{Ye2021UAVTA,
  author           = {Xinning Ye and Zhongkui Lei and Kun Liu},
  booktitle        = {2021 {IEEE} International Conference on Consumer Electronics and Computer Engineering ({ICCECE})},
  title            = {Multi-{UAV} Task Assignment based on Satisficing Decision Algorithm},
  year             = {2021},
  month            = jan,
  pages            = {439--442},
  publisher        = {{IEEE}},
  abstract         = {With the rapid development of UAV technology and the increasingly complex battlefield environment, UAV mission planning system has become a hot and difficult problem in the field of UAV combat. This paper establishes a mathematical model for the problem of establishing a multi-UAV collaborative target allocation based on the effects of UAV differences, target differences and battlefield posture on target allocation, and proposes a multi-UAV target allocation method based on Satisficing Decision(SD), and apply Satisficing Decision Theory (SDT) to solve the multi-target allocation problem of multi-UAV cooperation.},
  creationdate     = {2022-10-19T22:04:15},
  doi              = {10.1109/iccece51280.2021.9342335},
  file             = {:Ye_2021_Multi UAV Task Assignment Based on Satisficing Decision Algorithm.pdf:PDF},
  groups           = {Task Assignment},
  modificationdate = {2023-04-04T16:07:14},
}

@Article{Fazal2022TA,
  author           = {Nayyer Fazal and Muhammad Tahir Khan and Shahzad Anwar and Javaid Iqbal and Shahbaz Khan},
  journal          = {{PLOS} {ONE}},
  title            = {Task allocation in multi-robot system using resource sharing with dynamic threshold approach},
  year             = {2022},
  month            = may,
  number           = {5},
  pages            = {e0267982},
  volume           = {17},
  abstract         = {Task allocation is a fundamental requirement for multi-robot systems working in dynamic environments. An efficient task allocation algorithm allows the robots to adjust their behavior in response to environmental changes such as fault occurrences, or other robots’ actions to increase overall system performance. To address these challenges, this paper presents a Task Allocation technique based on a threshold level which is an accumulative value aggregated by a centralized unit using the Task-Robot ratio and the number of the available resource in the system. The threshold level serves as a reference for task acceptance and the task acceptance occurs despite resource shortage. The deficient resources for the accepted task are acquired through an auction process using objective minimization. Despite resource shortage, task acceptance occurs. The threshold approach and the objective minimization in the auction process reduce the overall completion time and increase the system’s resource utilization up to 96%, which is demonstrated theoretically and validated through simulations and real experimentation.},
  creationdate     = {2022-10-20T16:39:51},
  doi              = {10.1371/journal.pone.0267982},
  editor           = {Mohd Nadhir Ab Wahab},
  file             = {:Fazal_2022_Task Allocation in Multi Robot System Using Resource Sharing with Dynamic Threshold Approach.pdf:PDF},
  groups           = {Task Assignment},
  modificationdate = {2023-04-04T16:07:16},
  publisher        = {Public Library of Science ({PLoS})},
  url              = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0267982},
}

@Article{Wu2022A,
  author           = {Xiaojun Wu and Zhiyuan Gao and Sheng Yuan and Qiao Hu and Zerui Dang},
  journal          = {Sensors},
  title            = {A Dynamic Task Allocation Algorithm for Heterogeneous {UUV} Swarms},
  year             = {2022},
  month            = mar,
  number           = {6},
  pages            = {2122},
  volume           = {22},
  abstract         = {Aiming at the task allocation problem of heterogeneous unmanned underwater vehicle (UUV) swarms, this paper proposes a dynamic extended consensus-based bundle algorithm (DECBBA) based on consistency algorithm. Our algorithm considers the multi-UUV task allocation problem that each UUV can individually complete multiple tasks, constructs a "UUV-task" matching matrix and designs new marginal utility, reward and cost functions for the influence of time, path and UUV voyage. Furthermore, in view of the unfavorable factors that restrict the underwater acoustic communication range between UUVs in the real environment, our algorithm complete dynamic task allocation of UUV swarms with optimization in load balance indicator by the update of the UUV individual and the task completion status in the discrete time stage. The performance indicators (including global utility and task completion rate) of the dynamic task allocation algorithm in the scenario with communication constraints can be well close to the static algorithm in the ideal scenario without communication constraints. The simulation experiment results show that the algorithm proposed in this paper can quickly and efficiently obtain the dynamic and conflict-free task allocation assignment of UUV swarms with great performance.},
  comment          = {MDPI sensors},
  creationdate     = {2022-10-22T22:05:30},
  doi              = {10.3390/s22062122},
  groups           = {Task Assignment},
  keywords         = {auction algorithm, decentralized system, dynamic task allocation, multiple intelligent agents, unmanned underwater vehicle},
  modificationdate = {2023-04-04T16:07:17},
  publisher        = {{MDPI} {AG}},
}

@InCollection{Zhou2022Resource,
  author           = {Yifeng Zhou and Kai Di and Zichen Dong and Yichuan Jiang},
  booktitle        = {{IFIP} Advances in Information and Communication Technology},
  publisher        = {Springer International Publishing},
  title            = {Resource Scheduling for~Human-Machine Collaboration in~Multiagent Systems},
  year             = {2022},
  pages            = {173--184},
  abstract         = {To solve the human-machine resource scheduling problem in multiagent systems, we represent the complex constraints among subtasks by a directed graph and model the transmission effect of uncertainty of task execution in the modeling of human-machine collaboration. We prove that the human-machine resource scheduling problem is NP-hard and propose a heuristic scheduling algorithm to solve the problem. The algorithm determines the task priority based on cumulative delay risk estimation and time period division, so as to produce the resource scheduling scheme. Through experiments employing PSPLIB dataset, we validate the performance of the proposed heuristic algorithm by comparing with an optimal solution and the algorithm allocating resources based on the waiting time of tasks.},
  creationdate     = {2022-10-24T12:06:29},
  doi              = {10.1007/978-3-031-03948-5_15},
  file             = {:Zhou_2022_Resource Scheduling For~Human Machine Collaboration In~Multiagent Systems.pdf:PDF},
  groups           = {Task Assignment},
  keywords         = {Multiagent system, Resource scheduling, Human-machine collaboration},
  modificationdate = {2022-11-05T10:08:59},
}

@Misc{Ali2022Heter,
  author           = {Arsha Ali and Hebert Azevedo-Sa and Dawn M. Tilbury and Lionel P. Robert},
  month            = sep,
  title            = {Heterogeneous human{\textendash}robot task allocation based on artificial trust},
  year             = {2022},
  abstract         = {Effective human–robot collaboration requires the appropriate allocation of indivisible tasks between humans and robots. A task allocation method that appropriately makes use of the unique capabilities of each agent (either a human or a robot) can improve team performance. This paper presents a novel task allocation method for heterogeneous human–robot teams based on artificial trust from a robot that can learn agent capabilities over time and allocate both existing and novel tasks. Tasks are allocated to the agent that maximizes the expected total reward. The expected total reward incorporates trust in the agent to successfully execute the task as well as the task reward and cost associated with using that agent for that task. Trust in an agent is computed from an artificial trust model, where trust is assessed along a capability dimension by comparing the belief in agent capabilities with the task requirements. An agent’s capabilities are represented by a belief distribution and learned using stochastic task outcomes. Our task allocation method was simulated for a human–robot dyad. The team total reward of our artificial trust-based task allocation method outperforms other methods both when the human’s capabilities are initially unknown and when the human’s capabilities belief distribution has converged to the human’s actual capabilities. Our task allocation method enables human–robot teams to maximize their joint performance.},
  creationdate     = {2022-10-25T09:29:46},
  doi              = {10.1038/s41598-022-19140-5},
  file             = {:Ali_2022_Heterogeneous Human_robot Task Allocation Based on Artificial Trust.pdf:PDF},
  groups           = {Task Assignment},
  journal          = {Scientific Reports},
  modificationdate = {2023-04-04T16:07:18},
  number           = {1},
  publisher        = {Springer Science and Business Media {LLC}},
  volume           = {12},
}

@InProceedings{Lippi2021Mixed,
  author           = {Martina Lippi and Alessandro Marino},
  booktitle        = {2021 30th {IEEE} International Conference on Robot \& Human Interactive Communication ({RO}-{MAN})},
  title            = {A Mixed-Integer Linear Programming Formulation for Human Multi-Robot Task Allocation},
  year             = {2021},
  month            = aug,
  publisher        = {{IEEE}},
  abstract         = {In this work, we address a task allocation problem for human multi-robot settings. Given a set of tasks to perform, we formulate a general Mixed-Integer Linear Programming (MILP) problem aiming at minimizing the overall execution time while optimizing the quality of the executed tasks as well as human and robotic workload. Different skills of the agents, both human and robotic, are taken into account and human operators are enabled to either directly execute tasks or play supervisory roles; moreover, multiple manipulators can tightly collaborate if required to carry out a task. Finally, as realistic in human contexts, human parameters are assumed to vary over time, e.g., due to increasing human level of fatigue. Therefore, online monitoring is required and re-allocation is performed if needed. Simulations in a realistic scenario with two manipulators and a human operator performing an assembly task validate the effectiveness of the approach.},
  creationdate     = {2022-10-25T09:45:37},
  doi              = {10.1109/ro-man50785.2021.9515362},
  file             = {:Lippi_2021_A Mixed Integer Linear Programming Formulation for Human Multi Robot Task Allocation.pdf:PDF},
  groups           = {Task Assignment},
  modificationdate = {2023-04-04T16:07:21},
}

@Misc{Ali2022Considerations,
  author           = {Ali, Arsha and Tilbury, Dawn and {Robert, Lionel + "Jr"}},
  title            = {Considerations for Task Allocation in Human-Robot Teams},
  year             = {2022},
  creationdate     = {2022-10-25T09:59:30},
  doi              = {10.7302/6543},
  file             = {:Ali_2022_Considerations for Task Allocation in Human Robot Teams.pdf:PDF},
  groups           = {Task Assignment},
  keywords         = {human-robot teaming, human-robot interaction, Task Allocation, Human-Robot Teams, Human-Robot Task Allocation, task allocation methods, agent capability, Future of Work, human-computer interaction, AI Ethics, AI Fairness, robot trust, human-robot team trust, robot overtrust, under robot trust, undertrusting, overtrusting, calibrated trust, calibrated robot trust, Information Science, Social Sciences},
  language         = {en},
  modificationdate = {2022-11-05T10:08:59},
  publisher        = {2022 AAAI Fall Symposium},
}

@Misc{Xiao2020Benchmark,
  author           = {Kun Xiao and Junqi Lu and Ying Nie and Lan Ma and Xiangke Wang and Guohui Wang},
  month            = sep,
  title            = {A Benchmark for Multi-UAV Task Assignment of an Extended Team Orienteering Problem},
  year             = {2020},
  abstract         = {A benchmark for multi-UAV task assignment is presented in order to evaluate different algorithms. An extended Team Orienteering Problem is modeled for a kind of multi-UAV task assignment problem. Three intelligent algorithms, i.e., Genetic Algorithm, Ant Colony Optimization and Particle Swarm Optimization are implemented to solve the problem. A series of experiments with different settings are conducted to evaluate three algorithms. The modeled problem and the evaluation results constitute a benchmark, which can be used to evaluate other algorithms used for multi-UAV task assignment problems.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-10-26T09:49:08},
  eprint           = {2009.00363},
  file             = {:Xiao_2020_A Benchmark for Multi UAV Task Assignment of an Extended Team Orienteering Problem.pdf:PDF},
  groups           = {Task Assignment},
  keywords         = {cs.AI, cs.SY, eess.SY},
  modificationdate = {2022-11-05T10:08:59},
  primaryclass     = {cs.AI},
}

@InCollection{Pupa2021Dynamic,
  author           = {Andrea Pupa and Chiara Talignani Landi and Mattia Bertolani and Cristian Secchi},
  booktitle        = {Springer Proceedings in Advanced Robotics},
  publisher        = {Springer International Publishing},
  title            = {A Dynamic Architecture for Task Assignment and Scheduling for Collaborative Robotic Cells},
  year             = {2021},
  pages            = {74--88},
  abstract         = {In collaborative robotic cells, a human operator and a robot share the workspace in order to execute a common job, consisting of a set of tasks. A proper allocation and scheduling of the tasks for the human and for the robot is crucial for achieving an efficient human-robot collaboration. In order to deal with the dynamic and unpredictable behavior of the human and for allowing the human and the robot to negotiate about the tasks to be executed, a two layers architecture for solving the task allocation and scheduling problem is proposed. The first layer optimally solves the task allocation problem considering nominal execution times. The second layer, which is reactive, adapts online the sequence of tasks to be executed by the robot considering deviations from the nominal behaviors and requests coming from the human and from robot. The proposed architecture is experimentally validated on a collaborative assembly job.},
  creationdate     = {2022-10-26T11:13:24},
  doi              = {10.1007/978-3-030-71356-0_6},
  file             = {:Pupa_2021_A Dynamic Architecture for Task Assignment and Scheduling for Collaborative Robotic Cells.pdf:PDF},
  groups           = {Task Assignment},
  modificationdate = {2022-11-05T10:08:59},
}

 
@Misc{Gosrich2022Precedence,
  author           = {Gosrich, Walker and Mayya, Siddharth and Narayan, Saaketh and Malencia, Matthew and Agarwal, Saurav and Kumar, Vijay},
  month            = sep,
  note             = {arXiv:2209.14417 [cs] type: article},
  title            = {Multi-{Robot} {Coordination} and {Cooperation} with {Task} {Precedence} {Relationships}},
  year             = {2022},
  abstract         = {We propose a new formulation for the multi-robot task planning and allocation problem that incorporates (a) precedence relationships between tasks; (b) coordination for tasks allowing multiple robots to achieve increased efficiency; and (c) cooperation through the formation of robot coalitions for tasks that cannot be performed by individual robots alone. In our formulation, the tasks and the relationships between the tasks are specified by a task graph. We define a set of reward functions over the task graph's nodes and edges. These functions model the effect of robot coalition size on the task performance, and incorporate the influence of one task's performance on a dependent task. Solving this problem optimally is NP-hard. However, using the task graph formulation allows us to leverage min-cost network flow approaches to obtain approximate solutions efficiently. Additionally, we explore a mixed integer programming approach, which gives optimal solutions for small instances of the problem but is computationally expensive. We also develop a greedy heuristic algorithm as a baseline. Our modeling and solution approaches result in task plans that leverage task precedence relationships and robot coordination and cooperation to achieve high mission performance, even in large missions with many agents.},
  annote           = {Comment: 6 pages, 7 figures. Submitted to IEEE ICRA 2023},
  creationdate     = {2022-10-31T09:49:42},
  doi              = {10.48550/arXiv.2209.14417},
  file             = {:Gosrich_2022_Multi Robot Coordination and Cooperation with Task Precedence Relationships.pdf:PDF},
  groups           = {Precedence},
  keywords         = {Computer Science - Robotics},
  modificationdate = {2022-11-05T10:46:56},
  school           = {arXiv},
  url              = {http://arxiv.org/abs/2209.14417},
  urldate          = {2022-10-31},
}

@Article{Forte2021Online,
  author           = {Paolo Forte and Anna Mannucci and Henrik Andreasson and Federico Pecora},
  journal          = {{IEEE} Robotics and Automation Letters},
  title            = {Online Task Assignment and Coordination in Multi-Robot Fleets},
  year             = {2021},
  month            = jul,
  number           = {3},
  pages            = {4584--4591},
  volume           = {6},
  abstract         = {We propose a loosely-coupled framework for integrated task assignment, motion planning, coordination and control of heterogeneous fleets of robots subject to non-cooperative tasks. The approach accounts for the important real-world requirement that tasks can be posted asynchronously. We exploit systematic search for optimal task assignment, where interference is considered as a cost and estimated with knowledge of the kinodynamic models and current state of the robots. Safety is guaranteed by an online coordination algorithm, where the absence of collisions is treated as a hard constraint. The relation between the weight of interference cost in task assignment and computational overhead is analyzed empirically, and the approach is compared against alternative realizations using local search algorithms for task assignment.},
  creationdate     = {2022-11-02T21:18:00},
  doi              = {10.1109/lra.2021.3068918},
  file             = {:Forte_2021_Online Task Assignment and Coordination in Multi Robot Fleets.pdf:PDF},
  groups           = {Task Assignment},
  modificationdate = {2023-04-04T16:07:23},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
  url              = {https://ieeexplore.ieee.org/abstract/document/9387084},
}

@InProceedings{Noorian2022What,
  author           = {Shahin Sharifi Noorian and Sihang Qiu and Ujwal Gadiraju and Jie Yang and Alessandro Bozzon},
  booktitle        = {Proceedings of the {ACM} Web Conference 2022},
  title            = {What Should You Know? A Human-In-the-Loop Approach to Unknown Unknowns Characterization in Image Recognition},
  year             = {2022},
  month            = apr,
  publisher        = {{ACM}},
  creationdate     = {2022-11-09T08:24:21},
  doi              = {10.1145/3485447.3512040},
  file             = {:Noorian_2022_What Should You Know_ a Human in the Loop Approach to Unknown Unknowns Characterization in Image Recognition.pdf:PDF},
  groups           = {Sihang Qiu},
  modificationdate = {2023-04-04T16:07:25},
}

@InProceedings{Gupta2022Trust,
  author           = {Akshit Gupta and Debadeep Basu and Ramya Ghantasala and Sihang Qiu and Ujwal Gadiraju},
  booktitle        = {Proceedings of the {ACM} Web Conference 2022},
  title            = {To Trust or Not To Trust: How a Conversational Interface Affects Trust in a Decision Support System},
  year             = {2022},
  month            = apr,
  publisher        = {{ACM}},
  creationdate     = {2022-11-09T08:25:30},
  doi              = {10.1145/3485447.3512248},
  file             = {:Gupta_2022_To Trust or Not to Trust_ How a Conversational Interface Affects Trust in a Decision Support System.pdf:PDF},
  groups           = {Sihang Qiu},
  modificationdate = {2023-04-04T16:07:28},
}

 
@InProceedings{Osinski2020Simulation,
  author           = {Osiński, Błażej and Jakubowski, Adam and Zięcina, Paweł and Miłoś, Piotr and Galias, Christopher and Homoceanu, Silviu and Michalewski, Henryk},
  booktitle        = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
  title            = {Simulation-{Based} {Reinforcement} {Learning} for {Real}-{World} {Autonomous} {Driving}},
  year             = {2020},
  note             = {ISSN: 2577-087X},
  pages            = {6411--6418},
  abstract         = {We use reinforcement learning in simulation to obtain a driving system controlling a full-size real-world vehicle. The driving policy takes RGB images from a single camera and their semantic segmentation as input. We use mostly synthetic data, with labelled real-world data appearing only in the training of the segmentation network.Using reinforcement learning in simulation and synthetic data is motivated by lowering costs and engineering effort.In real-world experiments we confirm that we achieved successful sim-to-real policy transfer. Based on the extensive evaluation, we analyze how design decisions about perception, control, and training impact the real-world performance.},
  creationdate     = {2022-11-10T09:48:05},
  doi              = {10.1109/ICRA40945.2020.9196730},
  file             = {:Osiński_2020_Simulation Based Reinforcement Learning for Real World Autonomous Driving.pdf:PDF},
  groups           = {RL_Simulation},
  issn             = {2577-087X},
  keywords         = {Training, Visualization, Learning (artificial intelligence), Semantics, Robots, Image segmentation, Predictive models},
  modificationdate = {2022-11-10T09:49:16},
}

@Article{Capocchi2022DEVS,
  author           = {Laurent Capocchi and Jean-Fran{\c{c}}ois Santucci},
  journal          = {Information},
  title            = {Discrete Event Modeling and Simulation for Reinforcement Learning System Design},
  year             = {2022},
  month            = feb,
  number           = {3},
  pages            = {121},
  volume           = {13},
  abstract         = {Discrete event modeling and simulation and reinforcement learning are two frameworks suited for cyberphysical system design, which, when combined, can give powerful tools for system optimization or decision making process for example. This paper describes how discrete event modeling and simulation could be integrated into reinforcement learning concepts and tools in order to assist in the realization of reinforcement learning systems, more specially considering the temporal, hierarchical, and multi-agent aspects. An overview of these different improvements are given based on the implementation of the Q-Learning reinforcement learning algorithm in the framework of the Discrete Event system Specification (DEVS) and System Entity Structure (SES) formalisms.},
  comment          = {MDPI-Information
如何将DEMS集成到强化学习中？temporal, hierarchical, and multi-agent aspects},
  creationdate     = {2022-11-10T10:52:39},
  doi              = {10.3390/info13030121},
  file             = {:Capocchi_2022_Discrete Event Modeling and Simulation for Reinforcement Learning System Design.pdf:PDF},
  groups           = {RL_Simulation},
  keywords         = {modeling; simulation; machine learning; reinforcement learning},
  modificationdate = {2023-04-04T16:07:29},
  publisher        = {{MDPI} {AG}},
}

 
@InProceedings{Saadawi2016DEVS,
  author           = {Saadawi, Hesham and Wainer, Gabriel and Pliego, German},
  booktitle        = {2016 {Symposium} on {Theory} of {Modeling} and {Simulation} ({TMS}-{DEVS})},
  title            = {{DEVS} execution acceleration with machine learning},
  year             = {2016},
  month            = apr,
  pages            = {1--6},
  abstract         = {Discrete Event System Specification DEVS separates modeling and simulation execution. Simulation execution is done within a runtime environment that is often called a DEVS simulator. This separation creates an opportunity to incorporate smart algorithms in the simulator to optimize simulation execution. In this paper, we propose incorporating some predictive machine learning algorithms into the DEVS simulator that can cut simulation execution times significantly for many simulation applications without compromising the simulation accuracy. In this paper, we introduce a specific learning mechanism that can be embedded into the DEVS simulator to incrementally build a predictive model that learns from past simulations. We further look into issues related to the predictive model selection, its prediction accuracy, its effect on the overall simulation performance, and when to switch between the predictive model and the simulation during an execution.},
  creationdate     = {2022-11-15T11:00:58},
  doi              = {10.23919/TMS.2016.7918816},
  file             = {:Saadawi_2016_DEVS Execution Acceleration with Machine Learning.pdf:PDF},
  groups           = {RL_Simulation},
  keywords         = {Computational modeling, Semiconductor device modeling, Numerical models, Predictive models, Runtime, Discrete Event System Specification DEVS, Machine Learning, Regression Models},
  modificationdate = {2022-11-16T12:25:10},
}

@InCollection{Li2016Communication,
  author           = {Sirui Li and Weixing Sun and Tim Miller},
  booktitle        = {Lecture Notes in Computer Science},
  publisher        = {Springer International Publishing},
  title            = {Communication in Human-Agent Teams for Tasks with Joint Action},
  year             = {2016},
  pages            = {224--241},
  abstract         = {In many scenarios, humans must team with agents to achieve joint aims. When working collectively in a team of human and artificial agents, communication is important to establish a shared situation of the task at hand. With no human in the loop and little cost for communication, information about the task can be easily exchanged. However, when communication becomes expensive, or when there are humans in the loop, the strategy for sharing information must be carefully designed: too little information leads to lack of shared situation awareness, while too much overloads the human team members, decreasing performance overall. This paper investigates the effects of sharing beliefs and goals in agent teams and in human-agent teams. We performed a set of experiments using the BlocksWorlds for Teams (BW4T) testbed to assess different strategies for information sharing. In previous experimental studies using BW4T, explanations about agent behaviour were shown to have no effect on team performance. One possible reason for this is because the existing scenarios in BW4T contained joint tasks, but not joint actions. That is, atomic actions that required interdependent and simultaneous action between more than one agent. We implemented new scenarios in BW4T in which some actions required two agents to complete. Our results showed an improvement in artificial-agent team performance when communicating goals and sharing beliefs, but with goals contributing more to team performance, and that in human-agent teams, communicating only goals was more effective than communicating both goals and beliefs.},
  comment          = {bw4t human agent teamwork},
  creationdate     = {2022-11-15T17:25:12},
  doi              = {10.1007/978-3-319-42691-4_13},
  file             = {:Li_2016_Communication in Human Agent Teams for Tasks with Joint Action.pdf:PDF},
  groups           = {Teamwork},
  modificationdate = {2022-11-15T17:26:16},
}

@InProceedings{Qin2016Cooperative,
  author           = {Ruoxi Qin and Tian Wang and Haotian Jiang and Qianhong Yan and Weikang Wang and Hichem Snoussi},
  booktitle        = {2016 IEEE International Conference on Digital Signal Processing (DSP)},
  title            = {Cooperative target searching and tracking via UCT with probability distribution model},
  year             = {2016},
  month            = oct,
  publisher        = {{IEEE}},
  abstract         = {As Unmanned Aerial Vehicle's (UAV) battery life and stability develop, multiple UAVs are having more and more applications in the uninterrupted patrol and security. Thus UAV's searching, tracking and trajectory planning become important issues. This paper proposes an online distributed algorithm used in UAV's tracking and searching, with the consideration of UAV's practical need to recharge under limited power. We propose a Quantum Probability Model to describe the partially observable target positions, and we use Upper Confidence Tree (UCT) algorithm to find out the best searching and tracking route based on this model. We also introduce the Teammate Learning Model to handle the nonstationary problems in distributed reinforcement learning.},
  code             = {https://github.com/RuoxiQin/Unmanned-Aerial-Vehicle-Tracking},
  comment          = {源代码 https://github.com/RuoxiQin/Unmanned-Aerial-Vehicle-Tracking},
  creationdate     = {2022-11-15T21:29:07},
  doi              = {10.1109/icdsp.2016.7868620},
  file             = {:Qin_2016_Cooperative Target Searching and Tracking Via UCT with Probability Distribution Model.pdf:PDF},
  groups           = {Swarm Intelligence},
  modificationdate = {2023-04-04T16:07:31},
}

@InProceedings{Othman2018Machine,
  author           = {Muhammad Shalihin Bin Othman and Gary Tan},
  booktitle        = {2018 {IEEE}/{ACM} 22nd International Symposium on Distributed Simulation and Real Time Applications ({DS}-{RT})},
  title            = {Machine Learning Aided Simulation of Public Transport Utilization},
  year             = {2018},
  month            = oct,
  publisher        = {{IEEE}},
  abstract         = {Despite having one of the most efficient transportation systems in the world, Singapore is still faced with congestion issues regularly, especially during peak hour periods, due to a number of reasons. We investigate some of the factors contributing to this issue and propose a simulator supplied with predictive travel times through congestion prediction, in order to evaluate and improve bus utilization through effective scheduling. We introduced a conceptual framework to integrate neural network models into simulation so as to improve real-time supply based on several possibilities of demands. This paper will delineate the steps taken to produce the simulator and discuss the evaluation of these models.},
  creationdate     = {2022-11-16T10:30:00},
  doi              = {10.1109/distra.2018.8601011},
  file             = {:Othman_2018_Machine Learning Aided Simulation of Public Transport Utilization.pdf:PDF},
  groups           = {RL_Simulation},
  modificationdate = {2023-04-04T16:07:32},
}

@InProceedings{Schurr2003Impact,
  author           = {Nathan Schurr and Paul Scerri and Milind Tambe},
  booktitle        = {Workshop on Humans and Multi-Agent Systems at AAMAS},
  title            = {Impact of human advice on agent teams: A preliminary report},
  year             = {2003},
  pages            = {1--6},
  abstract         = {Given a large-scale team composed of intelligent members that are following derived heuristics, the performance of the team is often suboptimal. Providing more accurate heuristics may be infeasible or even impossible. We propose an approach that incoporates a human advisor interacting with a multiagent team in which the advice that the human gives is a component of the heuristic that each agent already uses. This approach allows us to have a clear way of adjusting the level of the team’s autonomy, addresses the issue of who on the team will be affected by the advice, and also factors in advice immediately (while the team is still performing).
We looked back at data from previous experiments with an advisor (fire chief) in a disaster rescue simulation. Then we applied our approach to a domain where robots maintain a room full of sensors. We studied different advisors and varied how much the team members listened.
Our initial results show that the problem of how to have a team of multiagents interpret advice from a human turned out to be much harder than we first thought. Our hypothesis is that human advice will be of assistance when the human can provide strategic advice that was previously unknown to the agents. Yet the the strategic advice must still be given in a manner in which the agents can understand through their limited methods of communication.},
  creationdate     = {2022-11-18T10:39:47},
  file             = {:Schurr_2003_Impact of Human Advice on Agent Teams_ a Preliminary Report.pdf:PDF},
  groups           = {Human-Agent-Robot},
  modificationdate = {2022-11-18T10:57:13},
}

 
@InProceedings{Buehler2020Theory,
  author           = {Buehler, Moritz C. and Weisswange, Thomas H.},
  booktitle        = {2020 {IEEE} {International} {Conference} on {Human}-{Machine} {Systems} ({ICHMS})},
  title            = {Theory of {Mind} based {Communication} for {Human} {Agent} {Cooperation}},
  year             = {2020},
  month            = sep,
  pages            = {1--6},
  abstract         = {For human agent cooperation, reasoning about the partner is necessary to enable an efficient interaction. To provide helpful information, it is important not only to account for environmental uncertainties or dangers but also to maintain a sophisticated understanding of each other's mental state, a theory of mind. Sharing every piece of information is not a good idea, as some may be irrelevant at time or already known, leading to distraction and annoyance. Instead, an agent will have to estimate the novelty and relevance of information for the receiver, to trade off the cost of communication against potential benefits. We propose the concept of theory of mind based communication as principled formulation to ground an agents cooperative communication on an understanding of the receiver's mental states to support her awareness and action selection. Therefore we formulate the problem of whether, when and what information to share as a sequential decision process with the human belief as central source of uncertainty. The agent's communication decision is obtained online during interaction by combining a second level Bayesian inference of human belief with planning under uncertainty, evaluating the influence of communication on the human belief and her future decisions. We discuss the resulting behavior on an illustrative communication scenario with different uncertain state aspects that an observing agent can communicate to the actor.},
  comment          = {人机合作时，agent对于合作伙伴的推理是非常有必要的。除了对于环境的不确定性或危险，还需要关注彼此的精神状态。
共享所有信息可能导致分心或过载，因此agent需要估计信息的新颖和相关性，权衡通信成本和收益},
  creationdate     = {2022-11-19T15:49:09},
  doi              = {10.1109/ICHMS49158.2020.9209472},
  file             = {:Buehler_2020_Theory of Mind Based Communication for Human Agent Cooperation.pdf:PDF},
  groups           = {Human-Agent-Robot},
  keywords         = {Uncertainty, Planning, Task analysis, Bayes methods, Decision making, Cognition, Receivers, Human agent interaction, Communication, Theory of Mind, Human Belief, POMDP, Planning under Uncertainty},
  modificationdate = {2023-01-31T19:09:18},
}

 
@InProceedings{Dalmasso2021Human,
  author           = {Dalmasso, Marc and Garrell, Anaís and Domínguez, José Enrique and Jiménez, Pablo and Sanfeliu, Alberto},
  booktitle        = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
  title            = {Human-{Robot} {Collaborative} {Multi}-{Agent} {Path} {Planning} using {Monte} {Carlo} {Tree} {Search} and {Social} {Reward} {Sources}},
  year             = {2021},
  note             = {ISSN: 2577-087X},
  pages            = {10133--10138},
  abstract         = {The collaboration between humans and robots in an object search task requires the achievement of shared plans obtained from communicating and negotiating. In this work, we assume that the robot computes, as a first step, a multi-agent plan for both itself and the human. Then, both plans are submitted to human scrutiny, who either agrees or modifies it forcing the robot to adapt its own restrictions or preferences. This process is repeated along the search task as many times as required by the human. Our planner is based on a decentralized variant of Monte Carlo Tree Search (MCTS), with one robot and one human as agents. Moreover, our algorithm allows the robot and the human to optimize their own actions by maintaining a probability distribution over the plans in a joint-action space. The method allows an objective function definition over action sequences, it assumes intermittent communication, it is anytime and suitable for on-line replanning. To test it, we have developed a human-robot communication mobile phone interface. Validation is provided by real-life search experiments of a Parcheesi token in an urban space, including also an acceptability study.},
  creationdate     = {2022-11-19T19:55:42},
  doi              = {10.1109/ICRA48506.2021.9560995},
  file             = {:Dalmasso_2021_Human Robot Collaborative Multi Agent Path Planning Using Monte Carlo Tree Search and Social Reward Sources.pdf:PDF},
  groups           = {Human-Agent-Robot},
  issn             = {2577-087X},
  keywords         = {Monte Carlo methods, Collaboration, Human-robot interaction, Search problems, Linear programming, Mobile handsets, Probability distribution},
  modificationdate = {2022-11-19T20:29:15},
}

@InProceedings{Kira2009Exerting,
  author           = {Zsolt Kira and Mitchell A. Potter},
  booktitle        = {2009 4th International Conference on Autonomous Robots and Agents},
  title            = {Exerting human control over decentralized robot swarms},
  year             = {2009},
  month            = feb,
  publisher        = {{IEEE}},
  abstract         = {Robot swarms are capable of performing tasks with robustness and flexibility using only local interactions between the agents. Such a system can lead to emergent behavior that is often desirable, but difficult to control and manipulate post-design. These properties make the real-time control of swarms by a human operator challenging-a problem that has not been adequately addressed in the literature. In this paper we present preliminary work on two possible forms of control: top-down control of global swarm characteristics and bottom-up control by influencing a subset of the swarm members. We present learning methods to address each of these. The first method uses instance-based learning to produce a generalized model from a sampling of the parameter space and global characteristics for specific situations. The second method uses evolutionary learning to learn placement and parameterization of virtual agents that can influence the robots in the swarm. Finally we show how these methods generalize and can be used by a human operator to dynamically control a swarm in real time.},
  comment          = {human对于swarm的实时控制具有挑战性。考虑两种方法：全局特征的自上而下控制和通过影响群体成员子集的自下而上控制。},
  creationdate     = {2022-11-20T09:51:12},
  doi              = {10.1109/icara.2000.4803934},
  file             = {:Kira_2009_Exerting Human Control Over Decentralized Robot Swarms.pdf:PDF},
  groups           = {Human-Swarm},
  modificationdate = {2023-04-04T16:07:33},
}

@Misc{Schaefer2022TaskEmbeddings,
  author           = {Lukas Schäfer and Filippos Christianos and Amos Storkey and Stefano V. Albrecht},
  month            = jul,
  title            = {Learning Task Embeddings for Teamwork Adaptation in Multi-Agent Reinforcement Learning},
  year             = {2022},
  abstract         = {Successful deployment of multi-agent reinforcement learning often requires agents to adapt their behaviour. In this work, we discuss the problem of teamwork adaptation in which a team of agents needs to adapt their policies to solve novel tasks with limited fine-tuning. Motivated by the intuition that agents need to be able to identify and distinguish tasks in order to adapt their behaviour to the current task, we propose to learn multi-agent task embeddings (MATE). These task embeddings are trained using an encoder-decoder architecture optimised for reconstruction of the transition and reward functions which uniquely identify tasks. We show that a team of agents is able to adapt to novel tasks when provided with task embeddings. We propose three MATE training paradigms: independent MATE, centralised MATE, and mixed MATE which vary in the information used for the task encoding. We show that the embeddings learned by MATE identify tasks and provide useful information which agents leverage during adaptation to novel tasks.},
  archiveprefix    = {arXiv},
  code             = {https://github.com/uoe-agents/MATE},
  comment          = {提供源代码https://github.com/uoe-agents/MATE
多任务，任务嵌入，有意思的是这里的训练任务和测试任务的map大小都不一样
训练编码-解码器来重构P和R，实验表明提供任务嵌入时，能够适应新任务；并提出三种范式：独立/集中/混合式任务嵌入
实际上是用了类似隐变量的方法，来对任务进行表征，但是从single-agent拓展到了multi-agent},
  creationdate     = {2022-11-20T10:08:12},
  eprint           = {2207.02249},
  file             = {:Schäfer_2022_Learning Task Embeddings for Teamwork Adaptation in Multi Agent Reinforcement Learning.pdf:PDF},
  groups           = {MARL, Multi_Agent_Meta_Learning},
  keywords         = {cs.MA, cs.AI, cs.LG},
  modificationdate = {2022-11-20T10:24:09},
  primaryclass     = {cs.MA},
  ranking          = {rank5},
}

@Misc{Tilbury2022Identity,
  author           = {Kyle Tilbury and Jesse Hoey},
  month            = aug,
  title            = {Identity and Dynamic Teams in Social Dilemmas},
  year             = {2022},
  abstract         = {We present our preliminary work on a multi-agent system involving the complex human phenomena of identity and dynamic teams. We outline our ongoing experimentation into understanding how these factors can eliminate some of the naive assumptions of current multi-agent approaches. These include a lack of complex heterogeneity between agents and unchanging team structures. We outline the human social psychological basis for identity, one's sense of self, and dynamic teams, the changing nature of human teams. We describe our application of these factors to a multi-agent system and our expectations for how they might improve the system's applicability to more complex problems, with specific relevance to ad hoc teamwork. We expect that the inclusion of more complex human processes, like identity and dynamic teams, will help with the eventual goal of having effective human-agent teams.},
  archiveprefix    = {arXiv},
  comment          = {关于动态团队中的一些心理、社会学方面的讨论，可以写进大论文},
  creationdate     = {2022-11-20T10:29:18},
  eprint           = {2208.03293},
  file             = {:Tilbury_2022_Identity and Dynamic Teams in Social Dilemmas.pdf:PDF},
  groups           = {Social Delemmas in AHT},
  keywords         = {cs.MA},
  modificationdate = {2022-11-20T10:30:23},
  primaryclass     = {cs.MA},
}

@Misc{Fosong2022Fewshot,
  author           = {Elliot Fosong and Arrasy Rahman and Ignacio Carlucho and Stefano V. Albrecht},
  month            = jul,
  title            = {Few-Shot Teamwork},
  year             = {2022},
  abstract         = {We propose the novel few-shot teamwork (FST) problem, where skilled agents trained in a team to complete one task are combined with skilled agents from different tasks, and together must learn to adapt to an unseen but related task. We discuss how the FST problem can be seen as addressing two separate problems: one of reducing the experience required to train a team of agents to complete a complex task; and one of collaborating with unfamiliar teammates to complete a new task. Progress towards solving FST could lead to progress in both multi-agent reinforcement learning and ad hoc teamwork.},
  archiveprefix    = {arXiv},
  comment          = {提出了few-shot teamwork这个概念，将其分为两个子问题：如何用少数样本训练一队智能体完成复杂任务；如何与不熟悉的队友完成新任务。没有给出解决方案，只是阐述问题},
  creationdate     = {2022-11-20T10:36:55},
  eprint           = {2207.09300},
  file             = {:Fosong_2022_Few Shot Teamwork.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  keywords         = {cs.MA, cs.AI},
  modificationdate = {2022-11-20T10:40:02},
  primaryclass     = {cs.MA},
}

@Misc{Carlucho2022Cooperative,
  author           = {Ignacio Carlucho and Arrasy Rahman and William Ard and Elliot Fosong and Corina Barbalata and Stefano V. Albrecht},
  month            = jul,
  title            = {Cooperative Marine Operations via Ad Hoc Teams},
  year             = {2022},
  abstract         = {While research in ad hoc teamwork has great potential for solving real-world robotic applications, most developments so far have been focusing on environments with simple dynamics. In this article, we discuss how the problem of ad hoc teamwork can be of special interest for marine robotics and how it can aid marine operations. Particularly, we present a set of challenges that need to be addressed for achieving ad hoc teamwork in underwater environments and we discuss possible solutions based on current state-of-the-art developments in the ad hoc teamwork literature.},
  archiveprefix    = {arXiv},
  comment          = {水下环境，海洋机器人；讨论了AHT如何能帮助海上作业，特别是搜救场景，描述了水下作业AHT主要挑战},
  creationdate     = {2022-11-20T10:40:35},
  eprint           = {2207.07498},
  file             = {:Carlucho_2022_Cooperative Marine Operations Via Ad Hoc Teams.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  keywords         = {cs.MA},
  modificationdate = {2022-11-20T10:44:54},
  primaryclass     = {cs.MA},
}

@Misc{Fujimoto2022AHTAdversaries,
  author           = {Ted Fujimoto and Samrat Chatterjee and Auroop Ganguly},
  month            = aug,
  title            = {Ad Hoc Teamwork in the Presence of Adversaries},
  year             = {2022},
  abstract         = {Advances in ad hoc teamwork have the potential to create agents that collaborate robustly in real-world applications. Agents deployed in the real world, however, are vulnerable to adversaries with the intent to subvert them. There has been little research in ad hoc teamwork that assumes the presence of adversaries. We explain the importance of extending ad hoc teamwork to include the presence of adversaries and clarify why this problem is difficult. We then propose some directions for new research opportunities in ad hoc teamwork that leads to more robust multi-agent cyber-physical infrastructure systems.},
  archiveprefix    = {arXiv},
  comment          = {考虑存在对抗的ad hoc teamwork，提出了一些潜在挑战和未来可能的研究方向，没有具体工作
提到了AHT的一些应用场景
New Frontiers in Adversarial Machine Learning Workshop at the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, 2022. Copyright 2022 by the author(s).},
  creationdate     = {2022-11-20T10:46:45},
  eprint           = {2208.05071},
  file             = {:Fujimoto_2022_Ad Hoc Teamwork in the Presence of Adversaries.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  keywords         = {cs.MA, cs.AI},
  modificationdate = {2022-12-08T16:54:39},
  primaryclass     = {cs.MA},
}

@PhdThesis{Tummala2022Advancing,
  author           = {Tummala, Vijayanth},
  school           = {Florida Institute of Technology},
  title            = {Advancing Human-Agent Teamwork},
  year             = {2022},
  abstract         = {Progress in the use of Artificial Intelligence (AI) has been made in many different fields. We are now reaching a point in AI development typical AI implementations are not enough: where we would need humans and AI systems to actively collaborate with each other, basing their actions on the actions and capabilities of each other. This collaboration could be in the form of agents assisting humans processing and analyzing information, assisting humans with smaller physical tasks or working with humans as an equal team member – having the same goals and performing the same tasks – to accomplish a goal. These advancements and new capabilities indicate the need for a teamwork model that can facilitate the creation and efficiency of human-AI teams. There are currently few teamwork models available that can assist AI developers and teamwork theorists in the creation of intelligent, automated teammates. The goal of the research mentioned in this dissertation is to fill this gap by creating a novel goal and information-sharing algorithm which improves and extends the working capabilities of an existing theoretical teamwork model and extends an existing AI planning agent system (a PDDL planner) to share environmental information obtained by humans and agents. The information shared is specifically related to the goals and tasks of each agent. The result of this research is a methodology for extending existing PDDL planners with the capability of mutually beneficial interactions between it and other agents or humans. The results of using the system with ad-hoc teams of existing PDDL planners are improved overall task performance.},
  creationdate     = {2022-11-20T11:00:26},
  file             = {:Tummala_2022_Advancing Human Agent Teamwork.pdf:PDF},
  groups           = {Human-Agent-Robot},
  modificationdate = {2022-11-20T11:02:30},
  url              = {http://hdl.handle.net/11141/3560},
}

@Article{Ahmed2022Deep,
  author           = {Ibrahim H. Ahmed and Cillian Brewitt and Ignacio Carlucho and Filippos Christianos and Mhairi Dunion and Elliot Fosong and Samuel Garcin and Shangmin Guo and Balint Gyevnar and Trevor McInroe and Georgios Papoudakis and Arrasy Rahman and Lukas Schäfer and Massimiliano Tamborski and Giuseppe Vecchio and Cheng Wang and Stefano~V. Albrecht},
  journal          = {{AI} Communications},
  title            = {Deep reinforcement learning for multi-agent interaction},
  year             = {2022},
  month            = sep,
  number           = {4},
  pages            = {357--368},
  volume           = {35},
  abstract         = {The development of autonomous agents which can interact with other agents to accomplish a given task is a core area of research in artificial intelligence and machine learning. Towards this goal, the Autonomous Agents Research Group develops novel machine learning algorithms for autonomous systems control, with a specific focus on deep reinforcement learning and multi-agent reinforcement learning. Research problems include scalable learning of coordinated agent policies and inter-agent communication; reasoning about the behaviours, goals, and composition of other agents from limited observations; and sample-efficient learning based on intrinsic motivation, curriculum learning, causal inference, and representation learning. This article provides a broad overview of the ongoing research portfolio of the group and discusses open problems for future directions.},
  creationdate     = {2022-11-20T11:03:09},
  doi              = {10.3233/aic-220116},
  editor           = {Stefano V. Albrecht and Michael Woolridge},
  file             = {:Ahmed_2022_Deep Reinforcement Learning for Multi Agent Interaction.pdf:PDF},
  groups           = {Teamwork},
  keywords         = {Deep reinforcement learning, multi-agent reinforcement learning, ad hoc teamwork, agent/opponent modelling, goal recognition, autonomous driving, multi-robot warehouse},
  modificationdate = {2023-04-04T16:07:35},
  publisher        = {{IOS} Press},
  url              = {https://content.iospress.com/articles/ai-communications/aic220116},
}

@Article{Sycara2002Integrating,
  author           = {Katia Sycara and Michael Lewis},
  journal          = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
  title            = {Integrating Agents into Human Teams},
  year             = {2002},
  month            = sep,
  number           = {3},
  pages            = {413--417},
  volume           = {46},
  abstract         = {Software agents represent a radical departure from earlier monolithic approaches to artificial intelligence by introducing intelligence in small packages in many different places. For each instance of potential aiding there are two questions: 1- can a software agent perform the task? and 2- can the agent's assistance contribute to team performance? Our research addresses these two issues by demonstrating the feasibility of sophisticated agent assistance in scenario-based technology demonstrations and investigating the contribution of agent assistance to human team performance using simplified, controllable laboratory experiments.},
  creationdate     = {2022-11-20T15:10:47},
  doi              = {10.1177/154193120204600342},
  file             = {:Sycara_2002_Integrating Agents into Human Teams.pdf:PDF},
  groups           = {Human-Agent-Robot},
  modificationdate = {2023-04-04T16:07:36},
  publisher        = {{SAGE} Publications},
}

@Article{Deka2021Human,
  author           = {Ankur Deka and Katia Sycara and Phillip Walker and Huao Li and Michael Lewis},
  journal          = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
  title            = {Human vs. Deep Neural Network Performance at a Leader Identification Task},
  year             = {2021},
  month            = sep,
  number           = {1},
  pages            = {1152--1156},
  volume           = {65},
  abstract         = {Control of robotic swarms through control over a leader(s) has become the dominant approach to supervisory control over these largely autonomous systems. Resilience in the face of attrition is one of the primary advantages attributed to swarms yet the presence of leader(s) makes them vulnerable to decapitation. Algorithms which allow a swarm to hide its leader are a promising solution. We present a novel approach in which neural networks, NNs, trained in a graph neural network, GNN, replace conventional controllers making them more amenable to training. Swarms and an adversary intent of finding the leader were trained and tested in 4 phases: 1-swarm to follow leader, 2-adversary to recognize leader, 3-swarm to hide leader from adversary, and 4-swarm and adversary compete to hide and recognize the leader. While the NN adversary was more successful in identifying leaders without deception, humans did better in conditions in which the swarm was trained to hide its leader from the NN adversary. The study illustrates difficulties likely to emerge in arms races between machine learners and the potential role humans may play in moderating them.},
  comment          = {群体通过跟随领导者来实现团队行为，但是leader容易遭受斩首行动；因此需要隐藏leader
本文研究了一种对抗行为，swarm需要隐藏团队leader，},
  creationdate     = {2022-11-20T15:18:34},
  doi              = {10.1177/1071181321651127},
  file             = {:Deka_2021_Human Vs. Deep Neural Network Performance at a Leader Identification Task.pdf:PDF},
  groups           = {Swarm Intelligence},
  modificationdate = {2023-04-04T16:07:38},
  publisher        = {{SAGE} Publications},
}

@Article{Soh2019Multitask,
  author           = {Harold Soh and Yaqi Xie and Min Chen and David Hsu},
  journal          = {The International Journal of Robotics Research},
  title            = {Multi-task trust transfer for human{\textendash}robot interaction},
  year             = {2019},
  month            = aug,
  number           = {2-3},
  pages            = {233--249},
  volume           = {39},
  abstract         = {Trust is essential in shaping human interactions with one another and with robots. In this article we investigate how human trust in robot capabilities transfers across multiple tasks. We present a human-subject study of two distinct task domains: a Fetch robot performing household tasks and a virtual reality simulation of an autonomous vehicle performing driving and parking maneuvers. The findings expand our understanding of trust and provide new predictive models of trust evolution and transfer via latent task representations: a rational Bayes model, a data-driven neural network model, and a hybrid model that combines the two. Experiments show that the proposed models outperform prevailing models when predicting trust over unseen tasks and users. These results suggest that (i) task-dependent functional trust models capture human trust in robot capabilities more accurately and (ii) trust transfer across tasks can be inferred to a good degree. The latter enables trust-mediated robot decision-making for fluent human–robot interaction in multi-task settings.},
  creationdate     = {2022-11-20T18:18:34},
  doi              = {10.1177/0278364919866905},
  groups           = {Trust},
  modificationdate = {2023-04-04T16:07:39},
  publisher        = {{SAGE} Publications},
  url              = {https://journals.sagepub.com/doi/abs/10.1177/0278364919866905},
}

@Article{Khavas2021Review,
  author           = {Zahra Rezaei Khavas},
  title            = {A Review on Trust in Human-Robot Interaction},
  year             = {2021},
  month            = may,
  abstract         = {Due to agile developments in the field of robotics and human-robot interaction, prospective robotic agents are intended to play the role of teammates and partner with humans to perform operations, rather than tools that are replacing humans helping humans in a specific task. this notion of partnering with robots raises new challenges for human-robot interaction (HRI), which gives rise to a new field of research in HRI, namely human-robot trust. Where humans and robots are working as partners, the performance of the work can be diminished if humans do not trust robots appropriately. Considering the impact of human-robot trust observed in different HRI fields, many researchers have investigated the field of human-robot trust and examined various concerns related to human-robot trust. In this work, we review the past works on human-robot trust based on the research topics and discuss selected trends in this field. Based on these reviews, we finally propose some ideas and areas of potential future research at the end of this paper.},
  archiveprefix    = {arXiv},
  creationdate     = {2022-11-20T18:18:50},
  eprint           = {2105.10045},
  file             = {:Khavas_2021_A Review on Trust in Human Robot Interaction.pdf:PDF},
  groups           = {Trust},
  keywords         = {cs.RO, cs.CY, cs.HC, 62x25},
  modificationdate = {2022-11-21T16:31:26},
  primaryclass     = {cs.RO},
}

@Book{Nam2020Book,
  author           = {Nam, Chang S. and Lyons, Joseph B.},
  publisher        = {Elsevier Science \& Technology},
  title            = {Trust in Human-Robot Interaction},
  year             = {2020},
  isbn             = {9780128194720},
  creationdate     = {2022-11-21T16:29:56},
  file             = {:Nam_2020_Trust in Human Robot Interaction.pdf:PDF},
  groups           = {Trust},
  modificationdate = {2022-11-21T16:31:15},
}

@Article{Valentini2017BestOfN,
  author           = {Gabriele Valentini and Eliseo Ferrante and Marco Dorigo},
  journal          = {Frontiers in Robotics and {AI}},
  title            = {The Best-of-n Problem in Robot Swarms: Formalization, State of the Art, and Novel Perspectives},
  year             = {2017},
  month            = mar,
  volume           = {4},
  abstract         = {The ability to collectively choose the best among a finite set of alternatives is a fundamental cognitive skill for robot swarms. In this paper, we propose a formal definition of the best-of-n problem and a taxonomy that details its possible variants. Based on this taxonomy, we analyze the swarm robotics literature focusing on the decision-making problem dealt with by the swarm. We find that, so far, the literature has primarily focused on certain variants of the best-of-n problem, while other variants have been the subject of only a few isolated studies. Additionally, we consider a second taxonomy about the design methodologies used to develop collective decision-making strategies. Based on this second taxonomy, we provide an in-depth survey of the literature that details the strategies proposed so far and discusses the advantages and disadvantages of current design methodologies.},
  creationdate     = {2022-11-21T19:27:40},
  doi              = {10.3389/frobt.2017.00009},
  file             = {:Valentini_2017_The Best of N Problem in Robot Swarms_ Formalization, State of the Art, and Novel Perspectives.pdf:PDF},
  groups           = {Swarm Intelligence},
  modificationdate = {2023-04-04T16:07:40},
  publisher        = {Frontiers Media {SA}},
}

@InProceedings{Agrawal2020Model,
  author           = {Ankit Agrawal and Jane Cleland-Huang and Jan-Philipp Steghofer},
  booktitle        = {2020 IEEE Tenth International Model-Driven Requirements Engineering (MoDRE)},
  title            = {Model-Driven Requirements for Humans-on-the-Loop Multi-{UAV} Missions},
  year             = {2020},
  month            = aug,
  publisher        = {{IEEE}},
  abstract         = {The use of semi-autonomous Unmanned Aerial Vehicles (UAVs or drones) to support emergency response scenarios, such as fire surveillance and search-and-rescue, has the potential for huge societal benefits. Onboard sensors and artificial intelligence (AI) allow these UAVs to operate autonomously in the environment. However, human intelligence and domain expertise are crucial in planning and guiding UAVs to accomplish the mission. Therefore, humans and multiple UAVs need to collaborate as a team to conduct a time-critical mission successfully. We propose a meta-model to describe interactions among the human operators and the autonomous swarm of UAVs. The meta-model also provides a language to describe the roles of UAVs and humans and the autonomous decisions. We complement the meta-model with a template of requirements elicitation questions to derive models for specific missions. We also identify common scenarios where humans should collaborate with UAVs to augment the autonomy of the UAVs. We introduce the meta-model and the requirements elicitation process with examples drawn from a search-and-rescue mission in which multiple UAVs collaborate with humans to respond to the emergency. We then apply it to a second scenario in which UAVs support first responders in fighting a structural fire. Our results show that the meta-model and the template of questions support the modeling of the human-on-the-loop human interactions for these complex missions, suggesting that it is a useful tool for modeling the human-on-the-loop interactions for multi-UAVs missions.},
  creationdate     = {2022-11-21T22:03:52},
  doi              = {10.1109/modre51215.2020.00007},
  file             = {:Agrawal_2020_Model Driven Requirements for Humans on the Loop Multi UAV Missions.pdf:PDF},
  groups           = {Human-Swarm},
  modificationdate = {2023-04-04T16:07:41},
}

 
@Misc{ClelandHuang2020HumanDrone,
  author           = {Cleland-Huang, Jane and Agrawal, Ankit},
  month            = oct,
  note             = {arXiv:2010.04101 [cs] type: article},
  title            = {Human-{Drone} {Interactions} with {Semi}-{Autonomous} {Cohorts} of {Collaborating} {Drones}},
  year             = {2020},
  abstract         = {Research in human-drone interactions has primarily focused on cases in which a person interacts with a single drone as an active controller, recipient of information, or a social companion; or cases in which an individual, or a team of operators interacts with a swarm of drones as they perform some coordinated flight patterns. In this position paper we explore a third scenario in which multiple humans and drones collaborate in an emergency response scenario. We discuss different types of interactions, and draw examples from current DroneResponse project.},
  annote           = {Comment: Proceedings of the Interdisciplinary Workshop on Human-Drone Interaction co-located with the 2020 ACM CHI Conference on Human Factors in Computing Systems (CHI 2020) - http://ceur-ws.org/Vol-2617/},
  comment          = {给出了一个通俗的例子，human之间存在合作，drone之间存在合作，drone为human提供状态、图像并发送请求；human对drone发出指令

任务非常具体，类似于无人机搜救、投递},
  creationdate     = {2022-11-21T22:11:07},
  doi              = {10.48550/arXiv.2010.04101},
  eprint           = {2010.04101},
  file             = {:Cleland-Huang_2020_Human Drone Interactions with Semi Autonomous Cohorts of Collaborating Drones.pdf:PDF},
  groups           = {Human-Swarm},
  keywords         = {Computer Science - Human-Computer Interaction, Computer Science - Software Engineering},
  modificationdate = {2023-02-16T22:40:53},
  url              = {http://arxiv.org/abs/2010.04101},
  urldate          = {2022-11-21},
}

@Article{Tang2019Novel,
  author           = {Tang, Hongwei and Sun, Wei and Yu, Hongshan and Lin, Anping and Xue, Min and Song, Yuxue},
  journal          = {Applied Intelligence},
  title            = {A novel hybrid algorithm based on {PSO} and {FOA} for target searching in unknown environments},
  year             = {2019},
  issn             = {1573-7497},
  month            = jul,
  number           = {7},
  pages            = {2603--2622},
  volume           = {49},
  abstract         = {In unknown environments, multiple-robot cooperation for target searching is a hot and difficult issue. Swarm intelligence algorithms, such as Particle Swarm Optimization (PSO) and Fruit Fly Optimization Algorithm (FOA), are widely used. To overcome local optima and enhance swarm diversity, this paper presents a novel multi-swarm hybrid FOA-PSO (MFPSO) algorithm for robot target searching. The main contributions of the proposed method are as follows. (1) The improved FOA (IFOA) provides a better value for the improved PSO (IPSO) to find the next optimal robot position value. (2) Multi-swarm strategy is introduced to enhance the diversity and achieve an effective exploration to avoid premature convergence and falling into local optima. (3) An escape mechanism named MSCM (Multi-Scale Cooperative Mutation) is used to address the limitation of local optima and enhance the escape ability for obstacle avoidance. All of the aspects mentioned above lead robots to the target without falling into local optima and allow the search mission to be performed more quickly. Several experiments in four parts are performed to verify the better performance of MFPSO. The experimental results show that the performance of MFPSO is much more significant than that of other current approaches.},
  comment          = {未知环境下基于智能优化算法的多机器人目标搜索研究_唐宏伟_湖南大学博士论文

位置环境，协同目标搜索；采用了PSO和FOA
- IFOA为IPSO提供了更好的数值，来确定下一个最优位置
- 引入多群策略，增加多样性，避免过早收敛，陷入局部最优
- 逃逸机制},
  creationdate     = {2022-11-21T22:17:14},
  doi              = {10.1007/s10489-018-1390-0},
  file             = {:Tang_2019_A Novel Hybrid Algorithm Based on PSO and FOA for Target Searching in Unknown Environments.pdf:PDF},
  groups           = {Swarm Intelligence},
  keywords         = {Particle swarm optimization, Fruit Fly optimization algorithm, Target searching, Multi-swarm, Multi-scale cooperative mutation},
  language         = {en},
  modificationdate = {2023-02-01T10:56:33},
  url              = {https://doi.org/10.1007/s10489-018-1390-0},
  urldate          = {2022-11-21},
}

@Article{Krell2019Collision,
  author           = {Evan Krell and Alaa Sheta and Arun Prassanth Ramaswamy Balasubramanian and Scott A. King},
  journal          = {Journal of Artificial Intelligence and Soft Computing Research},
  title            = {Collision-Free Autonomous Robot Navigation in Unknown Environments Utilizing {PSO} for Path Planning},
  year             = {2019},
  month            = aug,
  number           = {4},
  pages            = {267--282},
  volume           = {9},
  abstract         = {The autonomous navigation of robots in unknown environments is a challenge since it needs the integration of a several subsystems to implement different functionality. It needs drawing a map of the environment, robot map localization, motion planning or path following, implementing the path in real-world, and many others; all have to be implemented simultaneously. Thus, the development of autonomous robot navigation (ARN) problem is essential for the growth of the robotics field of research. In this paper, we present a simulation of a swarm intelligence method is known as Particle Swarm Optimization (PSO) to develop an ARN system that can navigate in an unknown environment, reaching a pre-defined goal and become collision-free. The proposed system is built such that each subsystem manipulates a specific task which integrated to achieve the robot mission. PSO is used to optimize the robot path by providing several waypoints that minimize the robot traveling distance. The Gazebo simulator was used to test the response of the system under various envirvector representing a solution to the optimization problem.onmental conditions. The proposed ARN system maintained robust navigation and avoided the obstacles in different unknown environments. vector representing a solution to the optimization problem.},
  comment          = {未知障碍环境的路径规划，最小化行进距离
多路点

貌似不是Multi-Agent，而是单个机器人},
  creationdate     = {2022-11-22T15:28:07},
  doi              = {10.2478/jaiscr-2019-0008},
  file             = {:Krell_2019_Collision Free Autonomous Robot Navigation in Unknown Environments Utilizing PSO for Path Planning.pdf:PDF},
  groups           = {Swarm Intelligence},
  keywords         = {mobile robot, particle swarm optimization, path planning},
  modificationdate = {2023-04-04T16:07:43},
  publisher        = {Walter de Gruyter {GmbH}},
}

@Article{Husain2022Search,
  author           = {Zainab Husain and Amna Al Zaabi and Hanno Hildmann and Fabrice Saffre and Dymitr Ruta and A. F. Isakovic},
  journal          = {Drones},
  title            = {{Search and Rescue in a Maze-like Environment with Ant and Dijkstra Algorithms}},
  year             = {2022},
  month            = sep,
  number           = {10},
  pages            = {273},
  volume           = {6},
  abstract         = {With the growing reliability of modern ad hoc networks, it is encouraging to analyze the potential involvement of autonomous ad hoc agents in critical situations where human involvement could be perilous. One such critical scenario is the Search and Rescue effort in the event of a disaster, in which timely discovery and help deployment is of utmost importance. This paper demonstrates the applicability of a bio-inspired technique, namely Ant Algorithms (AA), in optimizing the search time for a route or path to a trapped victim, followed by the application of Dijkstra’s algorithm in the rescue phase. The inherent exploratory nature of AA is put to use for faster mapping and coverage of the unknown search space. Four different AA are implemented, with different effects of the pheromone in play. An inverted AA, with repulsive pheromones, was found to be the best fit for this particular application. After considerable exploration, upon discovery of the victim, the autonomous agents further facilitate the rescue process by forming a relay network, using the already deployed resources. Hence, the paper discusses a detailed decision-making model of the swarm, segmented into two primary phases that are responsible for the search and rescue, respectively. Different aspects of the performance of the agent swarm are analyzed as a function of the spatial dimensions, the complexity of the search space, the deployed search group size, and the signal permeability of the obstacles in the area.},
  comment          = {室内无人机是否合理？类似迷宫的房间布局},
  creationdate     = {2022-11-22T22:47:14},
  doi              = {10.3390/drones6100273},
  file             = {:Husain_2022_Search and Rescue in a Maze like Environment with Ant and Dijkstra Algorithms.pdf:PDF},
  groups           = {Swarm Intelligence},
  keywords         = {search and rescue; SAR; ant algorithms; ant colony optimization; ACO; maze exploration; UAV; UAS; drones; civil security; public safety; smart city},
  modificationdate = {2023-04-04T16:07:44},
  publisher        = {{MDPI} {AG}},
}

@Article{Zheng2017Hybrid,
  author           = {Nan-ning Zheng and Zi-yi Liu and Peng-ju Ren and Yong-qiang Ma and Shi-tao Chen and Si-yu Yu and Jian-ru Xue and Ba-dong Chen and Fei-yue Wang},
  journal          = {Frontiers of Information Technology \& Electronic Engineering},
  title            = {Hybrid-augmented intelligence: collaboration and cognition},
  year             = {2017},
  month            = feb,
  number           = {2},
  pages            = {153--179},
  volume           = {18},
  abstract         = {The long-term goal of artificial intelligence (AI) is to make machines learn and think like human beings. Due to the high levels of uncertainty and vulnerability in human life and the open-ended nature of problems that humans are facing, no matter how intelligent machines are, they are unable to completely replace humans. Therefore, it is necessary to introduce human cognitive capabilities or human-like cognitive models into AI systems to develop a new form of AI, that is, hybrid-augmented intelligence. This form of AI or machine intelligence is a feasible and important developing model. Hybrid-augmented intelligence can be divided into two basic models: one is human-in-the-loop augmented intelligence with human-computer collaboration, and the other is cognitive computing based augmented intelligence, in which a cognitive model is embedded in the machine learning system. This survey describes a basic framework for human-computer collaborative hybrid-augmented intelligence, and the basic elements of hybrid-augmented intelligence based on cognitive computing. These elements include intuitive reasoning, causal models, evolution of memory and knowledge, especially the role and basic principles of intuitive reasoning for complex problem solving, and the cognitive learning framework for visual scene understanding based on memory and reasoning. Several typical applications of hybrid-augmented intelligence in related fields are given.},
  creationdate     = {2022-11-23T08:45:09},
  doi              = {10.1631/fitee.1700053},
  file             = {:Zheng_2017_Hybrid Augmented Intelligence_ Collaboration and Cognition.pdf:PDF},
  groups           = {Hybrid Intelligence},
  keywords         = {Human-machine collaboration; Hybrid-augmented intelligence; Cognitive computing; Intuitive reasoning; Causal model; Cognitive mapping; Visual scene understanding; Self-driving cars},
  modificationdate = {2023-04-04T16:07:45},
  publisher        = {Zhejiang University Press},
}

 
@InProceedings{Stern2019MAPF,
  author           = {Stern, Roni and Sturtevant, Nathan R. and Felner, Ariel and Koenig, Sven and Ma, Hang and Walker, Thayne T. and Li, Jiaoyang and Atzmon, Dor and Cohen, Liron and Kumar, T. K. Satish and Barták, Roman and Boyarski, Eli},
  booktitle        = {Twelfth {Annual} {Symposium} on {Combinatorial} {Search}},
  title            = {Multi-{Agent} {Pathfinding}: {Definitions}, {Variants}, and {Benchmarks}},
  year             = {2019},
  month            = jul,
  abstract         = {The multi-agent pathfinding problem (MAPF) is the fundamental problem of planning paths for multiple agents, where the key constraint is that the agents will be able to follow these paths concurrently without colliding with each other. Applications of MAPF include automated warehouses, autonomous vehicles, and robotics. Research on MAPF has been flourishing in the past couple of years. Different MAPF research papers assume different sets of assumptions, e.g., whether agents can traverse the same road at the same time, and have different objective functions, e.g., minimize makespan or sum of agents' actions costs. These assumptions and objectives are sometimes implicitly assumed or described informally. This makes it difficult for establishing appropriate baselines for comparison in research papers, as well as making it difficult for practitioners to find the papers relevant to their concrete application. This paper aims to fill this gap and facilitate future research and practitioners by providing a unifying terminology for describing the common MAPF assumptions and objectives. In addition, we also provide pointers to two MAPF benchmarks. In particular, we introduce a new grid-based benchmark for MAPF, and demonstrate experimentally that it poses a challenge to contemporary MAPF algorithms.},
  copyright        = {Authors who publish a paper in this conference agree to the following terms: Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright. The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys' fees incurred therein. Author(s) retain all proprietary rights other than copyright (such as patent rights). Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship. Author(s) may reproduce, or have reproduced, their article/paper for the author\&rsquo;s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author's employer, and then only on the author's or the employer\&rsquo;s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author's or the employer's creation (including tables of contents with links to other papers) without AAAI's written permission. Author(s) may make limited distribution of all or portions of their article/paper prior to publication. In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes. In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
  creationdate     = {2022-11-23T08:54:34},
  file             = {:Stern_2019_Multi Agent Pathfinding_ Definitions, Variants, and Benchmarks.pdf:PDF},
  groups           = {Planning},
  language         = {en},
  modificationdate = {2023-03-31T08:18:57},
  shorttitle       = {Multi-{Agent} {Pathfinding}},
  url              = {https://www.aaai.org/ocs/index.php/SOCS/SOCS19/paper/view/18341},
  urldate          = {2022-11-23},
}

@InProceedings{Kamar2016Directions,
  author           = {Kamar, Ece},
  booktitle        = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
  title            = {Directions in Hybrid Intelligence: Complementing AI Systems with Human Intelligence},
  year             = {2016},
  address          = {New York, New York, USA},
  pages            = {4070–4073},
  publisher        = {AAAI Press},
  series           = {IJCAI'16},
  abstract         = {Hybrid intelligence systems combine machine and human intelligence to overcome the shortcomings of existing AI systems. This paper reviews recent research efforts towards developing hybrid systems focusing on reasoning methods for optimizing access to human intelligence and on gaining comprehensive understanding of humans as helpers of AI systems. It concludes by discussing short and long term research directions.},
  creationdate     = {2022-11-23T09:46:41},
  doi              = {10.5555/3061053.3061219},
  file             = {:Kamar_2016_Directions in Hybrid Intelligence_ Complementing AI Systems with Human Intelligence.pdf:PDF},
  groups           = {Hybrid Intelligence},
  isbn             = {9781577357704},
  modificationdate = {2022-11-23T09:47:07},
  numpages         = {4},
}

@InProceedings{Hemmer2021Human,
  author           = {Hemmer, Patrick; Schemmer, Max; Vössing, Michael; and Kühl, Niklas},
  booktitle        = {PACIS 2021 Proceedings},
  title            = {Human-AI Complementarity in Hybrid Intelligence Systems: A Structured Literature Review},
  year             = {2021},
  number           = {78},
  abstract         = {Hybrid Intelligence is an emerging concept that emphasizes the complementary nature of human intelligence and artificial intelligence (AI). One key requirement for collaboration between humans and AI is the interpretability of the decisions provided by the AI to enable humans to assess whether to comply with the presented decisions. Due to the black-box nature of state-of-the-art AI, the explainable AI (XAI) research community has developed various means to increase interpretability. However, many studies show that increased interpretability through XAI does not necessarily result in complementary team performance (CTP). Through a structured literature review, we identify relevant factors that influence collaboration between humans and AI. Additionally, as we collect relevant research articles and synthesize their findings, we develop a research agenda with relevant hypotheses to lay the foundation for future research on human-AI complementarity in Hybrid Intelligence systems.},
  creationdate     = {2022-11-23T09:51:29},
  file             = {:Hemmer_2021_Human AI Complementarity in Hybrid Intelligence Systems_ a Structured Literature Review.pdf:PDF},
  groups           = {Hybrid Intelligence},
  modificationdate = {2022-11-24T13:36:30},
}

@InCollection{Bosch2019Six,
  author           = {Karel van den Bosch and Tjeerd Schoonderwoerd and Romy Blankendaal and Mark Neerincx},
  booktitle        = {Adaptive Instructional Systems},
  publisher        = {Springer International Publishing},
  title            = {Six Challenges for Human-{AI} Co-learning},
  year             = {2019},
  pages            = {572--589},
  abstract         = {The increasing use of ever-smarter AI-technology is changing the way individuals and teams learn and perform their tasks. In hybrid teams, people collaborate with artificially intelligent partners. To utilize the different strengths and weaknesses of human and artificial intelligence, a hybrid team should be designed upon the principles that foster successful human-machine learning and cooperation. The implementation of the identified principles sets a number of challenges. Machine agents should, just like humans, have mental models that contain information about the task context, their own role (self-awareness), and the role of others (theory of mind). Furthermore, agents should be able to express and clarify their mental states to partners. In this paper we identify six challenges for humans and machines to collaborate in an adaptive, dynamic and personalized fashion. Implications for research are discussed.},
  creationdate     = {2022-11-23T10:15:01},
  doi              = {10.1007/978-3-030-22341-0_45},
  file             = {:Bosch_2019_Six Challenges for Human AI Co Learning.pdf:PDF},
  groups           = {Hybrid Intelligence},
  keywords         = {Co-active learning, Human-agent teaming, Hybrid teams, Theory of mind, Explainable AI, Mental model},
  modificationdate = {2022-11-24T13:36:35},
}

@Book{Abbass2021Shepherding,
  author           = {Abbass, Hussein A. and Hunjet, Robert A.},
  publisher        = {Springer International Publishing AG},
  title            = {Shepherding Uxvs for Human-Swarm Teaming},
  year             = {2021},
  isbn             = {9783030608972},
  abstract         = {human和swarm之间的双向信息流，比较偏总体理论研究},
  creationdate     = {2022-11-23T22:18:27},
  file             = {:Abbass_2021_Shepherding Uxvs for Human Swarm Teaming.pdf:PDF},
  groups           = {Human-Swarm},
  modificationdate = {2022-11-25T09:20:31},
  subtitle         = {An Artificial Intelligence Approach to Unmanned X Vehicles},
}

 
@InProceedings{Dong2022Integrating,
  author           = {Dong, Xishuang and Sarker, Shouvon and Qian, Lijun},
  booktitle        = {2022 {International} {Conference} on {Intelligent} {Data} {Science} {Technologies} and {Applications} ({IDSTA})},
  title            = {Integrating {Human}-in-the-loop into {Swarm} {Learning} for {Decentralized} {Fake} {News} {Detection}},
  year             = {2022},
  month            = sep,
  pages            = {46--53},
  abstract         = {Social media has become an effective platform to generate and spread fake news that can mislead people and even distort public opinion. Centralized methods for fake news detection, however, cannot effectively protect user privacy during the process of centralized data collection for training models. Moreover, it cannot fully involve user feedback in the loop of learning detection models for further enhancing fake news detection. To overcome these challenges, this paper proposed a novel decentralized method, Human-in-the-loop Based Swarm Learning (HBSL), to integrate user feedback into the loop of learning and inference for recognizing fake news without violating user privacy in a decentralized manner. It consists of distributed nodes that are able to independently learn and detect fake news on local data. Furthermore, detection models trained on these nodes can be enhanced through decentralized model merging. Experimental results demonstrate that the proposed method outperforms the state-of-the-art decentralized method in regard of detecting fake news on a benchmark dataset.},
  comment          = {人在回路的分布式假新闻检测
swarm learning},
  creationdate     = {2022-11-24T13:34:34},
  doi              = {10.1109/IDSTA55301.2022.9923043},
  file             = {:Dong_2022_Integrating Human in the Loop into Swarm Learning for Decentralized Fake News Detection.pdf:PDF},
  groups           = {Hybrid Intelligence},
  keywords         = {Training, Privacy, Social networking (online), Merging, Distributed databases, Benchmark testing, Data science, Fake News Detection, Swarm Learning, Human- in-the-loop (HITL), Social Media},
  modificationdate = {2022-11-24T13:39:30},
}

@Misc{Suresh2018Gesture,
  author           = {Aamodh Suresh and Sonia Martinez},
  month            = apr,
  title            = {Gesture based Human-Swarm Interactions for Formation Control using interpreters},
  year             = {2018},
  abstract         = {We propose a novel Human-Swarm Interaction (HSI) framework which enables the user to control a swarm shape and formation. The user commands the swarm utilizing just arm gestures and motions which are recorded by an off-the-shelf wearable armband. We propose a novel interpreter system, which acts as an intermediary between the user and the swarm to simplify the user's role in the interaction. The interpreter takes in a high level input drawn using gestures by the user, and translates it into low level swarm control commands. This interpreter employs machine learning, Kalman filtering and optimal control techniques to translate the user input into swarm control parameters. A notion of Human Interpretable dynamics is introduced, which is used by the interpreter for planning as well as to provide feedback to the user. The dynamics of the swarm are controlled using a novel decentralized formation controller based on distributed linear iterations and dynamic average consensus. The framework is demonstrated theoretically as well as experimentally in a 2D environment, with a human controlling a swarm of simulated robots in real time.},
  archiveprefix    = {arXiv},
  comment          = {提出了一种新的HSI，支持用户通过手势和行动来控制群体形状和队形
采用机器学习、卡尔曼滤波和最优控制技术将用户输入转化为群体控制参数
引入了人类可解释动力学的概念，解释器将其用于计划以及向用户提供反馈},
  creationdate     = {2022-11-24T13:47:11},
  eprint           = {1804.08676},
  file             = {:Suresh_2018_Gesture Based Human Swarm Interactions for Formation Control Using Interpreters.pdf:PDF},
  groups           = {Human-Swarm},
  keywords         = {cs.RO, cs.MA, cs.SY},
  modificationdate = {2022-11-24T13:49:23},
  primaryclass     = {cs.RO},
}

@Article{Agrawal2021Explaining,
  author           = {Ankit Agrawal and Jane Cleland-Huang},
  journal          = {Proceedings of the {AAAI} Conference on Human Computation and Crowdsourcing},
  title            = {Explaining Autonomous Decisions in Swarms of Human-on-the-Loop Small Unmanned Aerial Systems},
  year             = {2021},
  month            = oct,
  pages            = {15--26},
  volume           = {9},
  abstract         = {Rapid advancements in Artificial Intelligence have shifted the focus from traditional human-directed robots to fully autonomous ones that do not require explicit human control. These are commonly referred to as Human-on-the-Loop (HotL) systems. Transparency of HotL systems necessitates clear explanations of autonomous behavior so that humans are aware of what is happening in the environment and can understand why robots behave in a certain way. However, in complex multi-robot environments, especially those in which the robots are autonomous and mobile, humans may struggle to maintain situational awareness. Presenting humans with rich explanations of autonomous behavior tends to overload them with lots of information and negatively affect their understanding of the situation. Therefore, explaining the autonomous behavior of multiple robots creates a design tension that demands careful investigation. This paper examines the User Interface (UI) design trade-offs associated with providing timely and detailed explanations of autonomous behavior for swarms of small Unmanned Aerial Systems (sUAS) or drones. We analyze the impact of UI design choices on human awareness of the situation. We conducted multiple user studies with both inexperienced and expert sUAS operators to present our design solution and initial guidelines for designing the HotL multi-sUAS interface.},
  comment          = {考虑人在回路的智能系统，transparency定义为人可以理解群体系统的态势以及swarm的自主行动逻辑
但提供过多的信息会导致human的信息过载，因此需要好好设计交互界面，更有利于human的态势感知
本文研究了与为小型无人驾驶航空系统 (sUAS) 或无人机群提供及时和详细的自主行为解释相关的用户界面 (UI) 设计权衡。
通过真人实验，提供了一些基本的交互设计建议

可以参考本文一些human-AI的实验设计，以及一些定量分析方法},
  creationdate     = {2022-11-25T09:04:07},
  doi              = {10.1609/hcomp.v9i1.18936},
  file             = {:Agrawal_2021_Explaining Autonomous Decisions in Swarms of Human on the Loop Small Unmanned Aerial Systems.pdf:PDF},
  groups           = {Human-Swarm},
  keywords         = {Human-on-the-loop, Situation Awareness, System Interface, Human Computation},
  modificationdate = {2023-04-04T16:07:46},
  publisher        = {Association for the Advancement of Artificial Intelligence ({AAAI})},
  ranking          = {rank3},
}

@InProceedings{Serpiva2021SwarmPaint,
  author           = {Valerii Serpiva and Ekaterina Karmanova and Aleksey Fedoseev and Stepan Perminov and Dzmitry Tsetserukou},
  booktitle        = {2021 International Conference on Unmanned Aircraft Systems ({ICUAS})},
  title            = {{SwarmPaint}: Human-Swarm Interaction for Trajectory Generation and Formation Control by {DNN}-based Gesture Interface},
  year             = {2021},
  month            = jun,
  publisher        = {{IEEE}},
  abstract         = {Teleoperation tasks with multi-agent systems have a high potential in supporting human-swarm collaborative teams in exploration and rescue operations. However, it requires an intuitive and adaptive control approach to ensure swarm stability in a cluttered and dynamically shifting environment. We propose a novel human-swarm interaction system, allowing the user to control swarm position and formation by either direct hand motion or by trajectory drawing with a hand gesture interface based on the DNN gesture recognition. The key technology of the SwarmPaint is the user's ability to perform various tasks with the swarm without additional devices by switching between interaction modes. Two types of interaction were proposed and developed to adjust a swarm behavior: free-form trajectory generation control and shaped formation control. Two preliminary user studies were conducted to explore user's performance and subjective experience from human-swarm interaction through the developed control modes. The experimental results revealed a sufficient accuracy in the trajectory tracing task (mean error of 5.6 cm by gesture draw and 3.1 cm by mouse draw with the pattern of dimension 1 m by 1 m) over three evaluated trajectory patterns and up to 7.3 cm accuracy in targeting task with two target patterns of 1 m achieved by SwarmPaint interface. Moreover, the participants evaluated the trajectory drawing interface as more intuitive (12.9%) and requiring less effort to utilize (22.7%) than direct shape and position control by gestures, although its physical workload and failure in performance were presumed as more significant (by 9.1% and 16.3%, respectively). The proposed SwarmPaint technology can be potentially applied in various human-swarm scenarios, including complex environment exploration, dynamic lighting generation, and interactive drone shows, allowing users to actively participate in the swarm behavior decision on a different scale of control.},
  comment          = {设计了一种基于手势识别的交互方式，来控制群体编队
两种调整方式：free-form trajectory generation control and shaped formation control
结论：队形控制精度足够高，绘制方式更直观、省力，但体力负荷较大、容易fail},
  creationdate     = {2022-11-25T09:14:54},
  doi              = {10.1109/icuas51884.2021.9476795},
  file             = {:Serpiva_2021_SwarmPaint_ Human Swarm Interaction for Trajectory Generation and Formation Control by DNN Based Gesture Interface.pdf:PDF},
  groups           = {Human-Swarm},
  modificationdate = {2023-04-04T16:07:57},
}

 
@Article{Macchini2021Personalized,
  author           = {Macchini, Matteo and De Matteïs, Ludovic and Schiano, Fabrizio and Floreano, Dario},
  journal          = {IEEE Robotics and Automation Letters},
  title            = {Personalized {Human}-{Swarm} {Interaction} {Through} {Hand} {Motion}},
  year             = {2021},
  issn             = {2377-3766},
  number           = {4},
  pages            = {8341--8348},
  volume           = {6},
  abstract         = {The control of collective robotic systems, such as drone swarms, is often delegated to autonomous navigation algorithms due to their high dimensionality. However, like other robotic entities, drone swarms can still benefit from being teleoperated by human operators, whose perception and decision-making capabilities are still out of the reach of autonomous systems. Drone swarm teleoperation is only at its dawn, and a standard human-swarm interface (HSI) is missing to date. In this study, we analyzed the spontaneous interaction strategies of naive users with a swarm of drones. We implemented a machine-learning algorithm to define a personalized Body-Machine Interface (BoMI) based only on a short calibration procedure. During this procedure, the human operator is asked to move spontaneously as if they were in control of a simulated drone swarm. We assessed that hands are the most commonly adopted body segment, and thus we chose a LEAP Motion controller to track them to let the users control the aerial drone swarm. This choice makes our interface portable since it does not rely on a centralized system for tracking the human body. We validated our HSIs generation algorithm on a set of participants in a realistic simulated environment, showing a positive user feedback and performance comparable with a remote controller after training. Our method leaves the user free to choose between position and velocity control only based on their body motion preferences.},
  comment          = {无人机群的感知和决策能力还需要人的帮助，但目前缺乏标准的人机交互界面
通过实验发现，human主要用手部动作来操作机群
本文基于机器学习，基于简短的校准就能定义个性化的交互接口，跟踪human手势来控制无人机群
允许用户根据身体偏好选择位置/速度控制},
  creationdate     = {2022-11-25T09:21:20},
  doi              = {10.1109/LRA.2021.3102324},
  file             = {:Macchini_2021_Personalized Human Swarm Interaction through Hand Motion.pdf:PDF},
  groups           = {Human-Swarm},
  keywords         = {Drones, Robots, Motion segmentation, Tracking, Task analysis, Standards, Kinematics, AI-Based methods, learning and adaptive systems, swarm robotics, telerobotics and teleoperation, wearablerobots},
  modificationdate = {2022-11-25T21:37:15},
}

@Misc{Kakish2021Towards,
  author           = {Zahi Kakish and Sritanay Vedartham and Spring Berman},
  month            = feb,
  title            = {Towards Decentralized Human-Swarm Interaction by Means of Sequential Hand Gesture Recognition},
  year             = {2021},
  abstract         = {In this work, we present preliminary work on a novel method for Human-Swarm Interaction (HSI) that can be used to change the macroscopic behavior of a swarm of robots with decentralized sensing and control. By integrating a small yet capable hand gesture recognition convolutional neural network (CNN) with the next-generation Robot Operating System \emph{ros2}, which enables decentralized implementation of robot software for multi-robot applications, we demonstrate the feasibility of programming a swarm of robots to recognize and respond to a sequence of hand gestures that capable of correspond to different types of swarm behaviors. We test our approach using a sequence of gestures that modifies the target inter-robot distance in a group of three Turtlebot3 Burger robots in order to prevent robot collisions with obstacles. The approach is validated in three different Gazebo simulation environments and in a physical testbed that reproduces one of the simulated environments.},
  archiveprefix    = {arXiv},
  comment          = {用手势来控制swarm},
  creationdate     = {2022-11-25T21:28:15},
  eprint           = {2102.02439},
  file             = {:Kakish_2021_Towards Decentralized Human Swarm Interaction by Means of Sequential Hand Gesture Recognition.pdf:PDF},
  groups           = {Human-Swarm},
  keywords         = {cs.RO, cs.MA},
  modificationdate = {2022-11-25T21:36:17},
  primaryclass     = {cs.RO},
}

@InProceedings{Distefano2021Using,
  author           = {Joseph P. Distefano and Hemanth Manjunatha and Souma Chowdhury and Karthik Dantu and David Doermann and Ehsan T. Esfahani},
  booktitle        = {2021 {IEEE} International Conference on Systems, Man, and Cybernetics ({SMC})},
  title            = {Using Physiological Information to Classify Task Difficulty in Human-Swarm Interaction},
  year             = {2021},
  month            = oct,
  publisher        = {{IEEE}},
  comment          = {当任务难度增加，human operator表现会下降，因此需要识别任务难度，自适应将任务分配给human
同时考虑UAV和UGV，部分客观环境
涉及了脑科学},
  creationdate     = {2022-11-25T21:38:47},
  doi              = {10.1109/smc52423.2021.9658653},
  file             = {:Distefano_2021_Using Physiological Information to Classify Task Difficulty in Human Swarm Interaction.pdf:PDF},
  groups           = {Human-Swarm},
  modificationdate = {2023-04-04T16:07:58},
}

@InProceedings{Mirri2019Human,
  author           = {Silvia Mirri and Catia Prandi and Paola Salomoni},
  booktitle        = {Proceedings of the {ACM} {SIGCOMM} 2019 Workshop on Mobile {AirGround} Edge Computing, Systems, Networks, and Applications - {MAGESys}{\textquotesingle}19},
  title            = {Human-Drone Interaction: state of the art, open issues and challenges},
  year             = {2019},
  publisher        = {{ACM} Press},
  abstract         = {In the evolution of the Human-Computer Interaction discipline, it is interesting to evaluate the users’ experience within the interaction between users and a specific category of robots, which are characterized by peculiar features: the unmanned aerial vehicles (UAVs). Drones are becoming more and more diffused, being used with different purposes. Hence, the interaction with these devices is geing common and here we aim at investigating how they can be exploited by means of different users’ interfaces and with different interaction mechanisms.
In this paper, we present a review of the state of the art in the context of the human-drone interaction, so as to study and discuss the main open issues and challenges currently highlighted and reported in research projects and papers available in the current literature.},
  comment          = {在人机交互学科的发展过程中，评估用户与特定类别机器人交互中的用户体验很有趣，这些机器人具有独特的特征：无人驾驶飞行器 (UAV)。无人机正变得越来越普及，被用于不同的目的。因此，与这些设备的交互变得越来越普遍，我们的目标是研究如何通过不同的用户界面和不同的交互机制来利用它们。在本文中，我们在人机交互的背景下对现有技术进行了回顾，以研究和讨论当前文献中可用的研究项目和论文中突出和报告的主要开放问题和挑战},
  creationdate     = {2022-11-25T21:46:09},
  doi              = {10.1145/3341568.3342111},
  file             = {:Mirri_2019_Human Drone Interaction_ State of the Art, Open Issues and Challenges.pdf:PDF},
  groups           = {Human-Swarm},
  modificationdate = {2023-02-14T16:40:58},
}

@Book{Mirsky2021Book,
  author           = {Reuth Mirsky and Sarah Keren and Christopher Geib},
  publisher        = {Springer International Publishing},
  title            = {Introduction to Symbolic Plan and Goal Recognition},
  year             = {2021},
  creationdate     = {2022-12-01T20:54:45},
  doi              = {10.1007/978-3-031-01589-2},
  file             = {:Mirsky_2021_Introduction to Symbolic Plan and Goal Recognition.pdf:PDF},
  modificationdate = {2022-12-01T20:55:24},
}

@Misc{Vecchio2022MIDGARD,
  author           = {Giuseppe Vecchio and Simone Palazzo and Dario C. Guastella and Ignacio Carlucho and Stefano V. Albrecht and Giovanni Muscato and Concetto Spampinato},
  month            = may,
  title            = {MIDGARD: A Simulation Platform for Autonomous Navigation in Unstructured Environments},
  year             = {2022},
  abstract         = {We present MIDGARD, an open-source simulation platform for autonomous robot navigation in outdoor unstructured environments. MIDGARD is designed to enable the training of autonomous agents (e.g., unmanned ground vehicles) in photorealistic 3D environments, and to support the generalization skills of learning-based agents through the variability in training scenarios. MIDGARD's main features include a configurable, extensible, and difficulty-driven procedural landscape generation pipeline, with fast and photorealistic scene rendering based on Unreal Engine. Additionally, MIDGARD has built-in support for OpenAI Gym, a programming interface for feature extension (e.g., integrating new types of sensors, customizing exposing internal simulation variables), and a variety of simulated agent sensors (e.g., RGB, depth and instance/semantic segmentation). We evaluate MIDGARD's capabilities as a benchmarking tool for robot navigation utilizing a set of state-of-the-art reinforcement learning algorithms. The results demonstrate MIDGARD's suitability as a simulation and training environment, as well as the effectiveness of our procedural generation approach in controlling scene difficulty, which directly reflects on accuracy metrics. MIDGARD build, source code and documentation are available at https://midgardsim.org/.},
  archiveprefix    = {arXiv},
  comment          = {UGV导航是一个有价值的挑战，尤其是再非结构化、可变、杂乱的真实环境。真实场景训练成本高，速度慢，因此对于模拟环境有所需求。

有一些面向室内导航的仿真环境，面向户外的大部分着重城市环境

利用UE提供先进的渲染技术，提供逼真的导航环境

提供了一个于Gym兼容的python接口，可以实现agent与环境之间快速可靠的通信},
  creationdate     = {2022-12-02T21:25:59},
  eprint           = {2205.08389},
  file             = {:Vecchio_2022_MIDGARD_ a Simulation Platform for Autonomous Navigation in Unstructured Environments.pdf:PDF},
  groups           = {RL_Simulation},
  keywords         = {cs.RO},
  modificationdate = {2022-12-02T22:02:54},
  primaryclass     = {cs.RO},
}

 
@TechReport{Alford2015Active,
  author           = {Alford, Ron and Borck, Hayley and Karneeb, Justin and Aha, David W.},
  title            = {Active {Behavior} {Recognition} in {Beyond} {Visual} {Range} {Air} {Combat}},
  year             = {2015},
  abstract         = {Accurately modeling uncontrolled agents or recognizing their behavior and intentions is critical to planning and acting in a multi-agent environment. However, behavior recognition systems are only as good as their observations. Here we argue that acting, even acting at random, can be a critical part of gathering those observations. Furthermore, we claim that acting intelligently via automated planning can significantly reduce the time it takes to confidently classify agent behaviors. We present a formalism and algorithm for integrated planning and recognition, as well as its implementation in a beyond visual range air combat simulator. We found that it yields better behavior recognition than non-integrated approaches. This provides evidence that behavior recognition is not just a necessary component of an intelligent agent, but that good behavior recognition requires intelligent acting.},
  chapter          = {Technical Reports},
  creationdate     = {2022-12-03T10:52:38},
  file             = {:Alford_2015_Active Behavior Recognition in beyond Visual Range Air Combat.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  language         = {en},
  modificationdate = {2022-12-03T10:54:23},
  url              = {https://apps.dtic.mil/sti/citations/ADA626583},
  urldate          = {2022-12-03},
}

 
@InProceedings{Shafipour2021Task,
  author           = {Shafipour, Elnaz and Fallah, Saber},
  booktitle        = {Towards {Autonomous} {Robotic} {Systems}},
  title            = {Task-{Based} {Ad}-hoc {Teamwork} with {Adversary}},
  year             = {2021},
  address          = {Cham},
  editor           = {Fox, Charles and Gao, Junfeng and Ghalamzan Esfahani, Amir and Saaj, Mini and Hanheide, Marc and Parsons, Simon},
  pages            = {76--87},
  publisher        = {Springer International Publishing},
  series           = {Lecture {Notes} in {Computer} {Science}},
  abstract         = {Many real-world applications require agents to cooperate and collaborate to accomplish shared missions; though, there are many instances where the agents should work together without communication or prior coordination. In the meantime, agents often coordinate in a decentralised manner to complete tasks that are displaced in an environment (e.g., foraging, demining, rescue or firefighting). Each agent in the team is responsible for selecting their own task and completing it autonomously. However, there is a possibility of an adversary in the team, who tries to prevent other agents from achieving their goals. In this study, we assume there is an agent who estimates the model of other agents in the team to boost the team’s performance regardless of the enemy’s attacks. Hence, we present On-line Estimators for Ad-hoc Task Allocation with Adversary (OEATA-A), a novel algorithm to have better estimations of the teammates’ future behaviour, which includes identifying enemies among friends.},
  comment          = {存在敌对势力，阻碍任务的完成
有一个基于推断任务分配模块 OEATA-a},
  creationdate     = {2022-12-07T10:07:11},
  doi              = {10.1007/978-3-030-89177-0_8},
  file             = {Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2F978-3-030-89177-0_8.pdf:application/pdf},
  groups           = {Ad Hoc Teamwork},
  isbn             = {9783030891770},
  keywords         = {Autonomous systems, Adversary agent, Learning agent, Multi-agent system, Decentralised task allocation},
  language         = {en},
  modificationdate = {2022-12-07T10:59:46},
}

@Article{Xie2020Learning,
  author           = {Annie Xie and Dylan P. Losey and Ryan Tolsma and Chelsea Finn and Dorsa Sadigh},
  title            = {Learning Latent Representations to Influence Multi-Agent Interaction},
  year             = {2020},
  month            = nov,
  abstract         = {Seamlessly interacting with humans or robots is hard because these agents are non-stationary. They update their policy in response to the ego agent's behavior, and the ego agent must anticipate these changes to co-adapt. Inspired by humans, we recognize that robots do not need to explicitly model every low-level action another agent will make; instead, we can capture the latent strategy of other agents through high-level representations. We propose a reinforcement learning-based framework for learning latent representations of an agent's policy, where the ego agent identifies the relationship between its behavior and the other agent's future strategy. The ego agent then leverages these latent dynamics to influence the other agent, purposely guiding them towards policies suitable for co-adaptation. Across several simulated domains and a real-world air hockey game, our approach outperforms the alternatives and learns to influence the other agent.},
  archiveprefix    = {arXiv},
  comment          = {元学习 隐变量编码},
  creationdate     = {2022-12-07T10:58:42},
  eprint           = {2011.06619},
  file             = {:Xie_2020_Learning Latent Representations to Influence Multi Agent Interaction.pdf:PDF},
  keywords         = {cs.RO, cs.AI, cs.LG},
  modificationdate = {2022-12-07T11:07:49},
  primaryclass     = {cs.RO},
}

@InProceedings{Zhao2022Crowd,
  author           = {Zhao, Yong and Zhu, Zhengqiu and Chen, Bin and Qiu, Sihang},
  booktitle        = {ChineseCSCW 2022},
  title            = {Crowd-Powered Source Searching in Complex Environments},
  year             = {2022},
  pages            = {1--15},
  abstract         = {Source searching algorithms are widely used in different domains and for various applications, for instance, to find gas or signal sources. As source searching algorithms advance, search problems need to be addressed in increasingly complex environments. Such environments could be high-dimensional and highly dynamic. Therefore, novel search algorithms have been designed, combining heuristic methods and intelligent optimization, to tackle search problems in large and complex search space. However, these intelligent search algorithms usually cannot guarantee completeness and optimality, and therefore commonly suffer from the problems such as local optimum. Recent studies have used crowd-powered systems to address the complex problems that machines cannot solve on their own. While leveraging human rationales in a computer system has been shown to be effective in making a system more reliable, whether using the power of the crowd can improve source searching algorithms remains unanswered. To this end, we propose a crowd-powered sourcing search approach, using human rationales as external supports to improve existing search algorithms, and meanwhile to minimize the human effort using machine predictions. Furthermore, we designed a prototype system, and carried out an experiment with 10 participants (4 experts and 6 non-experts). Quantitative and qualitative analysis showed that the sourcing search algorithm enhanced by crowd could achieve both high effectiveness and efficiency. Our work provides valuable insights in human-computer collaborative system design.},
  comment          = {人在回路的寻源问题},
  creationdate     = {2022-12-08T23:33:11},
  file             = {:Zhao_2022_Crowd Powered Source Searching in Complex Environments.pdf:PDF;:(CSWC2022)Zhao_2022_Human-AI Collaboration for Improving Search Algorithms.pdf:PDF},
  groups           = {Human-Swarm},
  modificationdate = {2023-02-19T12:27:43},
  url              = {https://www.researchgate.net/publication/365774893_Crowd-Powered_Source_Searching_in_Complex_Environments},
}

 
@InProceedings{Jia2022Wildfire,
  author           = {Jia, Qiong and Xin, Ming and Hu, Xiaolin and Chao, Haiyang},
  booktitle        = {2022 American Control Conference (ACC)},
  title            = {Learning-based {Wildfire} {Tracking} with {Unmanned} {Aerial} {Vehicles}},
  year             = {2022},
  month            = jun,
  note             = {ISSN: 2378-5861},
  pages            = {3212--3217},
  abstract         = {This paper designs a path planning algorithm for a group of unmanned aerial vehicles (UAVs) to track multiple spreading wildfire zones. Due to limited observable information, the fire evolution is hard to model. A regression neural network is online trained with real-time UAV observation data and applied for fire front prediction. To track fire fronts effectively, a UAV path planning algorithm is proposed by Q-learning. Various practical factors are taken into account by cost function designs such as moving target tracking, field of view of UAVs, spreading speed of fire zones, collision/obstacle avoidance, and maximum information collection. Simulation results validate the fire prediction accuracy and UAV tracking performance.},
  comment          = {野火蔓延跟踪},
  creationdate     = {2023-01-05T20:50:05},
  doi              = {10.23919/ACC53348.2022.9867512},
  file             = {:Jia_2022_Learning Based Wildfire Tracking with Unmanned Aerial Vehicles.pdf:PDF},
  issn             = {2378-5861},
  keywords         = {Target tracking, Q-learning, Simulation, Neural networks, Fires, Autonomous aerial vehicles, Prediction algorithms},
  modificationdate = {2023-01-06T15:01:57},
}

 
@Article{Li2021Cooperative,
  author           = {Li, Lili and Zhang, Xiaoyong and Yue, Wei and Liu, Zhongchang},
  journal          = {ISA Transactions},
  title            = {Cooperative search for dynamic targets by multiple {UAVs} with communication data losses},
  year             = {2021},
  issn             = {0019-0578},
  month            = aug,
  pages            = {230--241},
  volume           = {114},
  abstract         = {This paper studies the problem of cooperative searching for dynamical moving targets by multiple unmanned aerial vehicles (UAVs). The environmental information possessed by UAVs is inconsistent due to packet losses of shared environmental information in communication channels and the discrepancies of detected information among different UAVs. To unify the environmental information among UAVs, the lost information is compensated for by an improved Least Square Method (LSM) which incorporates the target location model into the fitting function to enhance data fitting precision. The Weighted Averaging Method (WAM) is used to merge multiple source information where the weight coefficients are set based on the uncertain values of environmental information. To search for dynamic targets and then automatically re-enter into search areas for UAVs, a Modified Genetic Algorithm (MGA) and rolling optimization techniques are utilized to generate real-time paths for UAVs. Simulation results and comparison studies with existing methods validate the effectiveness of the above cooperative searching strategy.},
  comment          = {值得参考},
  creationdate     = {2023-01-12T16:30:52},
  doi              = {10.1016/j.isatra.2020.12.055},
  file             = {:Li_2021_Cooperative Search for Dynamic Targets by Multiple UAVs with Communication Data Losses.pdf:PDF},
  groups           = {Swarm Intelligence},
  keywords         = {Multiple UAVs, Cooperative searching, Real-time path planning, Environmental information inconsistency},
  language         = {en},
  modificationdate = {2023-02-01T16:05:29},
  url              = {https://www.sciencedirect.com/science/article/pii/S0019057820305747},
  urldate          = {2023-01-12},
}

@Article{Yu2015Cooperative,
  author           = {Huili Yu and Kevin Meier and Matthew Argyle and Randal W. Beard},
  journal          = {{IEEE}/{ASME} Transactions on Mechatronics},
  title            = {Cooperative Path Planning for Target Tracking in Urban Environments Using Unmanned Air and Ground Vehicles},
  year             = {2015},
  month            = apr,
  number           = {2},
  pages            = {541--552},
  volume           = {20},
  comment          = {使用UAV和UGV跟踪城市地形中的运动目标
该算法考虑了由于环境中的障碍物造成的视觉遮挡。我们使用动态占用网格对目标状态进行建模，并使用二阶马尔可夫模型来表示目标运动。
设计了一种依赖拍卖方案的分散规划算法，该算法在有限的前瞻窗口内最大化UAV和UGV团队的联合检测概率之和},
  creationdate     = {2023-01-12T16:57:16},
  doi              = {10.1109/tmech.2014.2301459},
  file             = {:Yu_2015_Cooperative Path Planning for Target Tracking in Urban Environments Using Unmanned Air and Ground Vehicles.pdf:PDF},
  groups           = {Swarm Intelligence},
  modificationdate = {2023-04-04T16:08:00},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Li2020Path,
  author           = {Jing Li and Yonghua Xiong and Jinhua She and Min Wu},
  journal          = {{IEEE} Internet of Things Journal},
  title            = {A Path Planning Method for Sweep Coverage With Multiple {UAVs}},
  year             = {2020},
  month            = sep,
  number           = {9},
  pages            = {8967--8978},
  volume           = {7},
  comment          = {多无人机覆盖扫描
多个二维离散目标，多个无人机，续航有限，需要在最小时间内，覆盖最多目标并回到base},
  creationdate     = {2023-01-12T16:59:34},
  doi              = {10.1109/jiot.2020.2999083},
  file             = {:Li_2020_A Path Planning Method for Sweep Coverage with Multiple UAVs.pdf:PDF},
  groups           = {Swarm Intelligence},
  modificationdate = {2023-04-04T16:08:01},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

 
@Article{Pyke2021Dynamic,
  author           = {Pyke, Lewis M. and Stark, Craig R.},
  journal          = {Frontiers in Applied Mathematics and Statistics},
  title            = {Dynamic {Pathfinding} for a {Swarm} {Intelligence} {Based} {UAV} {Control} {Model} {Using} {Particle} {Swarm} {Optimisation}},
  year             = {2021},
  issn             = {2297-4687},
  volume           = {7},
  abstract         = {In recent years unmanned aerial vehicles (UAVs) have become smaller, cheaper, and more efficient, enabling the use of multiple autonomous drones where previously a single, human-operated drone would have been used. This likely includes crisis response and search and rescue missions. These systems will need a method of navigating unknown and dynamic environments. Typically, this would require an incremental heuristic search algorithm, however, these algorithms become increasingly computationally and memory intensive as the environment size increases. This paper used two different Swarm Intelligence (SI) algorithms: Particle Swarm Optimisation and Reynolds flocking to propose an overall system for controlling and navigating groups of autonomous drones through unknown and dynamic environments. This paper proposes Particle Swarm Optimisation Pathfinding (PSOP): a dynamic, cooperative algorithm; and, Drone Flock Control (DFC): a modular model for controlling systems of agents, in 3D environments, such that collisions are minimised. Using the Unity game engine, a real-time application, simulation environment, and data collection apparatus were developed and the performances of DFC-controlled drones—navigating with either the PSOP algorithm or a D* Lite implementation—were compared. The simulations do not consider UAV dynamics. The drones were tasked with navigating to a given target position in environments of varying size and quantitative data on pathfinding performance, computational and memory performance, and usability were collected. Using this data, the advantages of PSO-based pathfinding were demonstrated. PSOP was shown to be more memory efficient, more successful in the creation of high quality, accurate paths, more usable and as computationally efficient as a typical incremental heuristic search algorithm when used as part of a SI-based drone control model. This study demonstrated the capabilities of SI approaches as a means of controlling multi-agent UAV systems in a simple simulation environment. Future research may look to apply the DFC model, with the PSOP algorithm, to more advanced simulations which considered environment factors like atmospheric pressure and turbulence, or to real-world UAVs in a controlled environment.},
  comment          = {用PSO进行动态路径规划
将PSO与Reynolds flocking相结合
使用Unity开发实验环境，与D*进行对比，算法的扩展性更强},
  creationdate     = {2023-01-12T21:33:26},
  file             = {:Pyke_2021_Dynamic Pathfinding for a Swarm Intelligence Based UAV Control Model Using Particle Swarm Optimisation.pdf:PDF},
  groups           = {Swarm Intelligence},
  modificationdate = {2023-01-13T16:12:50},
  url              = {https://www.frontiersin.org/articles/10.3389/fams.2021.744955},
  urldate          = {2023-01-12},
}

@Misc{Son2019QTRAN,
  author           = {Kyunghwan Son and Daewoo Kim and Wan Ju Kang and David Earl Hostallero and Yung Yi},
  month            = may,
  title            = {QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning},
  year             = {2019},
  abstract         = {We explore value-based solutions for multi-agent reinforcement learning (MARL) tasks in the centralized training with decentralized execution (CTDE) regime popularized recently. However, VDN and QMIX are representative examples that use the idea of factorization of the joint action-value function into individual ones for decentralized execution. VDN and QMIX address only a fraction of factorizable MARL tasks due to their structural constraint in factorization such as additivity and monotonicity. In this paper, we propose a new factorization method for MARL, QTRAN, which is free from such structural constraints and takes on a new approach to transforming the original joint action-value function into an easily factorizable one, with the same optimal actions. QTRAN guarantees more general factorization than VDN or QMIX, thus covering a much wider class of MARL tasks than does previous methods. Our experiments for the tasks of multi-domain Gaussian-squeeze and modified predator-prey demonstrate QTRAN's superior performance with especially larger margins in games whose payoffs penalize non-cooperative behavior more aggressively.},
  archiveprefix    = {arXiv},
  creationdate     = {2023-01-13T11:19:06},
  eprint           = {1905.05408v1},
  file             = {:Son_2019_QTRAN_ Learning to Factorize with Transformation for Cooperative Multi Agent Reinforcement Learning.pdf:PDF},
  groups           = {MARL},
  keywords         = {cs.LG, cs.AI, cs.MA, stat.ML},
  modificationdate = {2023-01-13T11:19:55},
  primaryclass     = {cs.LG},
}

@Article{Niroui2019Deep,
  author           = {Farzad Niroui and Kaicheng Zhang and Zendai Kashino and Goldie Nejat},
  journal          = {{IEEE} Robotics and Automation Letters},
  title            = {Deep Reinforcement Learning Robot for Search and Rescue Applications: Exploration in Unknown Cluttered Environments},
  year             = {2019},
  month            = apr,
  number           = {2},
  pages            = {610--617},
  volume           = {4},
  accessdate       = {2023-01-13},
  creationdate     = {2023-01-13T18:52:10},
  doi              = {10.1109/lra.2019.2891991},
  file             = {:Niroui_2019_Deep Reinforcement Learning Robot for Search and Rescue Applications_ Exploration in Unknown Cluttered Environments.pdf:PDF},
  groups           = {Swarm Intelligence},
  modificationdate = {2023-04-04T16:08:03},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
  url              = {https://ieeexplore.ieee.org/abstract/document/8606991},
}

@Article{Li2020Deep,
  author           = {Haoran Li and Qichao Zhang and Dongbin Zhao},
  journal          = {{IEEE} Transactions on Neural Networks and Learning Systems},
  title            = {Deep Reinforcement Learning-Based Automatic Exploration for Navigation in Unknown Environment},
  year             = {2020},
  month            = jun,
  number           = {6},
  pages            = {2064--2076},
  volume           = {31},
  abstract         = {This paper investigates the automatic exploration problem under the unknown environment, which is the key point of applying the robotic system to some social tasks. The solution to this problem via stacking decision rules is impossible to cover various environments and sensor properties. Learning-based control methods are adaptive for these scenarios. However, these methods are damaged by low learning efficiency and awkward transferability from simulation to reality. In this paper, we construct a general exploration framework via decomposing the exploration process into the decision, planning, and mapping modules, which increases the modularity of the robotic system. Based on this framework, we propose a deep reinforcement learning-based decision algorithm that uses a deep neural network to learning exploration strategy from the partial map. The results show that this proposed algorithm has better learning efficiency and adaptability for unknown environments. In addition, we conduct the experiments on the physical robot, and the results suggest that the learned policy can be well transferred from simulation to the real robot.},
  comment          = {single-agent，考虑移出此分组},
  creationdate     = {2023-01-13T18:55:56},
  doi              = {10.1109/tnnls.2019.2927869},
  file             = {:Li_2020_Deep Reinforcement Learning Based Automatic Exploration for Navigation in Unknown Environment.pdf:PDF},
  groups           = {Swarm Intelligence},
  modificationdate = {2023-04-04T16:08:04},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

 
@InProceedings{Biswas2017Obstacle,
  author           = {Biswas, Sumana and Anavatti, Sreenatha G. and Garratt, Matthew A.},
  booktitle        = {Intelligent and {Evolutionary} {Systems}},
  title            = {Obstacle {Avoidance} for {Multi}-agent {Path} {Planning} {Based} on {Vectorized} {Particle} {Swarm} {Optimization}},
  year             = {2017},
  address          = {Cham},
  editor           = {Leu, George and Singh, Hemant Kumar and Elsayed, Saber},
  pages            = {61--74},
  publisher        = {Springer International Publishing},
  series           = {Proceedings in {Adaptation}, {Learning} and {Optimization}},
  abstract         = {This paper deals with an approach to path planning by obstacle avoidance for multi-agent systems. An effective framework is presented based on the Particle Swarm Optimization (PSO) method; an evolutionary computation (EC) technique that uses the dynamics of the swarm to search the solutions for the optimization problems. It describes the path replanning technique and obstacle avoidance for autonomous multi-agent systems. A simultaneous replanning concept is incorporated into the path planning to avoid both static and dynamic obstacles. This proposed algorithm reduces the computational time of the path planning. In the dynamic environment, the numerical results show that the Simultaneous Replanning Vectorized Particle Swarm Optimization (SRVPSO) algorithm is effective and also efficient for multi-agent systems.},
  comment          = {用PSO进行多Agent避障
考虑动态路径规划，有静态、动态障碍，可能需要重规划},
  creationdate     = {2023-01-13T20:26:18},
  doi              = {10.1007/978-3-319-49049-6_5},
  file             = {:Biswas_2017_Obstacle Avoidance for Multi Agent Path Planning Based on Vectorized Particle Swarm Optimization.pdf:PDF},
  groups           = {Swarm Intelligence},
  isbn             = {9783319490496},
  keywords         = {Path planning, Particle swarm optimization, Obstacle avoidance, Multi-agent systems},
  language         = {en},
  modificationdate = {2023-01-13T20:28:01},
}

@Article{Khasawneh2019Human,
  author           = {Amro Khasawneh and Hunter Rogers and Jeffery Bertrand and Kapil Chalil Madathil and Anand Gramopadhye},
  journal          = {Automation in Construction},
  title            = {Human adaptation to latency in teleoperated multi-robot human-agent search and rescue teams},
  year             = {2019},
  month            = mar,
  pages            = {265--277},
  volume           = {99},
  abstract         = {Teleoperation of unmanned vehicles in high stress environments has been a subject of research in many domains, which focus primarily on system and operator performance. Unmanned ground vehicles for rescue, also known as search and rescue robots, serve as extensions of responders in a disaster, providing real-time video and other relevant information about the situation. However, physically separating responder and robot introduces latency between the human input provided to the unmanned vehicle to execute an operation and the subsequent response provided by the system. This latency (lag or time delay) is determined by the distance and the bandwidth of the connection between the operator and the unmanned vehicle. Automating these systems may mitigate the effect of latency to an extent; however, this has its own consequences, such as leaving the responder out of the loop, which subsequently leads to detrimental effects on situational awareness. This research investigates the relationship between latency and the performance of the human operator of a teleoperated robot at different levels of system complexity and the effect of different levels of automation on this relationship. Eighty participants operated one or two unmanned teleoperated robots to complete two search and rescue tasks. The study utilized a 2 × 2 × 2 mixed-subjects experimental design with the automation level and latency level being the between-subjects factors and the system complexity (controlling one or two robots) being the within-subjects factor. The dependent variables were operator performance, perceived workload, and the subjective rating of trust with automation. A latency of 500 ms showed a significant decrease in performance in time to complete the task and a significant increase in the perceived physical workload. Both the automation level and latency level moderated the system complexity effect on the subjective rating of trust in the robotic system. The level of trust decreased over time in the one-robot condition as opposed to no change in the two-robot condition. The error rate decreased over time at different rates based on the number of robots or the latency level. Based on the results of the study, several design implications are suggested for improving performance including adding features to the automation that will allow the operator to use common strategies and providing necessary information using multiple sensory channels. Future research directions are also proposed.},
  accessdate       = {2023-01-16},
  comment          = {一区},
  creationdate     = {2023-01-16T15:28:05},
  doi              = {10.1016/j.autcon.2018.12.012},
  file             = {:Khasawneh_2019_Human Adaptation to Latency in Teleoperated Multi Robot Human Agent Search and Rescue Teams.pdf:PDF},
  groups           = {Swarm Intelligence},
  keywords         = {Search and rescue robots, Human-robot teaming, Human-robot interaction, Trust in automaton, Latency, Human performance},
  modificationdate = {2023-01-16T15:33:53},
  publisher        = {Elsevier {BV}},
  url              = {https://www.sciencedirect.com/science/article/pii/S0926580517310890},
}

 
@Article{Cardona2019Robot,
  author           = {Cardona, Gustavo A. and Calderon, Juan M.},
  journal          = {Applied Sciences},
  title            = {Robot {Swarm} {Navigation} and {Victim} {Detection} {Using} {Rendezvous} {Consensus} in {Search} and {Rescue} {Operations}},
  year             = {2019},
  issn             = {2076-3417},
  month            = jan,
  number           = {8},
  pages            = {1702},
  volume           = {9},
  abstract         = {Cooperative behaviors in multi-robot systems emerge as an excellent alternative for collaboration in search and rescue tasks to accelerate the finding survivors process and avoid risking additional lives. Although there are still several challenges to be solved, such as communication between agents, power autonomy, navigation strategies, and detection and classification of survivors, among others. The research work presented by this paper focuses on the navigation of the robot swarm and the consensus of the agents applied to the victims detection. The navigation strategy is based on the application of particle swarm theory, where the robots are the agents of the swarm. The attraction and repulsion forces that are typical in swarm particle systems are used by the multi-robot system to avoid obstacles, keep group compact and navigate to a target location. The victims are detected by each agent separately, however, once the agents agree on the existence of a possible victim, these agents separate from the general swarm by creating a sub-swarm. The sub-swarm agents use a modified rendezvous consensus algorithm to perform a formation control around the possible victims and then carry out a consensus of the information acquired by the sensors with the aim to determine the victim existence. Several experiments were conducted to test navigation, obstacle avoidance, and search for victims. Additionally, different situations were simulated with the consensus algorithm. The results show how swarm theory allows the multi-robot system navigates avoiding obstacles, finding possible victims, and settling down their possible use in search and rescue operations.},
  comment          = {多机器人系统中的合作行为成为搜索和救援任务协作的绝佳替代方案，可加快寻找幸存者的过程并避免冒着额外生命的风险。尽管仍有一些挑战需要解决，例如代理之间的通信、权力自主、导航策略以及幸存者的检测和分类等。本文提出的研究工作侧重于机器人群的导航和应用于受害者检测的代理的共识。导航策略基于粒子群理论的应用，其中机器人是群体的代理。多机器人系统使用群体粒子系统中典型的吸引力和排斥力来避开障碍物，保持群体紧凑并导航到目标位置。受害者由每个代理分别检测，但是，一旦代理同意可能的受害者的存在，这些代理就会通过创建子群与一般群分离。子群代理使用改进的会合共识算法围绕可能的受害者进行编队控制，然后对传感器获取的信息进行共识，以确定受害者的存在。进行了几项实验来测试导航、避障和搜索受害者。此外，还使用共识算法模拟了不同的情况。结果表明群体理论如何允许多机器人系统导航避开障碍物，找到可能的受害者，并确定它们在搜索和救援行动中的可能用途},
  copyright        = {http://creativecommons.org/licenses/by/3.0/},
  creationdate     = {2023-01-16T15:34:11},
  doi              = {10.3390/app9081702},
  file             = {:Cardona_2019_Robot Swarm Navigation and Victim Detection Using Rendezvous Consensus in Search and Rescue Operations.pdf:PDF},
  groups           = {Swarm Intelligence},
  keywords         = {swarm-robotics, rendezvous consensus, robot navigation, victim-detection},
  language         = {en},
  modificationdate = {2023-02-01T17:14:07},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://www.mdpi.com/2076-3417/9/8/1702},
  urldate          = {2023-01-16},
}

@InProceedings{Bertuccelli2006Search,
  author           = {Luca F. Bertuccelli and Jonathan P. How},
  booktitle        = {2006 American Control Conference},
  title            = {Search for dynamic targets with uncertain probability maps},
  year             = {2006},
  publisher        = {{IEEE}},
  abstract         = {This paper extends a recently developed statistical framework for UAV search with uncertain probability maps to the case of dynamic targets. The probabilities used to encode the information about the environment are typically assumed to be exactly known in the search theory literature, but they are often the result of prior information that is both erroneous and delayed, and will likely be poorly known to mission designers. Our previous work developed a new framework that accounted for the uncertainty in the probability maps for stationary targets, and this paper extends the approach to more realistic dynamic environments. The dynamic case considers probabilistic target motion, creating uncertain probability maps (UPMs) that take into account both poor knowledge of the probabilities and the propagation of their uncertainty through the environment. A key result of this paper is a new algorithm for implementing UPM's in real-time, and it is shown in various simulations that this algorithm leads to more cautious information updates that are less susceptible to false alarms. The paper also provides insights on the impact of the design parameters on the responsiveness of the new algorithm. Several numerical examples are presented to demonstrate the effectiveness of the new framework},
  accessdate       = {2023-01-31},
  comment          = {本文将最近开发的具有不确定概率图的无人机搜索统计框架扩展到动态目标的情况。用于编码环境信息的概率通常被认为是搜索理论文献中完全已知的，但它们通常是先前信息的结果，这些信息既错误又延迟，并且任务设计者可能知之甚少。我们之前的工作开发了一个新的框架来解释静止目标概率图中的不确定性，本文将该方法扩展到更真实的动态环境。动态案例考虑概率目标运动，创建不确定概率图 （UPM），该图既考虑了对概率的了解不足，也考虑了它们的不确定性在环境中的传播。本文的一个关键结果是一种实时实施芬欧汇川的新算法，并且在各种模拟中表明，该算法导致更谨慎的信息更新，不易发生误报。本文还对设计参数对新算法响应能力的影响提供了见解。通过几个数值算例来验证新框架的有效性},
  creationdate     = {2023-01-31T12:37:23},
  doi              = {10.1109/ACC.2006.1655444},
  file             = {:Bertuccelli_2006_Search for Dynamic Targets with Uncertain Probability Maps.pdf:PDF},
  groups           = {Swarm Intelligence},
  modificationdate = {2023-01-31T13:17:55},
  url              = {https://ieeexplore.ieee.org/abstract/document/1655444},
}

@InProceedings{Chung2006A,
  author           = {T.H. Chung and J.W. Burdick and R.M. Murray},
  booktitle        = {Proceedings 2006 {IEEE} International Conference on Robotics and Automation, 2006. {ICRA} 2006.},
  title            = {A decentralized motion coordination strategy for dynamic target tracking},
  year             = {2006},
  publisher        = {{IEEE}},
  abstract         = {This paper presents a decentralized motion planning algorithm for the distributed sensing of a noisy dynamical process by multiple cooperating mobile sensor agents. This problem is motivated by localization and tracking tasks of dynamic targets. Our gradient-descent method is based on a cost function that measures the overall quality of sensing. We also investigate the role of imperfect communication between sensor agents in this framework, and examine the trade-offs in performance between sensing and communication. Simulations illustrate the basic characteristics of the algorithms.},
  comment          = {该文提出一种分散式运动规划算法，用于通过多个协作的移动传感器代理对噪声动态过程进行分布式感知。这个问题是由动态目标的定位和跟踪任务引起的。我们的梯度下降方法基于测量整体传感质量的成本函数。我们还研究了传感器代理之间不完美通信在此框架中的作用，并研究了传感和通信之间的性能权衡。仿真说明了算法的基本特征},
  creationdate     = {2023-01-31T12:40:12},
  doi              = {10.1109/robot.2006.1642064},
  file             = {:Chung_2006_A Decentralized Motion Coordination Strategy for Dynamic Target Tracking.pdf:PDF},
  groups           = {Swarm Intelligence},
  modificationdate = {2023-01-31T13:17:48},
}

 
@Article{Xia2022Multi,
  author           = {Xia, Zhaoyue and Du, Jun and Wang, Jingjing and Jiang, Chunxiao and Ren, Yong and Li, Gang and Han, Zhu},
  journal          = {IEEE Transactions on Vehicular Technology},
  title            = {Multi-{Agent} {Reinforcement} {Learning} {Aided} {Intelligent} {UAV} {Swarm} for {Target} {Tracking}},
  year             = {2022},
  issn             = {1939-9359},
  number           = {1},
  pages            = {931--945},
  volume           = {71},
  abstract         = {Past few years have witnessed the widespread adoption of unmanned aerial vehicles (UAVs) in target tracking for regional monitor and strike. Most existing target tracking approaches rely on the target motion frames obtained by the camera equipped, or on ideally assuming a pre-set target trajectory. However, in practice, the real trajectory of the target cannot be perfectly known to the UAVs in advance, and also the target may intelligently adjust its flying strategy according to the environment. Besides, the limited flight performance, as well as information capture and processing capability, of a single UAV can hardly fulfill high tracking success rate requirements. To address aforementioned issues, this paper proposes an end-to-end cooperative multi-agent reinforcement learning (MARL) scheme, where UAVs are enabled to make intelligent flight decisions for cooperative target tracking, on the basis of the past and current states of the target. In order to reduce power consumption and prolong the lifetime of the UAV tracking system, the propulsion power consumption model and energy saving strategy are introduced. Moreover, to further increase the detection coverage, spatial information entropy is introduced in the tracking algorithm. Simulation results show that our proposed algorithm outperfoms the deep reinforcement learning baselines in terms of the mean episode rewards, while also yields high performances with respect to tracking success rates, power saving efficiency and detection coverage.},
  comment          = {过去几年见证了无人机（UAV）在区域监视和打击的目标跟踪中的广泛采用。大多数现有的目标跟踪方法依赖于配备的相机获得的目标运动帧，或者理想情况下假设预设的目标轨迹。然而，在实践中，无人机无法事先完全知道目标的真实轨迹，目标也可能根据环境智能调整其飞行策略。此外，单架无人机有限的飞行性能以及信息捕获和处理能力难以满足较高的跟踪成功率要求。针对上述问题，该文提出一种端到端的协作式多智能体强化学习（MARL）方案，使无人机能够根据目标过去和当前状态做出协同目标跟踪的智能飞行决策。为了降低功耗，延长无人机跟踪系统的使用寿命，介绍了推进功耗模型和节能策略。此外，为了进一步提高检测覆盖率，在跟踪算法中引入了空间信息熵。仿真结果表明，所提算法在平均情节奖励方面优于深度强化学习基线，同时在跟踪成功率、省电效率和检测覆盖率方面也表现出色。},
  creationdate     = {2023-01-31T13:05:18},
  doi              = {10.1109/TVT.2021.3129504},
  file             = {:Xia_2022_Multi Agent Reinforcement Learning Aided Intelligent UAV Swarm for Target Tracking.pdf:PDF},
  groups           = {Swarm Intelligence},
  keywords         = {Target tracking, Reinforcement learning, Unmanned aerial vehicles, Cameras, Real-time systems, Sensors, Radar, Unmanned aerial vehicles (UAVs), cooperative target tracking, multi-agent reinforcement learning (MARL), energy efficient},
  modificationdate = {2023-01-31T13:17:43},
}

@Article{Alanezi2022Dynamic,
  author           = {Mohammed A. Alanezi and Houssem R. E. H. Bouchekara and Tijani Abdul-Aziz Apalara and Mohammad Shoaib Shahriar and Yusuf A. Sha{\textquotesingle}aban and Muhammad Sharjeel Javaid and Mohammed Abdallah Khodja},
  journal          = {{IEEE} Access},
  title            = {Dynamic Target Search Using Multi-UAVs Based on Motion-Encoded Genetic Algorithm With Multiple Parents},
  year             = {2022},
  pages            = {77922--77939},
  volume           = {10},
  abstract         = {In this paper, a new optimization algorithm called Motion-Encoded Genetic Algorithm with Multiple Parents (MEGA-MPC) is developed to locate moving targets using multiple Unmanned Aerial Vehicles (UAVs). Bayesian theory is used to formulate the moving target tracking as an optimization problem where target detection probability defines the objective function as the probability of detecting the target. In the developed MEGA-MPC algorithm, a series of UAV motion paths encodes the search trajectory. In every iteration of the MEGA-MPC algorithm, UAV motion paths undergo evolution. The proposed approach for dynamic target search using multi-UAVs uses parallel computations to solve the optimization problem based on the MEGA-MPC algorithm where Each UAV can communicate with other UAVs if requested. The algorithm’s performance is tested with various characteristics under six distinct scenarios using a different number of UAVs and targets. The statistical analysis of the results obtained using MEGA-MPC compared with other well-known metaheuristics shows that MEGA-MPC offers better solutions to find dynamic targets since it outperforms all the compared algorithms.},
  accessdate       = {2023-01-31},
  comment          = {该文提出一种新的优化算法，即多亲运动编码遗传算法（MEGA-MPC），利用多无人机定位运动目标。贝叶斯理论用于将运动目标跟踪表述为优化问题，其中目标检测概率将目标函数定义为检测目标的概率。在开发的MEGA-MPC算法中，一系列无人机运动路径对搜索轨迹进行编码。在MEGA-MPC算法的每次迭代中，无人机运动路径都会经历演变。所提出的多无人机动态目标搜索方法使用并行计算来解决基于MEGA-MPC算法的优化问题，其中每架无人机都可以根据需要与其他无人机通信。该算法的性能在六种不同的场景中使用不同数量的无人机和目标进行了各种特征测试。与其他著名的元启发式算法相比，使用 MEGA-MPC 获得的结果的统计分析表明，MEGA-MPC 优于所有比较的算法，因此提供了更好的解决方案来查找动态目标。},
  creationdate     = {2023-01-31T13:12:47},
  doi              = {10.1109/ACCESS.2022.3190395},
  file             = {:Alanezi_2022_Dynamic Target Search Using Multi UAVs Based on Motion Encoded Genetic Algorithm with Multiple Parents.pdf:PDF},
  groups           = {Swarm Intelligence},
  modificationdate = {2023-01-31T18:44:46},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
  url              = {https://ieeexplore.ieee.org/abstract/document/9828382},
}

 
@InProceedings{Mohr2020UAV,
  author           = {Mohr, Hannah},
  booktitle        = {2020 {IEEE} {Aerospace} {Conference}},
  title            = {{UAV} {Implementation} of {Distributed} {Robust} {Target} {Location} in {Unknown} {Environments}},
  year             = {2020},
  month            = mar,
  note             = {ISSN: 1095-323X},
  pages            = {1--10},
  abstract         = {This paper presents the implementation of a target seeking application on a multi-agent unmanned aerial vehicle (UAV) testbed, in which the target to be located emits a signal that attenuates with increasing distance from its source. A heterogeneous swarm of multirotors collects local measurements of the signal emanating from the target, and each UAV shares its information with neighboring UAVs in a fully distributed manner to form an estimate of the signal's gradient, informing the direction of travel to move toward the target. The UAVs navigate around obstacles in an unknown environment, implementing a localized robust hybrid controller for obstacle avoidance when an obstacle is within the UAV's detection radius. The hybrid controller enables the UAVs to robustly avoid obstacles in the presence of potentially adversarial exogenous inputs through the use of overlapping control modes which preclude the topological obstructions that can arise in traditional obstacle avoidance schemes. The algorithms are implemented on the VT SpaceDrones platform, using mulitrotors equipped with Raspberry Pi microcontrollers. Each UAV performs its own target seeking and obstacle avoidance calculations in a distributed architecture, receiving position data from an OptiTrack motion capture system. The distributed implementation illustrates the adaptation of the control law to real world challenges, including unsynchronized clocks among different UAVs, limited computational power, and communication latency. Experimental and theoretical results are compared.},
  comment          = {本文介绍了在多智能体无人机（UAV）测试平台上实现目标搜索应用，其中目标发出信号，该信号随着距离其源的距离增加而衰减。多旋翼的异构群收集目标发出的信号的局部测量值，每架无人机以完全分布式的方式与相邻的无人机共享其信息，以形成对信号梯度的估计，通知向目标移动的行进方向。无人机在未知环境中绕过障碍物，实现本地化的稳健混合控制器，以便在障碍物在无人机的检测半径内时避障。混合控制器通过使用重叠的控制模式，使无人机能够在存在潜在对抗性外源输入的情况下稳健地避开障碍物，从而排除传统避障方案中可能出现的拓扑障碍物。这些算法在VT SpaceDrones平台上实现，使用配备Raspberry Pi微控制器的多转子。每架无人机在分布式架构中执行自己的目标搜寻和避障计算，从OptiTrack运动捕捉系统接收位置数据。分布式实现说明了控制律如何适应现实世界的挑战，包括不同无人机之间的时钟不同步、有限的计算能力和通信延迟。比较实验和理论结果。},
  creationdate     = {2023-01-31T18:44:19},
  doi              = {10.1109/AERO47225.2020.9172459},
  file             = {:Mohr_2020_UAV Implementation of Distributed Robust Target Location in Unknown Environments.pdf:PDF},
  groups           = {Swarm Intelligence},
  issn             = {1095-323X},
  keywords         = {Navigation, Shape, Microcontrollers, Estimation, Position measurement, Autonomous aerial vehicles, Real-time systems},
  modificationdate = {2023-01-31T19:09:25},
}

 
@Article{Wang2022Distributed,
  author           = {Wang, Yi’an and Li, Kun and Han, Ying and Yan, Xinxin},
  journal          = {ISA Transactions},
  title            = {Distributed multi-{UAV} cooperation for dynamic target tracking optimized by an {SAQPSO} algorithm},
  year             = {2022},
  issn             = {0019-0578},
  month            = oct,
  pages            = {230--242},
  volume           = {129},
  abstract         = {Real-time tracking of the dynamic intrusion targets consists of two crucial factors: the path forecast of the target and real-time path optimization of multi-UAV target tracking. For the first one, the uncertainty of the target trajectory is an obstacle to realizing real-time tracking. Thus a trajectory prediction method is proposed in this paper to ensure the sampling period of the target. Owing to the poor prediction accuracy of the single-step trajectory, a multi-step Unscented Kalman Filter (MUKF) is proposed to forecast its multi-step trajectory further in different regions. For the second one, there are two problems: poor optimization accuracy of the tracking trajectory and larger local optimization deviation, which will cause failure of the regional tracking. Under this circumstance, a hybrid algorithm called SAQPSO is proposed, combining the specific mechanism of two intelligence algorithms. The annealing mechanism in the Simulated Annealing (SA) algorithm is used to modify the Quantum Particle Swarm Optimization (QPSO) algorithm. Then the characteristic of quantum particles is used to update the population and enhance global searchability. Furthermore, to testify the effectiveness of the trajectory optimization algorithm and related target prediction method, a specific simulation environment is given as an example, in which the tracking trajectories of eight different algorithms are compared. Simulation results show the effectiveness of the proposed algorithm.},
  comment          = {侧重于运动目标的轨迹预测

动态入侵目标的实时跟踪包括两个关键因素：目标的路径预测和多无人机目标跟踪的实时路径优化。对于第一个，目标轨迹的不确定性是实现实时跟踪的障碍。为此，本文提出一种轨迹预测方法来保证目标的采样周期。由于单步轨迹的预测精度较差，提出了多步无迹卡尔曼滤波器（MUKF）以在不同区域进一步预测其多步轨迹。对于第二种，存在两个问题：跟踪轨迹优化精度差，局部优化偏差较大，会导致区域跟踪失败。在这种情况下，结合两种智能算法的具体机制，提出了一种称为SAQPSO的混合算法。模拟退火（SA）算法中的退火机制用于修改量子粒子群优化（QPSO）算法。然后利用量子粒子的特性更新种群，增强全局可搜索性。此外，为了验证轨迹优化算法和相关目标预测方法的有效性，给出了一个特定的仿真环境作为例子，其中比较了八种不同算法的跟踪轨迹。仿真结果表明了所提算法的有效性},
  creationdate     = {2023-01-31T18:46:06},
  doi              = {10.1016/j.isatra.2021.12.014},
  file             = {:Wang_2022_Distributed Multi UAV Cooperation for Dynamic Target Tracking Optimized by an SAQPSO Algorithm.pdf:PDF},
  groups           = {Swarm Intelligence},
  keywords         = {Multi-UAV, Dynamic target tracking, Trajectory prediction, Quantum Particle Swarm Optimization algorithm, Simulated Annealing algorithm},
  language         = {en},
  modificationdate = {2023-02-01T14:43:58},
  url              = {https://www.sciencedirect.com/science/article/pii/S0019057821006315},
  urldate          = {2023-01-31},
}

 
@InProceedings{Xia2021Multi,
  author           = {Xia, Zhaoyue and Du, Jun and Jiang, Chunxiao and Wang, Jingjing and Ren, Yong and Li, Gang},
  booktitle        = {{ICC} 2021 - {IEEE} {International} {Conference} on {Communications}},
  title            = {Multi-{UAV} {Cooperative} {Target} {Tracking} {Based} on {Swarm} {Intelligence}},
  year             = {2021},
  month            = jun,
  note             = {ISSN: 1938-1883},
  pages            = {1--6},
  abstract         = {In recent years, unmanned aerial vehicles (UAV) have been widely adopted to support complex target tracking tasks for military and civilian applications, especially in open and unknown environments. In practical cases, the moving trajectory of the target cannot be known to the UAVs in advance, which brings great challenges to UAVs to realize real-time and effective tracking. In addition, the limited tracking ability of a single UAV can hardly meet the requirements of a high tracking success rate. To deal with these problems above, this paper establishes a multi-UAV cooperative target tracking system. Besides, a deep reinforcement learning (DRL) based algorithm is designed to enable UAVs to make flight action decisions intelligently to track the moving air target, according to the past and current position information of the target only. To further increase the detection coverage of the UAV network when tracking, spatial information entropy is introduced to the reward designing in this algorithm. Simulation results validate that the proposed algorithm yields impressive target tracking performances, and significantly outperforms several common DRL baselines in terms of the tracking success rate. The convergence of the algorithm is also verified by the simulations.},
  comment          = {近年来，无人机（UAV）已被广泛用于支持军事和民用应用的复杂目标跟踪任务，特别是在开放和未知的环境中。在实际情况下，无人机无法提前知道目标的运动轨迹，这给无人机实现实时有效跟踪带来了极大的挑战。此外，单架无人机有限的跟踪能力难以满足高跟踪成功率的要求。针对上述问题，该文建立了多无人机协同目标跟踪系统。此外，基于深度强化学习（DRL）的算法使无人机能够仅根据目标过去和当前的位置信息，智能地做出飞行动作决策，以跟踪移动的空中目标。为了进一步提高无人机网络在跟踪时的检测覆盖率，该算法在奖励设计中引入了空间信息熵。仿真结果验证了所提算法的目标跟踪性能令人印象深刻，在跟踪成功率方面明显优于几种常见的DRL基线。仿真也验证了算法的收敛性。},
  creationdate     = {2023-01-31T19:04:47},
  doi              = {10.1109/ICC42927.2021.9500771},
  file             = {:Xia_2021_Multi UAV Cooperative Target Tracking Based on Swarm Intelligence.pdf:PDF},
  groups           = {Swarm Intelligence},
  issn             = {1938-1883},
  keywords         = {Target tracking, Simulation, Reinforcement learning, Unmanned aerial vehicles, Real-time systems, Trajectory, Information entropy},
  modificationdate = {2023-02-01T14:34:47},
}

@Article{Hooshangi2021Urban,
  author           = {Navid Hooshangi and Ali Asghar Alesheikh and Mahdi Panahi and Saro Lee},
  journal          = {Natural Hazards and Earth System Sciences},
  title            = {Urban search and rescue ({USAR}) simulation system: spatial strategies for agent task allocation under uncertain conditions},
  year             = {2021},
  month            = nov,
  number           = {11},
  pages            = {3449--3463},
  volume           = {21},
  abstract         = {Task allocation under uncertain conditions is a key problem for agents attempting to achieve harmony in disaster environments. This paper presents an agent-based simulation to investigate task allocation considering appropriate spatial strategies to manage uncertainty in urban search and rescue (USAR) operations. The proposed method is based on the contract net protocol (CNP) and implemented over five phases: ordering existing tasks considering intrinsic interval uncertainty, finding a coordinating agent, holding an auction, applying allocation strategies (four strategies), and implementing and observing the real environment. Applying allocation strategies is the main innovation of the method. The methodology was evaluated in Tehran's District 1 for 6.6, 6.9, and 7.2 magnitude earthquakes. The simulation began by calculating the numbers of injured individuals, which were 28 856, 73 195, and 111 463 people for each earthquake, respectively. Simulations were performed for each scenario for a variety of rescuers (1000, 1500, and 2000 rescuers). In comparison with the CNP, the standard duration of rescue operations with the proposed approach exhibited at least 13 % improvement, with a maximal improvement of 21 %. Interval uncertainty analysis and comparison of the proposed strategies showed that increased uncertainty led to increased rescue time for the CNP and strategies 1 to 4. The time increase was less with the uniform distribution strategy (strategy 4) than with the other strategies. The consideration of strategies in the task allocation process, especially spatial strategies, facilitated both optimization and increased flexibility of the allocation. It also improved conditions for fault tolerance and agent-based cooperation stability in the USAR simulation system.},
  comment          = {不确定条件下的任务分配是关键 尝试在灾难环境中实现和谐的代理的问题。 本文提出了一种基于代理的模拟来研究任务分配 考虑适当的空间策略来管理城市的不确定性 搜索和救援 （USAR） 行动。所提出的方法基于 合同网协议（CNP）并分五个阶段实施：订购 考虑内在区间不确定性的现有任务，找到 协调代理，举行拍卖，应用分配策略（四个 策略），并实施和观察真实环境。应用 分配策略是该方法的主要创新。方法论 在德黑兰第 1 区进行了 6.6、6.9 和 7.2 星等评估 地震。模拟从计算受伤人数开始 个人，分别为28 856人、73 195人和111 463人 分别地震。针对每个场景进行了模拟 各种救援人员（1000、1500 和 2000 名救援人员）。与 CNP，采用拟议方法的救援行动的标准持续时间 表现出至少13%的改善，最大改善为21%。 区间不确定性分析与策略比较 表明不确定性增加导致CNP的救援时间增加 以及战略1至4。制服的时间增加较少 分配策略（策略4）与其他策略相比。这 在任务分配过程中考虑策略，特别是 空间策略，促进优化和提高灵活性 的分配。它还改善了容错条件和 USAR仿真系统中基于代理的协作稳定性。},
  creationdate     = {2023-01-31T19:11:18},
  doi              = {10.5194/nhess-21-3449-2021},
  modificationdate = {2023-04-04T16:08:06},
  publisher        = {Copernicus {GmbH}},
}

 
@Article{Huang2021Energy,
  author           = {Huang, Hailong and Savkin, Andrey V.},
  journal          = {Ad Hoc Networks},
  title            = {Energy-efficient decentralized navigation of a team of solar-powered {UAVs} for collaborative eavesdropping on a mobile ground target in urban environments},
  year             = {2021},
  issn             = {1570-8705},
  month            = jun,
  pages            = {1--13},
  volume           = {117},
  abstract         = {This paper considers using a team of solar-powered UAVs to collaboratively eavesdrop on a mobile ground target in an urban environment. A practical application is that the police department uses UAVs to collect the transmitted data from a criminal suspect so that the transmitted content can be understood. Using UAVs in this task is a much more cost-effective and also less suspicious option than the conventional surveillance by a police helicopter. We focus on the online and decentralized path planning for the solar-powered UAV team. We formulate a multi-objective optimization problem which forces the UAVs to complete the uninterrupted eavesdropping task, not enter any No-fly space (NFS), keep a safe distance from each other and harvest as much energy as possible. We propose a Rapidly-exploring Random Tree (RRT) based path planning method, consisting of an initial planning phase and an online planning phase. In the online planning phase, each UAV plans its own path based on the shared information across the team. Since the UAV model can be taken into account in the planning phase, the obtained UAV paths are feasible without any later adjustment. More importantly, this method is easily implementable in real-time.},
  comment          = {本文考虑使用一组太阳能无人机协同窃听城市环境中的移动地面目标。一个实际的应用是公安部门使用无人机采集犯罪嫌疑人传输的数据，以便了解传输的内容。与警用直升机的常规监视相比，在此任务中使用无人机是一种更具成本效益且可疑性更低的选择。我们专注于太阳能无人机团队的在线和分散路径规划。我们制定了一个多目标优化问题，迫使无人机完成不间断的窃听任务，不进入任何禁飞空间（NFS），彼此保持安全距离并尽可能多地收集能量。我们提出了一种基于快速探索随机树 (RRT) 的路径规划方法，包括初始规划阶段和在线规划阶段。在在线规划阶段，每架无人机根据团队共享的信息规划自己的路径。由于在规划阶段可以考虑到无人机模型，所以得到的无人机路径是可行的，不需要后期任何调整。更重要的是，这种方法很容易实时实现},
  creationdate     = {2023-01-31T19:38:11},
  doi              = {10.1016/j.adhoc.2021.102485},
  file             = {:Huang_2021_Energy Efficient Decentralized Navigation of a Team of Solar Powered UAVs for Collaborative Eavesdropping on a Mobile Ground Target in Urban Environments.pdf:PDF},
  groups           = {Swarm Intelligence},
  keywords         = {Unmanned aerial vehicles (UAVs), Solar-powered UAVs, UAV networks, Energy-efficiency, Radio surveillance, Collaborative eavesdropping, UAV path planning, Decentralized navigation},
  language         = {en},
  modificationdate = {2023-02-01T16:34:11},
  url              = {https://www.sciencedirect.com/science/article/pii/S1570870521000512},
  urldate          = {2023-01-31},
}

@Article{Shaferman2015Tracking,
  author           = {Vitaly Shaferman and Tal Shima},
  journal          = {Journal of Dynamic Systems, Measurement, and Control},
  title            = {Tracking Multiple Ground Targets in Urban Environments Using Cooperating Unmanned Aerial Vehicles},
  year             = {2015},
  month            = may,
  number           = {5},
  volume           = {137},
  abstract         = {A distributed approach is proposed for planning a cooperative tracking task for a team of heterogeneous unmanned aerial vehicles (UAVs) tracking multiple predictable ground targets in a known urban environment. The solution methodology involves finding visibility regions, from which the UAV can maintain line-of-sight to each target during the scenario, and restricted regions, in which a UAV cannot fly, due to the presence of buildings or other airspace limitations. These regions are then used to pose a combined task assignment and motion planning optimization problem, in which each UAV’s cost function is associated with its location relative to the visibility and restricted regions, and the tracking performance of the other UAVs in the team. A distributed co-evolution genetic algorithm (CEGA) is derived for solving the optimization problem. The proposed solution is scalable, robust, and computationally parsimonious. The algorithm is centralized, implementing a distributed computation approach; thus, global information is used and the computational workload is divided between the team members. This enables the execution of the algorithm in relatively large teams of UAVs servicing a large number of targets. The viability of the algorithm is demonstrated in a Monte Carlo study, using a high fidelity simulation test-bed incorporating a visual database of an actual city.},
  comment          = {提出了一种分布式方法，用于为在已知城市环境中跟踪多个可预测地面目标的异构无人机 (UAV) 团队规划协作跟踪任务。该解决方案方法涉及寻找可见性区域，无人机可以在场景中保持对每个目标的视线，以及无人机由于存在建筑物或其他空域限制而无法飞行的限制区域。然后使用这些区域提出组合任务分配和运动规划优化问题，其中每个无人机的成本函数与其相对于可见性和限制区域的位置以及团队中其他无人机的跟踪性能相关联。推导了分布式协同进化遗传算法（CEGA）来解决优化问题。所提出的解决方案是可扩展的、健壮的并且在计算上是简约的。该算法是集中式的，实现了分布式计算方法；因此，使用全局信息并在团队成员之间分配计算工作量。这使得算法能够在服务于大量目标的相对较大的 UAV 团队中执行。该算法的可行性在蒙特卡洛研究中得到证明，该研究使用结合了实际城市可视化数据库的高保真模拟试验台},
  creationdate     = {2023-01-31T19:42:01},
  doi              = {10.1115/1.4028594},
  file             = {:Shaferman_2015_Tracking Multiple Ground Targets in Urban Environments Using Cooperating Unmanned Aerial Vehicles.pdf:PDF},
  groups           = {Swarm Intelligence},
  modificationdate = {2023-04-04T16:08:09},
  publisher        = {{ASME} International},
}

@Article{Ramchurn2015Human,
  author           = {Sarvapali D. Ramchurn and Feng Wu and Wenchao Jiang and Joel E. Fischer and Steve Reece and Stephen Roberts and Tom Rodden and Chris Greenhalgh and Nicholas R. Jennings},
  journal          = {Autonomous Agents and Multi-Agent Systems},
  title            = {Human{\textendash}agent collaboration for disaster response},
  year             = {2015},
  month            = feb,
  number           = {1},
  pages            = {82--111},
  volume           = {30},
  creationdate     = {2023-01-31T20:03:52},
  doi              = {10.1007/s10458-015-9286-4},
  file             = {:Ramchurn_2015_Human_agent Collaboration for Disaster Response.pdf:PDF;:Ramchurn_2015_Human_agent Collaboration for Disaster Response-slides.pdf:PDF},
  groups           = {Human-Agent-Robot},
  modificationdate = {2023-04-04T16:08:39},
  publisher        = {Springer Science and Business Media {LLC}},
}

@PhdThesis{Yourdshahi2021Online,
  author           = {Yourdshahi, Elnaz Shafipour},
  school           = {Lancaster University},
  title            = {On-Line Planning and Learning in Type-Based Ad-Hoc Teamwork},
  year             = {2021},
  type             = {phdthesis},
  creationdate     = {2023-02-03T15:50:34},
  file             = {:Yourdshahi_2021_On Line Planning and Learning in Type Based Ad Hoc Teamwork.pdf:PDF},
  groups           = {Ad Hoc Teamwork},
  keywords         = {Robots;Back propagation;Algorithms;Communication;Parameter estimation;Computer science;Mining;Morphology;Physics;Robotics;Teamwork},
  modificationdate = {2023-02-03T17:04:37},
  university       = {Lancaster University (United Kingdom)},
  url              = {https://www-pqdtcn-com-s.libyc.nudt.edu.cn:443/thesisDetails/E79A3D59B948AB2AF0B9FE4DBA8F271E},
}

 
@Article{Hou2021Distributed,
  author           = {Hou, Kun and Yang, Yajun and Yang, Xuerong and Lai, Jiazhe},
  journal          = {IEEE Access},
  title            = {Distributed {Cooperative} {Search} {Algorithm} {With} {Task} {Assignment} and {Receding} {Horizon} {Predictive} {Control} for {Multiple} {Unmanned} {Aerial} {Vehicles}},
  year             = {2021},
  issn             = {2169-3536},
  pages            = {6122--6136},
  volume           = {9},
  abstract         = {For target search using multiple unmanned aerial vehicles (UAVs) while knowing the probability distribution of the targets, a distributed cooperative search algorithm aiming to minimize the search time is proposed. First, an importance function for the representation of the environment is designed. Second, a mission planning system (MPS) is proposed, consisting of preliminary planning, task assignment, and post-planning layers. In the MPS, the search region is divided into a series of sub-regions of different sizes by centroidal Voronoi tessellation; these are regarded as subtasks assigned to the UAVs. The loading of the MPS improves the performance of global planning of the UAVs. Finally, receding horizon predictive control is used to plan the paths of the UAVs online. Moreover, the conflict between the requirements of target search and connectivity maintenance of the UAVs is mitigated using the minimum spanning tree strategy to optimize the communication topology while considering the communication cost when evaluating the tasks. The results of Monte Carlo simulations show that the introduction of the MPS into the traditional cooperative search framework effectively improves search and coverage efficiency.},
  creationdate     = {2023-02-07T16:32:12},
  doi              = {10.1109/ACCESS.2020.3048974},
  file             = {:Hou_2021_Distributed Cooperative Search Algorithm with Task Assignment and Receding Horizon Predictive Control for Multiple Unmanned Aerial Vehicles.pdf:PDF},
  groups           = {Swarm Intelligence},
  keywords         = {Task analysis, Search problems, Planning, Probability distribution, Path planning, Maintenance engineering, Uncertainty, Multi-UAV search, mission planning system, cooperative control, prior probability distribution},
  modificationdate = {2023-02-07T16:33:04},
}

 
@Article{Tang2020A,
  author           = {Tang, Hongwei and Sun, Wei and Yu, Hongshan and Lin, Anping and Xue, Min},
  journal          = {Expert Systems with Applications},
  title            = {A multirobot target searching method based on bat algorithm in unknown environments},
  year             = {2020},
  issn             = {0957-4174},
  month            = mar,
  pages            = {112945},
  volume           = {141},
  abstract         = {Multirobot target searching in unknown environments is a currently trending topic of discussion. In this paper, an improved bat algorithm (BA) for multirobot target searching in unknown environments, named adaptive robotic bat algorithm (ARBA), is proposed; it acts as the controlling mechanism for robots. The obstacle avoidance problem is considered in the proposed ARBA. The adaptive inertial weight strategy helps ARBA improve its diversity and provides an effective mechanism for escaping from local optima. In addition, the Doppler effect is introduced to improve ARBA; the effect can be adaptively compensated when the robot moves and helps robots avoid premature convergence. Moreover, the location of the target in an unknown environment is unknown, and a multi-swarm strategy is introduced into the ARBA to improve the diversity and expand the search space of robots so that robots can find the location of the target as well as the target itself faster than the existing algorithms. Experiments were conducted in three aspects to verify the effectiveness and efficiency of ARBA. We compared ARBA with the other algorithms in this field; the experimental results demonstrate that ARBA exhibits better performance in multirobot target searching and can be applied to multirobot intelligent systems.},
  creationdate     = {2023-02-16T10:34:31},
  doi              = {10.1016/j.eswa.2019.112945},
  file             = {:Tang_2020_A Multirobot Target Searching Method Based on Bat Algorithm in Unknown Environments.pdf:PDF},
  groups           = {Swarm Intelligence},
  keywords         = {Bat algorithm, Adaptive robotic BA, Doppler effect, Multirobot, Target searching},
  language         = {en},
  modificationdate = {2023-02-16T11:29:00},
  url              = {https://www.sciencedirect.com/science/article/pii/S0957417419306633},
  urldate          = {2023-02-16},
}

 
@InProceedings{Bashyal2008Human,
  author           = {Bashyal, Shishir and Venayagamoorthy, Ganesh Kumar},
  booktitle        = {2008 {IEEE} {Swarm} {Intelligence} {Symposium}},
  title            = {Human swarm interaction for radiation source search and localization},
  year             = {2008},
  month            = sep,
  pages            = {1--8},
  abstract         = {This study shows that appropriate human interaction can benefit a swarm of robots to achieve goals more efficiently. A set of desirable features for human swarm interaction is identified based on the principles of swarm robotics. Human swarm interaction architecture is then proposed that has all of the desirable features. A swarm simulation environment is created that allows simulating a swarm behavior in an indoor environment. The swarm behavior and the results of user interaction are studied by considering radiation source search and localization application of the swarm. Particle swarm optimization algorithm is slightly modified to enable the swarm to autonomously explore the indoor environment for radiation source search and localization. The emergence of intelligence is observed that enables the swarm to locate the radiation source completely on its own. Proposed human swarm interaction is then integrated in a simulation environment and user evaluation experiments are conducted. Participants are introduced to the interaction tool and asked to deploy the swarm to complete the missions. The performance comparison of the user guided swarm to that of the autonomous swarm shows that the interaction interface is fairly easy to learn and that user guided swarm is more efficient in achieving the goals. The results clearly indicate that the proposed interaction helped the swarm achieve emergence.},
  comment          = {这项研究表明，适当的人机交互可以使一群机器人更有效地实现目标。
通过考虑群体的辐射源搜索和定位应用来研究群体行为和用户交互的结果。
粒子群优化算法略有修改，使粒子群能够自主探索室内环境，进行辐射源搜索和定位。
与自主群的性能比较表明，用户引导群在实现目标方面效率更高。

使用PSO
第三章给出了HSI需要有的几个特征},
  creationdate     = {2023-02-16T22:44:14},
  doi              = {10.1109/SIS.2008.4668287},
  file             = {:Bashyal_2008_Human Swarm Interaction for Radiation Source Search and Localization.pdf:PDF},
  groups           = {Human-Swarm},
  keywords         = {Human robot interaction, Particle swarm optimization, Indoor environments, Insects, USA Councils, Intelligent robots, Laboratories, Collaboration, Chirp, Educational robots},
  modificationdate = {2023-03-08T14:53:43},
}

 
@Article{Dahiya2023Survey,
  author           = {Dahiya, Abhinav and Aroyo, Alexander M. and Dautenhahn, Kerstin and Smith, Stephen L.},
  journal          = {Robotics and Autonomous Systems},
  title            = {A survey of multi-agent {Human}–{Robot} {Interaction} systems},
  year             = {2023},
  issn             = {0921-8890},
  month            = mar,
  pages            = {104335},
  volume           = {161},
  abstract         = {This article presents a survey of literature in the area of Human–Robot Interaction (HRI), specifically on systems containing more than two agents (i.e., having multiple humans and/or multiple robots). We identify three core aspects of “Multi-agent” HRI systems that are useful for understanding how these systems differ from dyadic systems and from one another. These are the Team structure, Interaction style among agents, and the system’s Computational characteristics. Under these core aspects, we present five attributes of HRI systems, namely Team size, Team composition, Interaction model, Communication modalities, and Robot control. These attributes are used to characterize and distinguish one system from another. We populate resulting categories with examples from the recent literature along with a brief discussion of their applications. We also analyze how these attributes in multi-agent systems differ from the case of dyadic human–robot systems. Through this survey, we summarize key observations from the current literature, and identify challenges and promising areas for future research in this domain. In order to realize the vision of robots being part of the society and interacting seamlessly with humans, there is a need to expand research on multi-human–multi-robot systems. Not only do these systems require coordination among several agents, they also involve multi-agent and indirect interactions which are absent from dyadic HRI systems. Including multiple agents in HRI systems requires more advanced interaction schemes, behavior understanding and control methods to allow natural interactions among humans and robots. In addition, research on human behavioral understanding in mixed human–robot teams also requires more attention. This will help formulate and implement effective robot control policies in HRI systems with large numbers of heterogeneous robots and humans; a team composition reflecting many real-world scenarios.},
  creationdate     = {2023-02-16T22:44:45},
  doi              = {10.1016/j.robot.2022.104335},
  file             = {:Dahiya_2023_A Survey of Multi Agent Human–Robot Interaction Systems.pdf:PDF},
  groups           = {Human-Swarm},
  keywords         = {Human–Robot Interaction (HRI), Multi-agent system, Robots in groups, Human–robot teams},
  language         = {en},
  modificationdate = {2023-03-08T14:53:40},
  url              = {https://www.sciencedirect.com/science/article/pii/S092188902200224X},
  urldate          = {2023-02-16},
}

 
@Article{PerezCarabaza2018Ant,
  author           = {Perez-Carabaza, Sara and Besada-Portas, Eva and Lopez-Orozco, Jose A. and de la Cruz, Jesus M.},
  journal          = {Applied Soft Computing},
  title            = {Ant colony optimization for multi-{UAV} minimum time search in uncertain domains},
  year             = {2018},
  issn             = {1568-4946},
  month            = jan,
  pages            = {789--806},
  volume           = {62},
  abstract         = {This paper presents a new approach based on ant colony optimization (ACO) to determine the trajectories of a fleet of unmanned air vehicles (UAVs) looking for a lost target in the minimum possible time. ACO is especially suitable for the complexity and probabilistic nature of the minimum time search (MTS) problem, where a balance between the computational requirements and the quality of solutions is needed. The presented approach includes a new MTS heuristic that exploits the probability and spatial properties of the problem, allowing our ant based algorithm to quickly obtain high-quality high-level straight-segmented UAV trajectories. The potential of the algorithm is tested for different ACO parameterizations, over several search scenarios with different characteristics such as number of UAVs, or target dynamics and location distributions. The statistical comparison against other techniques previously used for MTS (ad hoc heuristics, cross entropy optimization, bayesian optimization algorithm and genetic algorithms) shows that the new approach outperforms the others.},
  creationdate     = {2023-02-19T00:57:31},
  doi              = {10.1016/j.asoc.2017.09.009},
  file             = {:Perez-Carabaza_2018_Ant Colony Optimization for Multi UAV Minimum Time Search in Uncertain Domains.pdf:PDF},
  groups           = {Swarm Intelligence},
  keywords         = {Ant colony optimization, Probabilistic path planning, UAVs, Minimum time search},
  language         = {en},
  modificationdate = {2023-03-08T14:53:35},
  url              = {https://www.sciencedirect.com/science/article/pii/S1568494617305483},
  urldate          = {2023-02-19},
}

 
@Article{Morin2023Ant,
  author           = {Morin, Michael and Abi-Zeid, Irène and Quimper, Claude-Guy},
  journal          = {European Journal of Operational Research},
  title            = {Ant colony optimization for path planning in search and rescue operations},
  year             = {2023},
  issn             = {0377-2217},
  month            = feb,
  number           = {1},
  pages            = {53--63},
  volume           = {305},
  abstract         = {In search and rescue operations, an efficient search path, colloquially understood as a path maximizing the probability of finding survivors, is more than a path planning problem. Maximizing the objective adequately, i.e., quickly enough and with sufficient realism, can have substantial positive impact in terms of human lives saved. In this paper, we address the problem of efficiently optimizing search paths in the context of the NP-hard optimal search path problem with visibility, based on search theory. To that end, we evaluate and develop ant colony optimization algorithm variants where the goal is to maximize the probability of finding a moving search object with Markovian motion, given a finite time horizon and finite resources (scans) to allocate to visible regions. Our empirical results, based on evaluating 96 variants of the metaheuristic with standard components tailored to the problem and using realistic size search environments, provide valuable insights regarding the best algorithm configurations. Furthermore, our best variants compare favorably, especially on the larger and more realistic instances, with a standard greedy heuristic and a state-of-the-art mixed-integer linear program solver. With this research, we add to the empirical body of evidence on an ant colony optimization algorithms configuration and applications, and pave the way to the implementation of search path optimization in operational decision support systems for search and rescue.},
  creationdate     = {2023-02-19T00:57:51},
  doi              = {10.1016/j.ejor.2022.06.019},
  file             = {:Morin_2023_Ant Colony Optimization for Path Planning in Search and Rescue Operations.pdf:PDF},
  groups           = {Swarm Intelligence},
  keywords         = {Evolutionary computations, Search and rescue, Optimal search path planning, Ant colony optimization, Humanitarian operations},
  language         = {en},
  modificationdate = {2023-03-08T14:53:31},
  url              = {https://www.sciencedirect.com/science/article/pii/S0377221722004945},
  urldate          = {2023-02-19},
}

@InCollection{Holzinger2016Towards,
  author           = {Andreas Holzinger and Markus Plass and Katharina Holzinger and Gloria Cerasela Cri{\c{s}}an and Camelia-M. Pintea and Vasile Palade},
  booktitle        = {Lecture Notes in Computer Science},
  publisher        = {Springer International Publishing},
  title            = {Towards interactive Machine Learning ({iML}): Applying Ant Colony Algorithms to Solve the Traveling Salesman Problem with the Human-in-the-Loop Approach},
  year             = {2016},
  pages            = {81--95},
  abstract         = {Home  Availability, Reliability, and Security in Information Systems  Conference paper
Towards interactive Machine Learning (iML): Applying Ant Colony Algorithms to Solve the Traveling Salesman Problem with the Human-in-the-Loop Approach
Download book PDF
Download book EPUB
Towards interactive Machine Learning (iML): Applying Ant Colony Algorithms to Solve the Traveling Salesman Problem with the Human-in-the-Loop Approach
Andreas Holzinger, Markus Plass, Katharina Holzinger, Gloria Cerasela Crişan, Camelia-M. Pintea & Vasile Palade 
Conference paper
First Online: 23 August 2016
3672 Accesses

50 Citations

1 Altmetric

Part of the Lecture Notes in Computer Science book series (LNISA,volume 9817)

Abstract
Most Machine Learning (ML) researchers focus on automatic Machine Learning (aML) where great advances have been made, for example, in speech recognition, recommender systems, or autonomous vehicles. Automatic approaches greatly benefit from the availability of “big data”. However, sometimes, for example in health informatics, we are confronted not a small number of data sets or rare events, and with complex problems where aML-approaches fail or deliver unsatisfactory results. Here, interactive Machine Learning (iML) may be of help and the “human-in-the-loop” approach may be beneficial in solving computationally hard problems, where human expertise can help to reduce an exponential search space through heuristics.

In this paper, experiments are discussed which help to evaluate the effectiveness of the iML-“human-in-the-loop” approach, particularly in opening the “black box”, thereby enabling a human to directly and indirectly manipulating and interacting with an algorithm. For this purpose, we selected the Ant Colony Optimization (ACO) framework, and use it on the Traveling Salesman Problem (TSP) which is of high importance in solving many practical problems in health informatics, e.g. in the study of proteins.},
  comment          = {Interactive machine learning: experimental evidence for the human in the algorithmic loop: A case study on Ant Colony Optimization 的参考文献

用蚁群算法求解TSP问题，算法运行过程中，人可以将算法暂停，然后选择一条边，并对其信息素进行滑块调整，然后恢复算法运行

通过这种方式加入人的干预，提升蚁群算法的性能},
  creationdate     = {2023-02-26T13:00:43},
  doi              = {10.1007/978-3-319-45507-5_6},
  file             = {:Holzinger_2016_Towards Interactive Machine Learning (iML)_ Applying Ant Colony Algorithms to Solve the Traveling Salesman Problem with the Human in the Loop Approach.pdf:PDF},
  groups           = {Human-Swarm},
  modificationdate = {2023-02-28T22:11:58},
}

 
@Article{Brooke1996SUS,
  author           = {Brooke, John},
  journal          = {Usability evaluation in industry},
  title            = {{SUS}: {A} quick and dirty usability scale},
  year             = {1996},
  month            = nov,
  number           = {194},
  pages            = {4--7},
  volume           = {189},
  abstract         = {Usability does not exist in any absolute sense; it can only be defined with reference to particular contexts. This, in turn, means that there are no absolute measures of usability, since, if the usability of an artefact is defined by the context in which that artefact is used, measures of usability must of necessity be defined by that context too. Despite this, there is a need for broad general measures which can be used to compare usability across a range of contexts. In addition, there is a need for "quick and dirty" methods to allow low cost assessments of usability in industrial systems evaluation. This chapter describes the System Usability Scale (SUS) a reliable, low-cost usability scale that can be used for global assessments of systems usability.},
  comment          = {System Usability Scale},
  creationdate     = {2023-03-01T23:23:50},
  file             = {:Brooke_1996_SUS_ a Quick and Dirty Usability Scale.pdf:PDF},
  groups           = {Human-Swarm},
  modificationdate = {2023-03-01T23:58:54},
  shorttitle       = {{SUS}},
}

 
@Misc{Hart1986NASATLX,
  author           = {Hart, Sandra G.},
  month            = jan,
  note             = {NTRS Author Affiliations: NASA Ames Research Center NTRS Document ID: 20000021487 NTRS Research Center: Ames Research Center (ARC)},
  title            = {{NASA} {Task} {Load} {Index} ({TLX})},
  year             = {1986},
  abstract         = {This booklet and the accompanying diskette contain the materials necessary to collect subjective workload assessments with the NASA Task Load Index on IBM PC compatible microcomputers. This procedure for collecting workload ratings was developed by the Human Performance Group at NASA Ames Research Center during a three year research effort that involved more than 40 laboratory, simulation, and inflight experiments Although the technique is still undergoing evaluation, this package is being distributed to allow other researchers to use it in their own experiments Comments or suggestions about the procedure would be greatly appreciated This package is intended to fill a "nuts and bolts" function of describing the procedure. A bibliography provides background information about previous empirical findings and the logic that supports the procedure.},
  creationdate     = {2023-03-02T00:11:05},
  file             = {:Hart_1986_NASA Task Load Index (TLX).pdf:PDF},
  groups           = {Human-Swarm},
  keywords         = {Man/System Technology And Life Support},
  modificationdate = {2023-03-02T00:11:42},
  url              = {https://ntrs.nasa.gov/citations/20000021487},
  urldate          = {2023-03-02},
}

@Article{Murphy2004Human,
  author           = {R.R. Murphy},
  journal          = {{IEEE} Transactions on Systems, Man and Cybernetics, Part C (Applications and Reviews)},
  title            = {Human{\textendash}Robot Interaction in Rescue Robotics},
  year             = {2004},
  month            = may,
  number           = {2},
  pages            = {138--153},
  volume           = {34},
  abstract         = {Rescue robotics has been suggested by a recent DARPA/NSF study as an application domain for the research in human-robot integration (HRI). This paper provides a short tutorial on how robots are currently used in urban search and rescue (USAR) and discusses the HRI issues encountered over the past eight years. A domain theory of the search activity is formulated. The domain theory consists of two parts: 1) a workflow model identifying the major tasks, actions, and roles in robot-assisted search (e.g., a workflow model) and 2) a general information flow model of how data from the robot is fused by various team members into information and knowledge. The information flow model also captures the types of situation awareness needed by each agent in the rescue robot system. The article presents a synopsis of the major HRI issues in reducing the number of humans it takes to control a robot, maintaining performance with geographically distributed teams with intermittent communications, and encouraging acceptance within the existing social structure.},
  creationdate     = {2023-03-08T14:51:34},
  doi              = {10.1109/tsmcc.2004.826267},
  file             = {:Murphy_2004_Human_Robot Interaction in Rescue Robotics.pdf:PDF},
  groups           = {Human-Swarm},
  modificationdate = {2023-04-04T16:08:40},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Valente2013Aerial,
  author           = {Jo{\~{a}}o Valente and Jaime Del Cerro and Antonio Barrientos and David Sanz},
  journal          = {Computers and Electronics in Agriculture},
  title            = {Aerial coverage optimization in precision agriculture management: A musical harmony inspired approach},
  year             = {2013},
  month            = nov,
  pages            = {153--159},
  volume           = {99},
  comment          = {面向农业的空域覆盖优化},
  creationdate     = {2023-03-22T21:11:46},
  doi              = {10.1016/j.compag.2013.09.008},
  groups           = {temp_for_hsi},
  modificationdate = {2023-04-04T16:08:43},
  publisher        = {Elsevier {BV}},
}

@Article{Barrientos2011Aerial,
  author           = {Antonio Barrientos and Julian Colorado and Jaime del Cerro and Alexander Martinez and Claudio Rossi and David Sanz and Jo{\~{a}}o Valente},
  journal          = {Journal of Field Robotics},
  title            = {Aerial remote sensing in agriculture: A practical approach to area coverage and path planning for fleets of mini aerial robots},
  year             = {2011},
  month            = aug,
  number           = {5},
  pages            = {667--689},
  volume           = {28},
  comment          = {面向农业的区域覆盖},
  creationdate     = {2023-03-22T21:13:16},
  doi              = {10.1002/rob.20403},
  groups           = {temp_for_hsi},
  modificationdate = {2023-04-04T16:08:44},
  publisher        = {Wiley},
}

@Article{Acevedo2012Cooperative,
  author           = {Jose Joaquin Acevedo and Bego{\~{n}}a C. Arrue and Ivan Maza and Anibal Ollero},
  journal          = {Journal of Intelligent \& Robotic Systems},
  title            = {Cooperative Large Area Surveillance with a Team of Aerial Mobile Robots for Long Endurance Missions},
  year             = {2012},
  month            = jul,
  number           = {1-4},
  pages            = {329--345},
  volume           = {70},
  creationdate     = {2023-03-22T21:14:47},
  doi              = {10.1007/s10846-012-9716-3},
  groups           = {temp_for_hsi},
  modificationdate = {2023-04-04T13:43:58},
  publisher        = {Springer Science and Business Media {LLC}},
}

@Article{Tian2020Search,
  author           = {Yulun Tian and Katherine Liu and Kyel Ok and Loc Tran and Danette Allen and Nicholas Roy and Jonathan P. How},
  journal          = {The International Journal of Robotics Research},
  title            = {Search and rescue under the forest canopy using multiple {UAVs}},
  year             = {2020},
  month            = jun,
  number           = {10-11},
  pages            = {1201--1221},
  volume           = {39},
  creationdate     = {2023-03-22T21:18:54},
  doi              = {10.1177/0278364920929398},
  groups           = {temp_for_hsi},
  modificationdate = {2023-04-04T16:08:45},
  publisher        = {{SAGE} Publications},
}

@Article{Elmaliach2009Multi,
  author           = {Elmaliach, Yehuda and Agmon, Noa and Kaminka, Gal A},
  journal          = {Annals of Mathematics and Artificial Intelligence},
  title            = {Multi-robot area patrol under frequency constraints},
  year             = {2009},
  pages            = {293--320},
  volume           = {57},
  creationdate     = {2023-03-22T22:05:16},
  groups           = {spanning tree},
  modificationdate = {2023-03-22T22:05:47},
  publisher        = {Springer},
}

@InProceedings{Hazon2005Redundancy,
  author           = {Hazon, Noam and Kaminka, Gal A},
  booktitle        = {Proceedings of the 2005 IEEE international conference on robotics and automation},
  title            = {Redundancy, efficiency and robustness in multi-robot coverage},
  year             = {2005},
  organization     = {IEEE},
  pages            = {735--741},
  creationdate     = {2023-03-22T22:05:22},
  groups           = {spanning tree},
  modificationdate = {2023-03-22T22:05:38},
}

@InProceedings{Agmon2006Constructing,
  author           = {Agmon, Noa and Hazon, Noam and Kaminka, Gal A},
  booktitle        = {Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.},
  title            = {Constructing spanning trees for efficient multi-robot coverage},
  year             = {2006},
  organization     = {IEEE},
  pages            = {1698--1703},
  creationdate     = {2023-03-22T22:05:20},
  groups           = {spanning tree},
  modificationdate = {2023-03-22T22:05:43},
}

@Article{Aurenhammer1991Voronoi,
  author           = {Franz Aurenhammer},
  journal          = {{ACM} Computing Surveys},
  title            = {Voronoi diagrams{\textemdash}a survey of a fundamental geometric data structure},
  year             = {1991},
  month            = sep,
  number           = {3},
  pages            = {345--405},
  volume           = {23},
  comment          = {Voronoi diagrams},
  creationdate     = {2023-03-22T22:08:36},
  doi              = {10.1145/116873.116880},
  groups           = {spanning tree, temp_for_hsi},
  modificationdate = {2023-04-04T16:08:45},
  publisher        = {Association for Computing Machinery ({ACM})},
}

@Article{Puig2011A,
  author           = {D. Puig and M.A. Garcia and L. Wu},
  journal          = {Robotics and Autonomous Systems},
  title            = {A new global optimization strategy for coordinated multi-robot exploration: Development and comparative evaluation},
  year             = {2011},
  month            = sep,
  number           = {9},
  pages            = {635--653},
  volume           = {59},
  abstract         = {This paper proposes a new multi-robot coordinated exploration algorithm that applies a global optimization strategy based on K-Means clustering to guarantee a balanced and sustained exploration of big workspaces. The algorithm optimizes the on-line assignment of robots to targets, keeps the robots working in separate areas and efficiently reduces the variance of average waiting time on those areas. The latter ensures that the different areas of the workspace are explored at a similar speed, thus avoiding that some areas are explored much later than others, something desirable for many exploration applications, such as search & rescue. The algorithm leads to the lowest variance of regional waiting time (WTV) and the lowest variance of regional exploration percentages (EPV). Both features are presented through a comparative evaluation of the proposed algorithm with different state-of-the-art approaches.},
  comment          = {本文提出了一种新的多机器人协同探索算法，该算法应用基于均值聚类的全局优化策略来保证对大工作空间的平衡和持续探索。该算法优化了机器人对目标的在线分配，使机器人保持在不同的区域工作，并有效地减少了这些区域的平均等待时间的方差。后者确保以相似的速度探索工作空间的不同区域，从而避免探索某些区域比其他区域晚得多，这是许多探索应用程序所希望的，例如搜索和救援。该算法导致区域等待时间 (WTV) 的最低方差和区域探索百分比 (EPV) 的最低方差。这两个特征都是通过对所提出的算法与不同的最先进方法的比较评估来呈现的},
  creationdate     = {2023-03-22T22:31:53},
  doi              = {10.1016/j.robot.2011.05.004},
  groups           = {temp_for_hsi},
  modificationdate = {2023-04-04T16:08:46},
  publisher        = {Elsevier {BV}},
  url              = {https://www.sciencedirect.com/science/article/pii/S0921889011000881},
}

@Article{Li2020Improving,
  author           = {Lin Li and Xinkai Zuo and Huixiang Peng and Fan Yang and Haihong Zhu and Dalin Li and Jun Liu and Fei Su and Yifan Liang and Gang Zhou},
  journal          = {Journal of Intelligent {\&} Robotic Systems},
  title            = {Improving Autonomous Exploration Using Reduced Approximated Generalized Voronoi Graphs},
  year             = {2020},
  month            = feb,
  number           = {1},
  pages            = {91--113},
  volume           = {99},
  creationdate     = {2023-03-23T00:44:58},
  doi              = {10.1007/s10846-019-01119-6},
  groups           = {temp_for_hsi},
  modificationdate = {2023-04-04T16:08:47},
  publisher        = {Springer Science and Business Media {LLC}},
  url              = {https://link.springer.com/article/10.1007/s10846-019-01119-6},
}

@Article{Hu2020Voronoi,
  author           = {Junyan Hu and Hanlin Niu and Joaquin Carrasco and Barry Lennox and Farshad Arvin},
  journal          = {{IEEE} Transactions on Vehicular Technology},
  title            = {Voronoi-Based Multi-Robot Autonomous Exploration in Unknown Environments via Deep Reinforcement Learning},
  year             = {2020},
  month            = dec,
  number           = {12},
  pages            = {14413--14423},
  volume           = {69},
  creationdate     = {2023-03-23T00:53:11},
  doi              = {10.1109/tvt.2020.3034800},
  groups           = {temp_for_hsi},
  modificationdate = {2023-04-04T16:08:47},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
  url              = {https://ieeexplore.ieee.org/abstract/document/9244647},
}

@InCollection{Bhattacharya2013Distributed,
  author           = {Subhrajit Bhattacharya and Nathan Michael and Vijay Kumar},
  booktitle        = {Springer Tracts in Advanced Robotics},
  publisher        = {Springer Berlin Heidelberg},
  title            = {Distributed Coverage and Exploration in Unknown Non-convex Environments},
  year             = {2013},
  pages            = {61--75},
  comment          = {分布式 voronoi},
  creationdate     = {2023-03-23T01:17:35},
  doi              = {10.1007/978-3-642-32723-0_5},
  groups           = {temp_for_hsi},
  modificationdate = {2023-03-23T01:37:20},
  url              = {https://doi.org/10.1007/s10846-020-01255-4https://link.springer.com/chapter/10.1007/978-3-642-32723-0_5},
}

@Article{Renzaglia2020A,
  author           = {Alessandro Renzaglia and Jilles Dibangoye and Vincent Le Doze and Olivier Simonin},
  journal          = {Journal of Intelligent {\&} Robotic Systems},
  title            = {A Common Optimization Framework for Multi-Robot Exploration and Coverage in 3D Environments},
  year             = {2020},
  month            = sep,
  number           = {3-4},
  pages            = {1453--1468},
  volume           = {100},
  abstract         = {This paper studies the problems of static coverage and autonomous exploration of unknown three-dimensional environments with a team of cooperating aerial vehicles. Although these tasks are usually considered separately in the literature, we propose a common framework where both problems are formulated as the maximization of online acquired information via the definition of single-robot optimization functions, which differs only slightly in the two cases to take into account the static and dynamic nature of coverage and exploration respectively. A common derivative-free approach based on a stochastic approximation of these functions and their successive optimization is proposed, resulting in a fast and decentralized solution. The locality of this methodology limits however this solution to have local optimality guarantees and specific additional layers are proposed for the two problems to improve the final performance. Specifically, a Voronoi-based initialization step is added for the coverage problem and a combination with a frontier-based approach is proposed for the exploration case. The resulting algorithms are finally tested in simulations and compared with possible alternatives.},
  comment          = {voronoi初始化},
  creationdate     = {2023-03-23T01:38:39},
  doi              = {10.1007/s10846-020-01255-4},
  groups           = {temp_for_hsi},
  modificationdate = {2023-04-04T16:08:50},
  publisher        = {Springer Science and Business Media {LLC}},
  url              = {https://link.springer.com/article/10.1007/s10846-020-01255-4},
}

@InProceedings{Zhou2021Multi,
  author           = {Xiaolin Zhou and Xiaojie Liu and Xingwei Wang and Shiguang Wu and Mingyang Sun},
  booktitle        = {2021 {IEEE} 24th International Conference on Computational Science and Engineering ({CSE})},
  title            = {Multi-Robot Coverage Path Planning based on Deep Reinforcement Learning},
  year             = {2021},
  month            = oct,
  publisher        = {{IEEE}},
  abstract         = {The multi-robot coverage path planning (CPP) is the design of optimal motion sequence of robots, which can make robots execute the task covering all positions of the work area except the obstacles. In this article, the communication capability of the multi-robot system is applied, and a multi-robot CPP mechanism is proposed to control the robots to perform CPP tasks in an unknown environment. In this mechanism, an algorithm based on deep reinforcement learning is proposed, which can generate the next action for robots in real-time according to the current state of the robots. In addition, a real-time obstacle avoidance scheme for multi-robot is proposed based on the information interaction capability of multi-robot. Experiment results show that the method can plan the optimal path for multi-robot to complete the covering task in an unknown environment. Moreover, compared with other reinforcement learning methods, the algorithm proposed can efficiently learning with fast convergence speed and good stability.},
  creationdate     = {2023-03-23T02:00:11},
  doi              = {10.1109/cse53436.2021.00015},
  groups           = {temp_for_hsi},
  modificationdate = {2023-04-04T16:08:51},
}

@InProceedings{Xu2019Survey,
  author           = {Xiaoling Xu and Lixin Yang and Wei Meng and Qianqian Cai and Minyue Fu},
  booktitle        = {2019 Chinese Control Conference ({CCC})},
  title            = {Multi-Agent Coverage Search in Unknown Environments with Obstacles: A Survey},
  year             = {2019},
  month            = jul,
  publisher        = {{IEEE}},
  abstract         = {Coverage search problem is a hot research topic in computational geometry, robotics, and artificial intelligence. It not only involves basic theoretical problems such as environment modeling, optimal agent deployment, search path planning, algorithm design and analysis, but also has a wide range of applications such as the evacuation of dangerous areas, target search, and exploration of unknown environments. In view of this kind of problem, many excellent results have been achieved in recent years. In this paper, we provide a comprehensive survey on coverage search in unknown environments especially in obstacle-rich environments. The relevant works on environment modeling, agent deployment, search algorithm, etc. will be discussed and summarized. Finally, the challenges will be identified and some suggested future work will be highlighted.},
  comment          = {未知环境多智能体覆盖的综述，参考性不是很强},
  creationdate     = {2023-03-23T02:03:56},
  doi              = {10.23919/chicc.2019.8865126},
  file             = {:Xu_2019_Multi Agent Coverage Search in Unknown Environments with Obstacles_ a Survey.pdf:PDF},
  groups           = {temp_for_hsi},
  modificationdate = {2023-04-04T16:08:51},
}

@Article{Youssefi2021Swarm,
  author           = {Khalil Al-Rahman Youssefi and Modjtaba Rouhani},
  journal          = {Expert Systems with Applications},
  title            = {Swarm intelligence based robotic search in unknown maze-like environments},
  year             = {2021},
  month            = sep,
  pages            = {114907},
  volume           = {178},
  abstract         = {This paper proposes a novel decentralize and asynchronous robotic search algorithm based on particle swarm optimization (PSO), which has focused on solving mazes and finding targets in unknown environments with minimal inter-swarm communication and without any synchronization or communication center. In the proposed method, robots are advanced particles of the PSO algorithm, enriched with a toolkit, including an angle of rotation to change the course when confronted with obstacles to avoid them (AoR tool), and a memory to remember and reuse their best personal experiences to turn back from dead-ends (Mem tool). This toolkit enables the swarm to avoid obstacles and solve mazes while moving toward the target. The performance of the proposed algorithm is tested in a specially designed framework. As a validation, the proposed algorithm is compared with some recently published methods, including Adaptive Robotic PSO (A-RPSO), Robotic Bat Algorithm (RBA), and Adaptive Robotic Bat Algorithm (ARBA), in simple search environments that they can solve. The results of this comparison show that the introduced search method has the highest success rate (100%) in environments of different sizes and reflects the nature of swarm intelligence better. The proposed method is also tested in various maze-like search environments. The results depict the algorithm’s high efficiency to solve mazes in varying complexity levels and locate the target in a reliable time. It is also shown that the performance of the proposed algorithm does not decrease and remains constant as the complexity of search environments increases.},
  comment          = {迷宫环境 未知环境 搜索},
  creationdate     = {2023-03-23T02:48:16},
  doi              = {10.1016/j.eswa.2021.114907},
  groups           = {temp_for_hsi},
  modificationdate = {2023-04-04T16:08:52},
  publisher        = {Elsevier {BV}},
}

@InCollection{Gossain2022Multi,
  author           = {Hardik Gossain and Bhavya Sharma and Rachit Jain and Jai Garg},
  booktitle        = {{AI} and {IoT} for Smart City Applications},
  publisher        = {Springer Nature Singapore},
  title            = {Multi Robot Environment Exploration Using Swarm},
  year             = {2022},
  pages            = {171--183},
  abstract         = {This chapter describes the exploration of unknown environment using multiple robots. We designed this system in order to overcome the various disadvantages of exploration and mapping robots. When it comes to the mapping of large unknown environment, single robot would be inefficient and inaccurate. It leads to more exploration time and inaccurate mapping of the environment. There are multiple algorithms which already exist and are used but seeing their inefficiency we have worked on RRT that is random-exploration random tree. Multiple robots will be deployed in an unknown environment and based on which the robots will explore environment, to make robot communicate with each other we have implemented swarm algorithm so that robot communicate with each other in coordinated way. The Swarm algorithm will help robots explore the environment in a coordinated way because of which the two robots will never explore the same area repeatedly and hence will same time and will lead to more accurate mapping of the environment. To implement this solution, we used ROS as the middle ware, with the help of the ROS we can encounter and handle many real time parameters which will make our system more accurate and efficient. GAZEBO simulator is used in order to test and implement the system. This multi robot system will be very useful in the future smart city ecosystems.},
  creationdate     = {2023-03-23T02:55:23},
  doi              = {10.1007/978-981-16-7498-3_11},
  groups           = {temp_for_hsi},
  modificationdate = {2023-03-23T02:57:21},
  url              = {https://link.springer.com/chapter/10.1007/978-981-16-7498-3_11},
}

@Article{Gao2018Optimal,
  author           = {Chunqing Gao and Yingxin Kou and Zhanwu Li and An Xu and You Li and Yizhe Chang},
  journal          = {Mathematical Problems in Engineering},
  title            = {Optimal Multirobot Coverage Path Planning: Ideal-Shaped Spanning Tree},
  year             = {2018},
  month            = sep,
  pages            = {1--10},
  volume           = {2018},
  creationdate     = {2023-03-23T03:00:54},
  doi              = {10.1155/2018/3436429},
  groups           = {spanning tree},
  modificationdate = {2023-04-04T16:08:52},
  publisher        = {Hindawi Limited},
}

@Article{Cabreira2019Survey,
  author           = {Tau{\~{a}} Cabreira and Lisane Brisolara and Paulo R. Ferreira Jr.},
  journal          = {Drones},
  title            = {Survey on Coverage Path Planning with Unmanned Aerial Vehicles},
  year             = {2019},
  month            = jan,
  number           = {1},
  pages            = {4},
  volume           = {3},
  comment          = {提到了很多Cooperative Coverage的应用场景参考文献，可以参考},
  creationdate     = {2023-03-23T03:03:34},
  doi              = {10.3390/drones3010004},
  groups           = {temp_for_hsi},
  modificationdate = {2023-04-04T16:08:54},
  publisher        = {{MDPI} {AG}},
  url              = {https://www.mdpi.com/2504-446X/3/1/4},
}

@InProceedings{Li2018Multi,
  author           = {Jiadong Li and Xueqi Li and Lijuan Yu},
  booktitle        = {2018 33rd Youth Academic Annual Conference of Chinese Association of Automation ({YAC})},
  title            = {Multi-{UAV} cooperative coverage path planning in plateau and mountain environment},
  year             = {2018},
  month            = may,
  publisher        = {{IEEE}},
  abstract         = {In the environment where the topography of the plateau and mountainous terrain fluctuates, it is necessary to develop an optimal inspection plan when using a group drone to carry out detection and reconnaissance missions in this area. The overall inspection program is divided into two phases: the global optimal route planning, which is based on the parallel search strategy, establishes a nonlinear programming model with the minimum and maximum detection width as the objective function. Then it finds the track and duration of the entire area with a single drone in the least number of turns;In the local planning stage, the search area is divided according to the initial position and endurance of the group drone. An integer programming model is established and the minimum number of drones required is obtained by using a genetic algorithm. A patrol scheme is formulated to complete the continuous inspection task. The final simulation results show that this method of trajectory planning is effective and efficient.},
  comment          = {在高原、山地等地势起伏的环境中，使用集群无人机在该区域执行探测侦察任务，需要制定最优的巡检方案。整体检测方案分为两个阶段：全局最优路径规划，基于并行搜索策略，建立以最小和最大检测宽度为目标函数的非线性规划模型。然后以最少的圈数用单架无人机找到整个区域的航迹和持续时间；在局部规划阶段，根据集群无人机的初始位置和续航时间划分搜索区域。建立整数规划模型，利用遗传算法求出所需无人机的最小数量。制定巡检方案，完成巡检任务。最终的仿真结果表明该轨迹规划方法是有效且高效的},
  creationdate     = {2023-03-23T03:07:28},
  doi              = {10.1109/yac.2018.8406484},
  groups           = {temp_for_hsi},
  modificationdate = {2023-04-04T16:08:55},
  url              = {https://ieeexplore.ieee.org/document/8406484},
}

@Article{Fevgas2022Coverage,
  author           = {Georgios Fevgas and Thomas Lagkas and Vasileios Argyriou and Panagiotis Sarigiannidis},
  journal          = {Sensors},
  title            = {Coverage Path Planning Methods Focusing on Energy Efficient and Cooperative Strategies for Unmanned Aerial Vehicles},
  year             = {2022},
  month            = feb,
  number           = {3},
  pages            = {1235},
  volume           = {22},
  abstract         = {The coverage path planning (CPP) algorithms aim to cover the total area of interest with minimum overlapping. The goal of the CPP algorithms is to minimize the total covering path and execution time. Significant research has been done in robotics, particularly for multi-unmanned unmanned aerial vehicles (UAVs) cooperation and energy efficiency in CPP problems. This paper presents a review of the early-stage CPP methods in the robotics field. Furthermore, we discuss multi-UAV CPP strategies and focus on energy-saving CPP algorithms. Likewise, we aim to present a comparison between energy efficient CPP algorithms and directions for future research.},
  comment          = {更关注能源效率},
  creationdate     = {2023-03-23T03:11:45},
  doi              = {10.3390/s22031235},
  groups           = {temp_for_hsi},
  modificationdate = {2023-04-04T16:08:56},
  publisher        = {{MDPI} {AG}},
  url              = {https://www.mdpi.com/1424-8220/22/3/1235},
}

 
@InProceedings{Chen2022Multi,
  author           = {Chen, Bo and Zhang, Wenli and Zhang, Fangfang and Xin, Jianbin and Liu, Yanhong},
  booktitle        = {Proceedings of 2021 {Chinese} {Intelligent} {Automation} {Conference}},
  title            = {Multi-robot {Cooperate} {Region} {Coverage} {Search} {Algorithm} {Based} on {Distributed} {Control}},
  year             = {2022},
  address          = {Singapore},
  editor           = {Deng, Zhidong},
  pages            = {546--554},
  publisher        = {Springer},
  series           = {Lecture {Notes} in {Electrical} {Engineering}},
  abstract         = {Aiming at the situation that multi-robot system cannot establish global communication when performing region coverage task in unknown environment, a multi-robot cooperative region coverage search algorithm based on distributed control is proposed. Bio-inspired neural network and raster map are combined to represent dynamic search environment. Weighted average method (WAM) is used to fuse environmental information collected by different robots. In addition, several dynamic search alliances are formed among robots under the framework of distributed model prediction (DMPC). Within the alliance, each member makes iterative collaborative decision in turn, and the genetic algorithm (GA) is used to optimize the solution to obtain the next movement path of each robot. Simulation results show the effectiveness and superiority of the proposed algorithm.},
  comment          = {分布式
信息融合，分布式模型预测控制，动态联盟，遗传算法

针对多机器人系统在未知环境下执行区域覆盖任务时无法建立全局通信的情况，提出了一种基于分布式控制的多机器人协作区域覆盖搜索算法。仿生神经网络和栅格地图相结合来表示动态搜索环境。加权平均法（WAM）用于融合不同机器人收集的环境信息。此外，在分布式模型预测（DMPC）的框架下，机器人之间形成了几个动态搜索联盟。在联盟内部，各成员轮流进行迭代协同决策，利用遗传算法（GA）对解进行优化，得到各机器人的下一步运动路径。仿真结果表明了所提算法的有效性和优越性},
  creationdate     = {2023-03-23T14:24:57},
  doi              = {10.1007/978-981-16-6372-7_60},
  file             = {:Chen_2022_Multi Robot Cooperate Region Coverage Search Algorithm Based on Distributed Control.pdf:PDF},
  groups           = {temp_for_hsi},
  isbn             = {9789811663727},
  keywords         = {Multi-robot, Distributed control, Weighted average method, Bio-inspired neural network},
  language         = {en},
  modificationdate = {2023-03-23T14:27:46},
}

@InProceedings{Portugal2010MSP,
  author           = {David Portugal and Rui Rocha},
  booktitle        = {Proceedings of the 2010 {ACM} Symposium on Applied Computing},
  title            = {MSP algorithm: multi-robot patrolling based on territory allocation using balanced graph partitioning},
  year             = {2010},
  month            = mar,
  publisher        = {{ACM}},
  abstract         = {This article addresses the problem of efficient multi-robot patrolling in a known environment. The proposed approach assigns regions to each mobile agent. Every region is represented by a subgraph extracted from the topological representation of the global environment. A new algorithm is proposed in order to deal with the local patrolling task assigned for each robot, named Multilevel Subgraph Patrolling (MSP) Algorithm. It handles some major graph theory classic problems like graph partitioning, Hamilton cycles, non-Hamilton cycles and longest path searches. The flexible, scalable, robust and high performance nature of this approach is testified by simulation results.},
  comment          = {图划分

本文解决了在已知环境中高效多机器人巡逻的问题。所提出的方法将区域分配给每个移动代理。每个区域都由从全球环境的拓扑表示中提取的子图表示。为了处理分配给每个机器人的本地巡逻任务，提出了一种新算法，称为多级子图巡逻（MSP）算法。它处理一些主要的图论经典问题，如图划分、汉密尔顿循环、非汉密尔顿循环和最长路径搜索。仿真结果证明了这种方法的灵活、可扩展、稳健和高性能的特性},
  creationdate     = {2023-03-23T14:27:27},
  doi              = {10.1145/1774088.1774360},
  groups           = {temp_for_hsi},
  modificationdate = {2023-04-04T16:08:57},
}

@InCollection{Nattero2017Partitioning,
  author           = {Cristiano Nattero and Fulvio Mastrogiovanni},
  booktitle        = {Intelligent Autonomous Systems 14},
  publisher        = {Springer International Publishing},
  title            = {Partitioning Strategies for Multi-robot Area Coverage with No Communication},
  year             = {2017},
  pages            = {615--628},
  comment          = {混合蚁群优化方法

在本文中，我们将基于拓扑的无通信多机器人区域覆盖分区策略建模为平衡图分区问题，该问题受连接性和可靠性要求的约束。我们将其形式化为组合优化问题，提出包含其最相关特征的广义公式，并引入基本问题的两种变体。此外，我们还讨论了一种基于混合蚁群优化方法的元启发式求解方法。最后，通过仿真验证解决方案的性能。},
  creationdate     = {2023-03-23T14:28:30},
  doi              = {10.1007/978-3-319-48036-7_45},
  file             = {:Nattero_2017_Partitioning Strategies for Multi Robot Area Coverage with No Communication.pdf:PDF},
  groups           = {temp_for_hsi},
  modificationdate = {2023-03-23T14:35:19},
}

 
@InProceedings{Tolstaya2021Multi,
  author           = {Tolstaya, Ekaterina and Paulos, James and Kumar, Vijay and Ribeiro, Alejandro},
  booktitle        = {2021 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
  title            = {Multi-{Robot} {Coverage} and {Exploration} using {Spatial} {Graph} {Neural} {Networks}},
  year             = {2021},
  month            = sep,
  note             = {ISSN: 2153-0866},
  pages            = {8944--8950},
  abstract         = {The multi-robot coverage problem is an essential building block for systems that perform tasks like inspection, exploration, or search and rescue. We discretize the coverage problem to induce a spatial graph of locations and represent robots as nodes in the graph. Then, we train a Graph Neural Network controller that leverages the spatial equivariance of the task to imitate an expert open-loop routing solution. This approach generalizes well to much larger maps and larger teams that are intractable for the expert. In particular, the model generalizes effectively to a simulation of ten quadrotors and dozens of buildings in an urban setting. We also demonstrate the GNN controller can surpass planning-based approaches in an exploration task.},
  comment          = {基于 空间图神经网络，实现多机器人覆盖和探索，可以看看这篇文章的相关文献},
  creationdate     = {2023-03-23T14:36:59},
  doi              = {10.1109/IROS51168.2021.9636675},
  groups           = {temp_for_hsi},
  issn             = {2153-0866},
  keywords         = {Buildings, Inspection, Search problems, Routing, Graph neural networks, Task analysis, Intelligent robots},
  modificationdate = {2023-03-23T14:37:38},
}

@InProceedings{Preiss2017Crazyswarm,
  author           = {James A. Preiss and Wolfgang Honig and Gaurav S. Sukhatme and Nora Ayanian},
  booktitle        = {2017 {IEEE} International Conference on Robotics and Automation ({ICRA})},
  title            = {Crazyswarm: A large nano-quadcopter swarm},
  year             = {2017},
  month            = may,
  publisher        = {{IEEE}},
  creationdate     = {2023-03-23T16:05:19},
  doi              = {10.1109/icra.2017.7989376},
  groups           = {temp_for_hsi},
  modificationdate = {2023-04-04T16:08:57},
}

 
@Article{Parker2006Building,
  author           = {Parker, L.E. and Tang, Fang},
  journal          = {Proceedings of the IEEE},
  title            = {Building {Multirobot} {Coalitions} {Through} {Automated} {Task} {Solution} {Synthesis}},
  year             = {2006},
  issn             = {1558-2256},
  month            = jul,
  number           = {7},
  pages            = {1289--1305},
  volume           = {94},
  abstract         = {This paper presents a reasoning system that enables a group of heterogeneous robots to form coalitions to accomplish a multirobot task using tightly coupled sensor sharing. Our approach, which we call ASyMTRe, maps environmental sensors and perceptual and motor control schemas to the required flow of information through the multirobot system, automatically reconfiguring the connections of schemas within and across robots to synthesize valid and efficient multirobot behaviors for accomplishing a multirobot task. We present the centralized anytime ASyMTRe configuration algorithm, proving that the algorithm is correct, and formally addressing issues of completeness and optimality. We then present a distributed version of ASyMTRe, called ASyMTRe-D, which uses communication to enable distributed coalition formation. We validate the centralized approach by applying the ASyMTRe methodology to two application scenarios: multirobot transportation and multirobot box pushing. We then validate the ASyMTRe-D implementation in the multirobot transportation task, illustrating its fault-tolerance capabilities. The advantages of this new approach are that it: 1) enables robots to synthesize new task solutions using fundamentally different combinations of sensors and effectors for different coalition compositions and 2) provides a general mechanism for sharing sensory information across networked robots},
  creationdate     = {2023-03-23T16:08:17},
  doi              = {10.1109/JPROC.2006.876933},
  groups           = {temp_for_hsi},
  keywords         = {Robot sensing systems, Control system synthesis, Robotics and automation, Sensor systems, Transportation, Couplings, Motor drives, Multirobot systems, Fault tolerance, Network synthesis, Coalition formation, information invariants, multirobot teams, schema theory, sensor sharing, task allocation},
  modificationdate = {2023-03-23T16:08:29},
}

@Article{Bayindir2016Review,
  author           = {Levent Bay{\i}nd{\i}r},
  journal          = {Neurocomputing},
  title            = {A review of swarm robotics tasks},
  year             = {2016},
  month            = jan,
  pages            = {292--321},
  volume           = {172},
  abstract         = {Swarm intelligence principles have been widely studied and applied to a number of different tasks where a group of autonomous robots is used to solve a problem with a distributed approach, i.e. without central coordination. A survey of such tasks is presented, illustrating various algorithms that have been used to tackle the challenges imposed by each task. Aggregation, flocking, foraging, object clustering and sorting, navigation, path formation, deployment, collaborative manipulation and task allocation problems are described in detail, and a high-level overview is provided for other swarm robotics tasks. For each of the main tasks, (1) swarm design methods are identified, (2) past works are divided in task-specific categories, and (3) mathematical models and performance metrics are described. Consistently with the swarm intelligence paradigm, the main focus is on studies characterized by distributed control, simplicity of individual robots and locality of sensing and communication. Distributed algorithms are shown to bring cooperation between agents, obtained in various forms and often without explicitly programming a cooperative behavior in the single robot controllers. Offline and online learning approaches are described, and some examples of past works utilizing these approaches are reviewed.},
  creationdate     = {2023-03-23T16:22:34},
  doi              = {10.1016/j.neucom.2015.05.116},
  groups           = {temp_for_hsi},
  modificationdate = {2023-04-04T16:08:58},
  publisher        = {Elsevier {BV}},
}

 
@InProceedings{Podevijn2012Self,
  author           = {Podevijn, Gaetan and O'Grady, Rehan and Dorigo, Marco},
  booktitle        = {Proceedings of the Workshop on Robot Feedback in Human-Robot Interaction: How to Make a Robot Readable for a Human Interaction Partner},
  title            = {Self-organised {Feedback} in {Human} {Swarm} {Interaction}},
  year             = {2012},
  abstract         = {Human-swarm interaction (HSI) consists of bidirectional interaction between a human operator and swarms of autonomous robots. In HSI, a human operator directs robots to carry out tasks. However, in order to direct a swarm of robots, the operator must receive appropriate feedback about what is going on in the swarm. In this paper, we argue that self-organised mechanisms should be responsible for providing feedback in HSI systems, and argue against the current approach that involves an extra ‘intepretation layer’ layer dependent on additional infrastructure and modelling. We present a recent study that we conducted in the field of HSI, in which a human operator had to guide groups of robots to designated task completion zones. Based on this study, we propose some initial steps towards our vision of self-organised feedback.},
  comment          = {人群交互 (HSI) 包括人类操作员和自主机器人群之间的双向交互。在 HSI 中，操作员指挥机器人执行任务。然而，为了指挥一群机器人，操作员必须收到关于群体中发生的事情的适当反馈。在本文中，我们认为自组织机制应该负责在 HSI 系统中提供反馈，并反对当前的方法，该方法涉及依赖于额外基础设施和建模的额外“解释层”层。我们介绍了我们最近在 HSI 领域进行的一项研究，其中人类操作员必须引导机器人组到指定的任务完成区域。基于这项研究，我们提出了实现自组织反馈愿景的一些初步步骤},
  creationdate     = {2023-03-23T16:38:39},
  groups           = {temp_for_hsi},
  language         = {en},
  modificationdate = {2023-03-23T21:38:51},
  url              = {https://www.academia.edu/2812245/Self_organised_Feedback_in_Human_Swarm_Interaction},
  urldate          = {2023-03-23},
}

@Article{Nunnally2013Using,
  author           = {Steven Nunnally and Phillip Walker and Mike Lewis and Nilanjan Chakraborty and Katia Sycara},
  journal          = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
  title            = {Using Haptic Feedback in Human Robotic Swarms Interaction},
  year             = {2013},
  month            = sep,
  number           = {1},
  pages            = {1047--1051},
  volume           = {57},
  abstract         = {Robotic swarms display emergent behaviors that are robust to failure of individual robots, although they can not necessarily accomplish complex tasks with these behaviors. The research objective is to make use of their robust behaviors to accomplish complex tasks in many types of environments. For now, it is difficult to affect swarm “goals”, and therefore difficult them to direct to perform complex tasks. The extant literature on Human Swarm Interaction (HSI) focuses on demonstrating the usefulness of human operator inputs for swarms to accomplish complex tasks. The human typically gets visual feedback of the state of the swarm and influences the robots through a computer interface. This paper presents a user study investigating the effectiveness of haptic feedback in improving HSI. We use methods developed in studies using haptics in multi-robot systems (where the communication and structure is very rigid) and potential field algorithms developed for fully-autonomous swarms to determine the benefits of haptic feedback from the semi-autonomous control algorithm. In some environments, haptic feedback proved beneficial whereas in other environments haptic feedback did not improve performance over visual feedback alone. However, presence of haptic feedback did not degrade the performance under any of the experimental conditions. This supports our working hypothesis that haptic feedback is potentially useful in HSI.},
  comment          = {机器人群表现出对单个机器人故障具有鲁棒性的紧急行为，尽管它们不一定能用这些行为完成复杂的任务。研究目标是利用其稳健的行为在多种类型的环境中完成复杂的任务。目前，很难影响群体“目标”，因此很难指导它们执行复杂的任务。关于人类群体相互作用（HSI）的现有文献侧重于证明人类操作员输入对群体完成复杂任务的有用性。人类通常会获得群体状态的视觉反馈，并通过计算机界面影响机器人。本文介绍了一项用户研究，调查触觉反馈在改善HSI方面的有效性。我们使用在多机器人系统（通信和结构非常僵化）中使用触觉的研究开发的方法和为完全自主群开发的势场算法来确定半自主控制算法的触觉反馈的好处。在某些环境中，触觉反馈被证明是有益的，而在另一些环境中，触觉反馈并没有比单独的视觉反馈提高性能。然而，触觉反馈的存在并没有降低任何实验条件下的性能。这支持了我们的工作假设，即触觉反馈在HSI中可能有用。},
  creationdate     = {2023-03-23T16:59:11},
  doi              = {10.1177/1541931213571233},
  groups           = {Human-Swarm},
  modificationdate = {2023-04-04T16:08:59},
  publisher        = {{SAGE} Publications},
}

 
@Article{Batra2022Augmented,
  author           = {Batra, Sumeet and Klingner, John and Correll, Nikolaus},
  journal          = {Artificial Life and Robotics},
  title            = {Augmented reality for human–swarm interaction in a swarm-robotic chemistry simulation},
  year             = {2022},
  issn             = {1614-7456},
  month            = may,
  number           = {2},
  pages            = {407--415},
  volume           = {27},
  abstract         = {We present a novel augmented reality (AR) framework to show relevant information about swarm dynamics to a user in the absence of markers by using blinking frequency to distinguish between groups in the swarm. In order to distinguish between groups, clusters of the same group are identified by blinking at a specific time interval that is distinct from the time interval at which their neighbors blink. The problem is thus to find blinking sequences that are distinct for each group with respect to the group’s neighbors. Selecting an appropriate sequence is an instance of the distributed graph coloring problem, which can be solved in \$\$O({\textbackslash}log (n))\$\$time with n being the number of robots involved. We demonstrate our approach using a swarm chemistry simulation in which robots simulate individual atoms that form molecules following the rules of chemistry. An AR display is then used to display information about the internal state of individual swarm members as well as their topological relationship, corresponding to molecular bonds in a context that uses robot swarms to teach basic chemistry concepts.},
  comment          = {在HSI中采用了AR技术},
  creationdate     = {2023-03-23T20:25:09},
  doi              = {10.1007/s10015-022-00763-w},
  groups           = {temp_for_hsi},
  keywords         = {群体机器人, 化学, 增强现实, 人机交互},
  language         = {en},
  modificationdate = {2023-03-23T20:26:00},
  url              = {https://doi.org/10.1007/s10015-022-00763-w},
  urldate          = {2023-03-23},
}

@Article{Shao2021UAV,
  author           = {Rui Shao and Rentuo Tao and Youda Liu and Yuhao Yang and Dasheng Li and Jianjun Chen},
  journal          = {{EURASIP} Journal on Advances in Signal Processing},
  title            = {{UAV} cooperative search in dynamic environment based on hybrid-layered {APF}},
  year             = {2021},
  month            = oct,
  number           = {1},
  volume           = {2021},
  abstract         = {Unmanned aerial vehicle (UAV) detection has the advantages of flexible deployment and no casualties. It has become a force that cannot be ignored in the battlefield. Scientific and efficient mission planning can help improving the survival rate and mission completion rate of the UAV search in dynamic environments. Towards the mission planning problem of UAV collaborative search for multi-types of time-sensitive moving targets, a search algorithm based on hybrid layered artificial potential fields algorithm (HL-APF) was proposed. This method consists of two parts, a distributed artificial field algorithm and a centralized layered algorithm. In the improved artificial potential field (IAPF), this paper utilized a new target attraction field function which was segmented by the search distance to quickly search for dynamic targets. Moreover, in order to solve the problem of repeated search by the UAV in a short time interval, a search repulsion field generated by the UAV search path was proposed. Besides, in order to solve the unknown target search and improve the area coverage, a centralized layered scheduling algorithm controlled by the cloud server (CS) was added. CS divides the mission area into several sub-areas, and allocates UAV according to the priority function based on the search map. The CS activation mechanism can make full use of prior information, and the UAV assignment cool-down mechanism can avoid the repeated assignment of the same UAV. The simulation results show that compared with the hybrid artificial potential field and ant colony optimization and IAPF, HL-APF can significantly improve the number of targets and mission area coverage. Moreover, comparative experiment results of CS mechanism proved the necessity of setting CS activation and cool-down for improving the search performance. Finally, it also verified the robustness of the method under the failure of some UAVs.},
  comment          = {针对无人机协同搜索多类型时间敏感运动目标的任务规划问题，提出一种基于混合分层人工势场算法（HL-APF）的搜索算法。该方法由分布式人工场算法和集中式分层算法两部分组成。},
  creationdate     = {2023-03-23T20:31:01},
  doi              = {10.1186/s13634-021-00807-6},
  groups           = {temp_for_hsi},
  modificationdate = {2023-04-04T16:08:59},
  publisher        = {Springer Science and Business Media {LLC}},
}

@InProceedings{Li2020Research,
  author           = {Rui Li and Hongzhong Ma},
  booktitle        = {2020 3rd International Conference on Unmanned Systems ({ICUS})},
  title            = {Research on {UAV} Swarm Cooperative Reconnaissance and Combat Technology},
  year             = {2020},
  month            = nov,
  publisher        = {{IEEE}},
  comment          = {无人机群协同侦察与作战技术研究},
  creationdate     = {2023-03-23T20:33:09},
  doi              = {10.1109/icus50048.2020.9274902},
  groups           = {temp_for_hsi},
  modificationdate = {2023-04-04T16:09:00},
}

@InProceedings{Husni2017Cooperative,
  author           = {Nyayu Latifah Husni and Ade Silvia Handayani and Siti Nurmaini and Irsyadi Yani},
  booktitle        = {2017 International Conference on Electrical Engineering and Computer Science ({ICECOS})},
  title            = {Cooperative searching strategy for swarm robot},
  year             = {2017},
  month            = aug,
  publisher        = {{IEEE}},
  abstract         = {This paper represents a cooperative searching strategy of swarm robot in finding and localizing odor source. 3 robots, namely: Cyborg G11, Cyborg G12, and Cyborg G13 were used as agents that form the swarm in this research. The cooperation among the robots trough communication has shown the success of the swarm in odor searching. When one of the agents in swarm robot detected a high concentration of odor in its place, it will use the data of this concentration as its own information in order to decide what position it should go. Moreover, this data was also shared to the other robot trough wireless communication such that they can compare their own data with the data shared to them. By having the information, each agent can decide what solution they should take. The experimental work in this research shows that using communication among the robot can support the robots in searching and finding the odor source.},
  comment          = {finding and localizing odor source 气体寻源和定位},
  creationdate     = {2023-03-23T20:34:35},
  doi              = {10.1109/icecos.2017.8167174},
  groups           = {temp_for_hsi},
  modificationdate = {2023-04-04T16:09:01},
  url              = {https://ieeexplore.ieee.org/abstract/document/8167174},
}

@InProceedings{Yang2017Cooperative,
  author           = {Fan Yang and Xiuling Ji and Chengwei Yang and Jie Li and Bing Li},
  booktitle        = {2017 {IEEE} International Conference on Unmanned Systems ({ICUS})},
  title            = {Cooperative search of {UAV} swarm based on improved ant colony algorithm in uncertain environment},
  year             = {2017},
  month            = oct,
  publisher        = {{IEEE}},
  abstract         = {Considering the situation that the regional environment is completely unknown and is dynamically changing, a novel method for multi-UAV (Unmanned Aerial Vehicle) conducting cooperative area search based on Ant Colony (AC) Theory is proposed. Firstly, the regional environment model and the UAV dynamic model were established. Secondly, the transfer rules of the waypoints were determined for UAVs by improving the behavior criterion of the ant colony algorithm and the updating principle of the pheromone map. Finally, Compared with the conventional search methods, the simulation results with higher coverage rate and search efficiency demonstrated the improved ant colony algorithm was rationality and validity.},
  comment          = {对蚁群算法进行了改进 uav},
  creationdate     = {2023-03-23T20:41:12},
  doi              = {10.1109/icus.2017.8278346},
  groups           = {temp_for_hsi},
  modificationdate = {2023-04-04T16:09:02},
}

 
@InProceedings{Zhicai2021A,
  author           = {Zhicai Ren and Jiang, Bo and Hong Xu},
  booktitle        = {2021 {IEEE} 6th {International} {Conference} on {Computer} and {Communication} {Systems} ({ICCCS})},
  title            = {A {Cooperative} {Search} {Algorithm} {Based} on {Improved} {Particle} {Swarm} {Optimization} {Decision} for {UAV} {Swarm}},
  year             = {2021},
  month            = apr,
  pages            = {140--145},
  abstract         = {This paper presents an autonomous cooperative search algorithm for distributed micro unmanned aerial vehicles (UAVs), The probability model of the search graph is established by bayesian theory, and the search problem is transformed into the optimization problem of the cost function. In order to avoid a large number of UAVs gathering in the same place at the same time. We introduce the region density value as one of the optimization objectives. The purpose is to make the UAV swarm move to undeveloped areas and we use a particle swarm optimization algorithm to solve this model. The simulation shows that the algorithm effectively prevents clustering when the UAV cluster is large, so this algorithm can improve the search efficiency of UAV swarm.},
  comment          = {提出一种分布式微型无人机自主协同搜索算法，利用贝叶斯理论建立搜索图的概率模型，将搜索问题转化为代价函数的优化问题。以避免大量无人机同时聚集在同一个地方。我们引入区域密度值作为优化目标之一。目的是使无人机群移动到未开发区域，我们使用粒子群优化算法来求解该模型。仿真表明该算法在无人机集群较大时有效地防止了聚类，因此该算法可以提高无人机群的搜索效率},
  creationdate     = {2023-03-23T20:45:08},
  doi              = {10.1109/ICCCS52626.2021.9449283},
  file             = {:Ren_2021_A Cooperative Search Algorithm Based on Improved Particle Swarm Optimization Decision for UAV Swarm.pdf:PDF},
  groups           = {temp_for_hsi},
  keywords         = {Communication systems, Conferences, Clustering algorithms, Self-organizing networks, Search problems, Cost function, Bayes methods, regional search, swarm of UAVs, particle swarm optimization algorithm},
  modificationdate = {2023-04-01T16:02:48},
}

 
@TechReport{Heydari2021Reinforcement,
  author           = {Heydari, Javad and Saha, Olimpiya and Ganapathy, Viswanath},
  title            = {Reinforcement {Learning}-{Based} {Coverage} {Path} {Planning} with {Implicit} {Cellular} {Decomposition}},
  year             = {2021},
  month            = oct,
  note             = {arXiv:2110.09018 [cs] type: article},
  abstract         = {Coverage path planning in a generic known environment is shown to be NP-hard. When the environment is unknown, it becomes more challenging as the robot is required to rely on its online map information built during coverage for planning its path. A significant research effort focuses on designing heuristic or approximate algorithms that achieve reasonable performance. Such algorithms have sub-optimal performance in terms of covering the area or the cost of coverage, e.g., coverage time or energy consumption. In this paper, we provide a systematic analysis of the coverage problem and formulate it as an optimal stopping time problem, where the trade-off between coverage performance and its cost is explicitly accounted for. Next, we demonstrate that reinforcement learning (RL) techniques can be leveraged to solve the problem computationally. To this end, we provide some technical and practical considerations to facilitate the application of the RL algorithms and improve the efficiency of the solutions. Finally, through experiments in grid world environments and Gazebo simulator, we show that reinforcement learning-based algorithms efficiently cover realistic unknown indoor environments, and outperform the current state of the art.},
  annote           = {Comment: 20 pages},
  comment          = {貌似是单agent},
  creationdate     = {2023-03-23T22:59:44},
  doi              = {10.48550/arXiv.2110.09018},
  file             = {arXiv Fulltext PDF:https\://arxiv.org/pdf/2110.09018.pdf:application/pdf},
  groups           = {temp_for_hsi},
  keywords         = {Computer Science - Robotics, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  modificationdate = {2023-03-23T23:04:51},
  school           = {arXiv},
  url              = {http://arxiv.org/abs/2110.09018},
  urldate          = {2023-03-23},
}

@InProceedings{Cummings2004Human,
  author           = {M. L. Cummings},
  booktitle        = {2nd Annual Swarming: Autonomous Intelligent Networked Systems Conference},
  title            = {Human supervisory control of swarming networks},
  year             = {2004},
  month            = jun,
  pages            = {1--9},
  creationdate     = {2023-03-24T01:18:41},
  groups           = {temp_for_hsi},
  modificationdate = {2023-03-24T01:20:07},
}

@Article{Penders2011A,
  author           = {Jacques Penders and Lyuba Alboul and Ulf Witkowski and Amir Naghsh and Joan Saez-Pons and Stefan Herbrechtsmeier and Mohamed El-Habbal},
  journal          = {Advanced Robotics},
  title            = {A Robot Swarm Assisting a Human Fire-Fighter},
  year             = {2011},
  month            = jan,
  number           = {1-2},
  pages            = {93--117},
  volume           = {25},
  abstract         = {Emergencies in industrial warehouses are a major concern for fire-fighters. The large dimensions, together with the development of dense smoke that drastically reduces visibility, represent major challenges. The GUARDIANS robot swarm is designed to assist fire-fighters in searching a large warehouse. In this paper we discuss the technology developed for a swarm of robots assisting fire-fighters. We explain the swarming algorithms that provide the functionality by which the robots react to and follow humans while no communication is required. Next we discuss the wireless communication system, which is a so-called mobile ad-hoc network. The communication network provides also the means to locate the robots and humans. Thus, the robot swarm is able to provide guidance information to the humans. Together with the fire-fighters we explored how the robot swarm should feed information back to the human fire-fighter. We have designed and experimented with interfaces for presenting swarm-based information to human beings.},
  comment          = {HSI用于火灾领域},
  creationdate     = {2023-03-27T16:24:57},
  doi              = {10.1163/016918610x538507},
  file             = {:Penders_2011_A Robot Swarm Assisting a Human Fire Fighter.pdf:PDF},
  groups           = {temp_for_hsi},
  modificationdate = {2023-04-04T20:28:46},
  publisher        = {Informa {UK} Limited},
}

@Article{Zhao2022Cooperative,
  author           = {Pengcheng Zhao and Xiang Li and Shang Gao and Xiaohui Wei},
  journal          = {J Syst Architect},
  title            = {Cooperative task assignment in spatial crowdsourcing via multi-agent deep reinforcement learning},
  year             = {2022},
  month            = jul,
  pages            = {102551},
  volume           = {128},
  creationdate     = {2022-11-05T10:41:00},
  doi              = {10.1016/j.sysarc.2022.102551},
  file             = {:Zhao_2022_Cooperative Task Assignment in Spatial Crowdsourcing Via Multi Agent Deep Reinforcement Learning.pdf:PDF},
  groups           = {Spatial, RL-based},
  modificationdate = {2023-04-04T20:27:04},
  publisher        = {Elsevier {BV}},
}

 
@Article{Chen2023Milestones,
  author           = {Chen, Long and Li, Yuchen and Huang, Chao and Li, Bai and Xing, Yang and Tian, Daxin and Li, Li and Hu, Zhongxu and Na, Xiaoxiang and Li, Zixuan and Teng, Siyu and Lv, Chen and Wang, Jinjun and Cao, Dongpu and Zheng, Nanning and Wang, Fei-Yue},
  journal          = {IEEE Transactions on Intelligent Vehicles},
  title            = {Milestones in {Autonomous} {Driving} and {Intelligent} {Vehicles}: {Survey} of {Surveys}},
  year             = {2023},
  issn             = {2379-8904},
  number           = {2},
  pages            = {1046--1056},
  volume           = {8},
  abstract         = {Interest in autonomous driving (AD) and intelligent vehicles (IVs) is growing at a rapid pace due to the convenience, safety, and economic benefits. Although a number of surveys have reviewed research achievements in this field, they are still limited in specific tasks, lack of systematic summary and research directions in the future. Here we propose a Survey of Surveys (SoS) for total technologies of AD and IVs that reviews the history, summarizes the milestones, and provides the perspectives, ethics, and future research directions. To our knowledge, this article is the first SoS with milestones in AD and IVs, which constitutes our complete research work together with two other technical surveys. We anticipate that this article will bring novel and diverse insights to researchers and abecedarians, and serve as a bridge between past and future.},
  creationdate     = {2023-03-30T21:33:38},
  doi              = {10.1109/TIV.2022.3223131},
  file             = {:Chen_2023_Milestones in Autonomous Driving and Intelligent Vehicles_ Survey of Surveys.pdf:PDF},
  groups           = {TIV_ref},
  keywords         = {Autonomous vehicles, Planning, Vehicle dynamics, Heuristic algorithms, Object detection, Location awareness, Task analysis, Survey of surveys, milestones, autonomous driving, intelligent vehicles},
  modificationdate = {2023-03-30T21:34:14},
  shorttitle       = {Milestones in {Autonomous} {Driving} and {Intelligent} {Vehicles}},
}

 
@Article{Zhu2022Crowdsensing,
  author           = {Zhu, Zhengqiu and Wang, Xiao and Zhao, Yong and Qiu, Sihang and Liu, Zhong and Chen, Bin and Wang, Fei-Yue},
  journal          = {IEEE Transactions on Intelligent Vehicles},
  title            = {Crowdsensing {Intelligence} by {Decentralized} {Autonomous} {Vehicles} {Organizations} and {Operations}},
  year             = {2022},
  issn             = {2379-8904},
  number           = {4},
  pages            = {804--808},
  volume           = {7},
  abstract         = {With the rapid growth of connected and autonomous vehicles (CAVs), vehicular crowdsensing (VCS) has emerged as an effective way in a wide range of applications, especially in intelligent transportation systems. However, centralized VCS frameworks have confronted many problems, such as privacy, security, utility, and dependability. To remedy these challenges, blockchain technology can be applied in VCS systems for effectively forming decentralized autonomous vehicles organizations and operations. This article briefly introduces blockchain-based VCS solutions addressing the current problems and presents potential directions for future research.},
  creationdate     = {2023-03-30T21:35:44},
  doi              = {10.1109/TIV.2022.3224918},
  file             = {:Zhu_2022_Crowdsensing Intelligence by Decentralized Autonomous Vehicles Organizations and Operations.pdf:PDF},
  groups           = {TIV_ref},
  keywords         = {Blockchains, Autonomous vehicles, Smart contracts, Privacy, Crowdsensing, Security, Data privacy, Connected vehicles, Connected and autonomous vehicles, vehicular crowdsensing, blockchain technology},
  modificationdate = {2023-03-30T21:39:20},
}

 
@Article{Zhu2023A,
  author           = {Zhu, Zhengqiu and Zhao, Yong and Chen, Bin and Qiu, Sihang and Liu, Zhong and Xie, Kun and Ma, Liang},
  journal          = {IEEE Transactions on Intelligent Vehicles},
  title            = {A {Crowd}-{Aided} {Vehicular} {Hybrid} {Sensing} {Framework} for {Intelligent} {Transportation} {Systems}},
  year             = {2023},
  issn             = {2379-8904},
  number           = {2},
  pages            = {1484--1497},
  volume           = {8},
  abstract         = {In traditional practices of transportation system's constructions, traffic-related information is collected based on dedicated sensor networks, which are not only coverage-limited but also cost-consuming. With the enrichment of the concepts concerning “social sensors” and “social transportation”, Sparse Mobile Crowdsensing (MCS) is proposed to collect data from only a few subareas by recruiting participants with portable devices and to infer the data in unsensed subareas with acceptable errors. However, in real-world sensing campaigns, the Sparse MCS systems often fail to collect data from any subareas of interest since the assumption about sufficient participants is not always realistic. To be specific, the recruitment of participants is often limited by interest deficiency, privacy awareness, and distribution biases. To handle this problem, we introduce the dedicated sensing vehicles (DSVs) into traditional Sparse MCS to improve subarea coverage and inference performance. To achieve effective collaboration among DSVs and mobile users, we first design a crowd-aided vehicular hybrid sensing framework, which defines the order of task assignment for different participants as well as the budget allocation. In terms of DSVs route planning, we propose a three-step strategy, including optimal route searching, fused route selection, and final route determination. Moreover, mobile users are selected based on a novel selection strategy. Experimental findings on two real-world datasets validate the effectiveness (with less inference error) of the hybrid sensing framework, in comparison with the user-only/DSV-only framework and five baselines. Results reveal important implications of applying the hybrid sensing paradigm in intelligent transportation systems to enhance data collection},
  creationdate     = {2023-03-30T21:37:30},
  doi              = {10.1109/TIV.2022.3216318},
  file             = {:Zhu_2023_A Crowd Aided Vehicular Hybrid Sensing Framework for Intelligent Transportation Systems.pdf:PDF},
  groups           = {TIV_ref},
  keywords         = {Sensors, Task analysis, Transportation, Resource management, Intelligent vehicles, Social networking (online), Crowdsensing, Social sensors, social transportation, sparse mobile crowdsensing, hybrid sensing framework, intelligent transportation systems},
  modificationdate = {2023-03-30T21:39:24},
}

 
@Article{Teng2023Hierarchical,
  author           = {Teng, Siyu and Chen, Long and Ai, Yunfeng and Zhou, Yuanye and Xuanyuan, Zhe and Hu, Xuemin},
  journal          = {IEEE Transactions on Intelligent Vehicles},
  title            = {Hierarchical {Interpretable} {Imitation} {Learning} for {End}-to-{End} {Autonomous} {Driving}},
  year             = {2023},
  issn             = {2379-8904},
  number           = {1},
  pages            = {673--683},
  volume           = {8},
  abstract         = {End-to-end autonomous driving provides a simple and efficient framework for autonomous driving systems, which can directly obtain control commands from raw perception data. However, it fails to address stability and interpretability problems in complex urban scenarios. In this paper, we construct a two-stage end-to-end autonomous driving model for complex urban scenarios, named HIIL (Hierarchical Interpretable Imitation Learning), which integrates interpretable BEV mask and steering angle to solve the problems shown above. In Stage One, we propose a pretrained Bird's Eye View (BEV) model which leverages a BEV mask to present an interpretation of the surrounding environment. In Stage Two, we construct an Interpretable Imitation Learning (IIL) model that fuses BEV latent feature from Stage One with an additional steering angle from Pure-Pursuit (PP) algorithm. In the HIIL model, visual information is converted to semantic images by the semantic segmentation network, and the semantic images are encoded to extract the BEV latent feature, which are decoded to predict BEV masks and fed to the IIL as perception data. In this way, the BEV latent feature bridges the BEV and IIL models. Visual information can be supplemented by the calculated steering angle for PP algorithm, speed vector, and location information, thus it could have better performance in complex and terrible scenarios. Our HIIL model meets an urgent requirement for interpretability and robustness of autonomous driving. We validate the proposed model in the CARLA simulator with extensive experiments which show remarkable interpretability, generalization, and robustness capability in unknown scenarios for navigation tasks.},
  creationdate     = {2023-03-30T21:39:04},
  doi              = {10.1109/TIV.2022.3225340},
  file             = {:Teng_2023_Hierarchical Interpretable Imitation Learning for End to End Autonomous Driving.pdf:PDF},
  groups           = {TIV_ref},
  keywords         = {Semantics, Data models, Autonomous vehicles, Cameras, Reinforcement learning, Predictive models, Robustness, Autonomous driving, imitation learning, motion planning, end-to-End driving, interpretability},
  modificationdate = {2023-03-30T21:46:40},
}

 
@Article{Li2023,
  author           = {Li, Bai and Fan, Lili and Ouyang, Yakun and Tang, Shiqi and Wang, Xiao and Cao, Dongpu and Wang, Fei-Yue},
  journal          = {IEEE Transactions on Intelligent Vehicles},
  title            = {Online {Competition} of {Trajectory} {Planning} for {Automated} {Parking}: {Benchmarks}, {Achievements}, {Learned} {Lessons}, and {Future} {Perspectives}},
  year             = {2023},
  issn             = {2379-8904},
  number           = {1},
  pages            = {16--21},
  volume           = {8},
  abstract         = {Automated parking is a typical function in a self-driving car. The trajectory planning module directly reflects the intelligence level of an automated parking system. Although many competitions have been launched for autonomous driving, most of them focused on on-road driving scenarios. However, driving on a structured road greatly differs from parking in an unstructured environment. In addition, previous competitions typically competed on the overall driving performance instead of the trajectory planning performance. A trajectory planning competition of automated parking (TPCAP) has been recently organized. This event competed on parking-oriented planners without involving other modules, such as localization, perception, or tracking control. This study reports the TPCAP benchmarks, achievements, experiences, and future perspectives.},
  creationdate     = {2023-03-31T08:41:26},
  doi              = {10.1109/TIV.2022.3228963},
  keywords         = {Trajectory, Trajectory planning, Benchmark testing, Planning, Source coding, Location awareness, Automobiles, Automated parking, trajectory planning, motion planning, autonomous driving, autonomous racing},
  modificationdate = {2023-03-31T08:41:48},
  shorttitle       = {Online {Competition} of {Trajectory} {Planning} for {Automated} {Parking}},
}

@InProceedings{Tu2021,
  author           = {Tu, James and Wang, Tsunhsuan and Wang, Jingkang and Manivasagam, Sivabalan and Ren, Mengye and Urtasun, Raquel},
  booktitle        = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  title            = {Adversarial attacks on multi-agent communication},
  year             = {2021},
  pages            = {7768--7777},
  creationdate     = {2023-03-31T08:52:25},
  modificationdate = {2023-03-31T08:52:25},
}

 
@Article{Lin2023CADer,
  author           = {Lin, Menglong and Chen, Tao and Ren, Bangbang and Chen, Honghui and Zhang, Mengmeng and Guo, Deke},
  journal          = {IEEE Transactions on Intelligent Vehicles},
  title            = {{CADer}: {A} {Deep} {Reinforcement} {Learning} {Approach} for {Designing} the {Communication} {Architecture} of {System} of {Systems}},
  year             = {2023},
  issn             = {2379-8904},
  pages            = {1--13},
  abstract         = {Modern war relies heavily on various combat units, such as command and control units, surveillance units, and reconnaissance units. These units are usually combined organically as a combat SoS (System of Systems) to conduct given missions. To ensure that the combat SoS executes correctly, the communication systems (e.g., communication vehicles, communication UAVs) must be planned carefully to connect all constituent systems or units. However, the communication architecture design problem for combat SoS (SoS-CAD) is very challenging as the combat SoS usually resides in a dynamic and confrontational environment. In this paper, we formally formulate the SoS-CAD problem with integer programming and prove its NP-hardness. The SoS-CAD problem has high requirements for the solving speed as well as the solution quality, but traditional meta-heuristic algorithms cannot satisfy them. Thus, we propose a deep reinforcement learning-based method called CADer to solve it. Specifically, we introduce the attention mechanism and dynamic embedding mechanism into CADer with considering the characteristics of the SoS-CAD problem itself. The massive experiment results show that CADer performs well in the generalization ability and can achieve the best tradeoff between the solving speed and the solution quality against the meta-heuristic algorithm.},
  creationdate     = {2023-04-01T15:53:14},
  doi              = {10.1109/tiv.2023.3236104},
  file             = {:Lin_2023_CADer_ a Deep Reinforcement Learning Approach for Designing the Communication Architecture of System of Systems.pdf:PDF},
  groups           = {temp_for_hsi},
  keywords         = {Reinforcement learning, Heuristic algorithms, Wireless sensor networks, Vehicle dynamics, Deep learning, System of systems, Communication networks, Deep reinforcement learning, System of systems, Communication network covering},
  modificationdate = {2023-04-01T16:02:32},
  shorttitle       = {{CADer}},
}

 
@Article{Chirala2023Heuristics,
  author           = {Chirala, Venkata Sirimuvva and Sundar, Kaarthik and Venkatachalam, Saravanan and Smereka, Jonathon M. and Kassoumeh, Sam},
  journal          = {IEEE Trans. Intell. Veh.},
  title            = {Heuristics for {Multi}-{Vehicle} {Routing} {Problem} {Considering} {Human}-{Robot} {Interactions}},
  year             = {2023},
  issn             = {2379-8904},
  pages            = {1--11},
  abstract         = {Autonomous mobile robots (AMRs) are being used extensively in civilian and military applications for applications such as underground mining, nuclear plant operations, planetary exploration, intelligence, surveillance and reconnaissance (ISR) missions and manned-unmanned teaming. We consider a multi-objective, multiple-vehicle routing problem in which teams of manned ground vehicles (MGVs) and AMRs are deployed respectively in a leader-follower framework to execute missions with differing requirements for MGVs and AMRs while considering human-robot interactions (HRI). HRI studies highlight the costs of managing a team of follower AMRs by a leader MGV. This paper aims to compute feasible visit sequences, replenishments, team compositions and number of MGV-AMR teams deployed such that the requirements for MGVs and AMRs for the missions are met and the routing, replenishment, HRI and team deployment costs are at minimum. The problem is first modeled as a a mixed-integer linear program (MILP) that can be solved to optimality by off-the-shelf commercial solvers for small-sized instances. For larger instances, a variable neighborhood search algorithm is offered to compute near optimal solutions and address the challenges that arise when solving the combinatorial multi-objective routing optimization problem. Finally, computational experiments that corroborate the effectiveness of the proposed algorithms are presented.},
  creationdate     = {2023-04-04T17:07:45},
  doi              = {10.1109/TIV.2023.3261274},
  groups           = {TIV_ref},
  keywords         = {Human-robot interaction, Robots, Costs, Optimization, Task analysis, Vehicle routing, Planning, unmanned autonomous vehicles, human-robot interaction, vehicle routing, variable neighborhood search, multi-objective optimization},
  modificationdate = {2023-04-04T20:26:33},
}

@InProceedings{Meyer2022Game,
  author           = {Meyer, Joel and Pinosky, Allison and Trzpit, Thomas and Colgate, Ed and Murphey, Todd D.},
  booktitle        = {2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)},
  title            = {A {Game} {Benchmark} for {Real}-{Time} {Human}-{Swarm} {Control}},
  year             = {2022},
  month            = aug,
  note             = {ISSN: 2161-8089},
  pages            = {743--750},
  abstract         = {We present a game benchmark for testing human-swarm control algorithms and interfaces in a real-time, high-cadence scenario. Our benchmark consists of a swarm vs. swarm game in a virtual ROS environment in which the goal of the game is to "capture" all agents from the opposing swarm; the game’s high-cadence is a result of the capture rules, which cause agent team sizes to fluctuate rapidly. These rules require players to consider both the number of agents currently at their disposal and the behavior of their opponent’s swarm when they plan actions. We demonstrate our game benchmark with a default human-swarm control system that enables a player to interact with their swarm through a high-level touchscreen interface. The touchscreen interface transforms player gestures into swarm control commands via a low-level decentralized ergodic control framework. We compare our default human-swarm control system to a flocking-based control system, and discuss traits that are crucial for swarm control algorithms and interfaces operating in real-time, high-cadence scenarios like our game benchmark. Our game benchmark code is available on Github; more information can be found at https://sites.google.com/view/swarm-game-benchmark},
  comment          = {一个对战平台，双方各自操作swarm，目标是捕获敌方阵营所有的swarm个体
即将开源

我们将默认的人类群体控制系统与基于群体的控制系统进行比较，并讨论群体控制算法和界面在实时、高节奏场景（如我们的游戏基准测试）中运行的关键特征},
  creationdate     = {2022-11-21T22:00:18},
  doi              = {10.1109/CASE49997.2022.9926423},
  file             = {:Meyer_2022_A Game Benchmark for Real Time Human Swarm Control.pdf:PDF},
  groups           = {Human-Swarm},
  issn             = {2161-8089},
  keywords         = {Computer aided software engineering, Codes, Games, Transforms, Benchmark testing, Touch sensitive screens, Control systems},
  modificationdate = {2023-04-04T20:25:43},
  urldate          = {2023-04-04},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: fileDirectoryLatex-hccz95-LAPTOP-7EP5QO1D:E:\\Desktop\\CurrentWork\\neuro;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Ad Hoc Teamwork\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Ad Hoc Communication\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Ad Hoc Task Allocation\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:Human-Agent Ad Hoc\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:Social Delemmas in AHT\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Coalition Structure\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Hybrid Intelligence\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:knosys\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:MARL\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:ApplicationsInMARL\;0\;0\;0x8a8a8aff\;\;\;;
3 StaticGroup:AutonomousDriving-MARL\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Power-MARL\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Robotics-MARL\;0\;0\;0x8a8a8aff\;\;\;;
3 StaticGroup:RTS-MARL\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:ApplicationsInRL\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Equilibrium\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Meta Learning\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:DomainRandomization\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Meta_NeuroComputing\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Multi_Agent_Meta_Learning\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:Task Similarity\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Planning\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Review\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:RL_Simulation\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Security Game\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Sihang Qiu\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Swarm Intelligence\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Human-Swarm\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Task Assignment\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Precedence\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:RL-based\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Spatial\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Teamwork\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Human-Agent-Robot\;0\;0\;0x8a8a8aff\;\;\;;
3 StaticGroup:Trust\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:temp_for_hsi\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:spanning tree\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:TIV_ref\;0\;1\;0x8a8a8aff\;\;\;;
}
